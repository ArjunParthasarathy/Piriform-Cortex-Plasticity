{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Use smaller network for testing - ex 2000 neurons\n",
    "# Even for the project, doing it for 10^6 neurons would take too long\n",
    "# Problem this creates: test network is denser than actual network b/c we have 10^3 neurons but 10^2 connections per neuron\n",
    "num_neurons = 2000\n",
    "num_i = int(0.1 * num_neurons)\n",
    "num_e = int(0.9 * num_neurons)\n",
    "\n",
    "# Num excitatory inputs and inhibitory inputs to each neuron (in reality it should be 500 but we reduce it here to make things faster)\n",
    "k = 100\n",
    "\n",
    "# Number of olfactory bulb channels (glomeruli) to each neuron\n",
    "D = 10 ** 3\n",
    "# For each neuron, how many glomeruli inputs it receives (should be 10^2)\n",
    "num_channel_inputs = 100\n",
    "\n",
    "# Channel signal if not active for odor a\n",
    "i_0 = 2.\n",
    "# Channel signal if active for odor a\n",
    "i_1 = 10.\n",
    "# Probability that a channel is active for an odor a\n",
    "f = 0.1\n",
    "# Number of odors\n",
    "P = 16\n",
    "# Novel activity is up to P // 2, and familiar activity is after\n",
    "novel_inds = torch.arange(0, P // 2)\n",
    "familiar_inds = torch.arange(P // 2, P)\n",
    "\n",
    "# Creates sparse adjacency matrix with the given probability of edge connection and size mxn\n",
    "def create_adj_matrix(p, m, n):\n",
    "    # num_connections = int(p * m * n)\n",
    "    # m_coords = torch.randint(0, m, (num_connections,))\n",
    "    # n_coords = torch.randint(0, n, (num_connections,))\n",
    "    # indices = torch.vstack((m_coords, n_coords))\n",
    "    # values = torch.ones(num_connections)\n",
    "    # A_mn = torch.sparse_coo_tensor(indices, values, (m, n))\n",
    "    probs = torch.ones(m, n) * p\n",
    "    A_mn = torch.bernoulli(probs)\n",
    "    return A_mn\n",
    "\n",
    "# New way of generating correlations between odors: we want different sets of odors to be correlated differently, so that when we subtract each neuron's mean activity over odors, it doesn't cancel out the variation between odors (if all the odors are correlated the same, they will tend to produce similar values for a single neuron and therefore subtracting by the mean will remove these values and only leave small fluctuations)\n",
    "# So we sample a small set of odors P' and make them linearly independent, and then by multiplying by a P'x P gaussian matrix we project into mitral cell activity space for all P odors, basically making the P odors a linear combination of the set of P' odors (the smaller P' is, the more correlated the resulting set of P odors will be)\n",
    "# We also scale the variance depending on how small P' is, so we will maintain differently correlated odors, just with higher total correlation if P' is small\n",
    "\n",
    "P_prime = 4\n",
    "def correlated_mitral_activity():\n",
    "    # Each of the P' odors is independent (correlation of 0)\n",
    "    sigma_p_prime = torch.zeros((P_prime, P_prime)).fill_diagonal_(1)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(P_prime), sigma_p_prime)\n",
    "    p_prime_activity = dist.sample(torch.Size([D]))\n",
    "    var = 1 / P_prime\n",
    "    projection = torch.normal(torch.zeros((P_prime, P)), torch.ones(P_prime, P) * np.sqrt(var))\n",
    "    activity = p_prime_activity @ projection\n",
    "    return activity.to(gpu)\n",
    "\n",
    "# Takes in mitral activity I and computes feedforward activity h_bar_ff\n",
    "def compute_feedforward_activity(W_ff, I):\n",
    "    with torch.device(gpu):        \n",
    "        h_ff = (W_ff @ I) * (1 / np.sqrt(num_channel_inputs))\n",
    "        h_bar_ff = torch.zeros_like(h_ff)\n",
    "        h_bar_ff[:num_e] = h_ff[:num_e] - torch.mean(h_ff[:num_e], dim=0, keepdim=True)\n",
    "    return h_bar_ff\n",
    "\n",
    "# Computes feedforward (channel) weights mapping mitral activity onto E,I neurons\n",
    "def compute_initial_feedforward_weights():\n",
    "    # Probability that a channel weight will be nonzero\n",
    "    p = num_channel_inputs / D\n",
    "    with torch.device(gpu):\n",
    "        a = create_adj_matrix(p, num_e, D)\n",
    "        # Inhibitory neurons don't receive channel input\n",
    "        # This is the first simplification, where we neglect the first inhibitory layer I_ff\n",
    "        b = torch.zeros(size=(num_i, D))\n",
    "        W_ff = torch.cat(tensors=(a, b), dim=0)\n",
    "\n",
    "    return W_ff\n",
    "\n",
    "def compute_initial_recurrent_weights():\n",
    "    k_ee = k_ei = k_ie = k_ii = k\n",
    "    #p_ee = k_ee / num_e\n",
    "    # k inhibitory inputs to that e neuron, out of num_i total inhibitory neurons gives the connection probability per neuron\n",
    "    p_ei = k_ei / num_i\n",
    "    p_ie = k_ie / num_e\n",
    "    #p_ii = k_ii / num_i\n",
    "    \n",
    "    # Constants\n",
    "    #w_ee = 0.1\n",
    "    w_ei = 0.2\n",
    "    w_ie = 0.5\n",
    "    #w_ii = 0.3\n",
    "    # Ignore ee and ii weights for now:\n",
    "    p_ee = p_ii = w_ee = w_ii = 0\n",
    "    with torch.device(gpu):\n",
    "        W_ee = create_adj_matrix(p_ee, num_e, num_e) * w_ee\n",
    "        W_ei = create_adj_matrix(p_ei, num_e, num_i) * -w_ei\n",
    "        W_ie = create_adj_matrix(p_ie, num_i, num_e) * w_ie\n",
    "        W_ii = create_adj_matrix(p_ii, num_i, num_i) * -w_ii\n",
    "        \n",
    "        # Concat\n",
    "        W_1 = torch.cat(tensors=(W_ee, W_ei), dim=1)\n",
    "        W_2 = torch.cat(tensors=(W_ie, W_ii), dim=1)\n",
    "        W_rec = torch.cat(tensors=(W_1, W_2), dim=0)\n",
    "    \n",
    "    return W_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Computes activation threshold for neurons, based on the standard deviation of their firing rates across odors\n",
    "# This average standard deviation, multiplied by theta=2, ensures that each neuron will fire for only 5% of odors\n",
    "def compute_threshold(total_input, theta):\n",
    "    # For now, use diff thresholds for each neuron\n",
    "    center = torch.mean(total_input, dim=1, keepdim=True)\n",
    "    shift = torch.std(total_input, dim=1, keepdim=True)\n",
    "    threshold = center + (theta * shift)\n",
    "    # Since inhibitory neurons are linear\n",
    "    threshold[num_e:, :] = 0\n",
    "    return threshold\n",
    "\n",
    "# %%\n",
    "# ReLU for excitatory, linear for inhibitory\n",
    "def neuron_activations(X):\n",
    "    # Mask to keep excitatory\n",
    "    mask1 = torch.ones((num_neurons, 1), device=gpu)\n",
    "    mask1[num_e:, :] = 0\n",
    "    # Mask to keep inhibitory\n",
    "    mask2 = torch.zeros((num_neurons, 1), device=gpu)\n",
    "    mask2[num_e:, :] = 1\n",
    "    return (torch.relu(X) * mask1) + (X * mask2)\n",
    "\n",
    "# %%\n",
    "# Computes R for each odor, with the activation threshold theta\n",
    "def compute_piriform_response(W_ff, I, W_rec, threshold_mult):\n",
    "    h_bar_ff = compute_feedforward_activity(W_ff, I)\n",
    "    # The coefficient of x_bar\n",
    "    tau = 1\n",
    "    # time step\n",
    "    dt = 0.1\n",
    "    # Number of time steps\n",
    "    T = 200\n",
    "    \n",
    "    # Initial condition where states are gaussian\n",
    "    mu_0 = 0.\n",
    "    sigma_0 = 0.2\n",
    "    X_0 = torch.normal(mu_0, sigma_0, size=(num_neurons, P))\n",
    "    X = X_0.to(gpu)\n",
    "    \n",
    "    pts = []\n",
    "    for i in range(T-2):\n",
    "        with torch.no_grad():\n",
    "            part1 = -1 * X\n",
    "            part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "            part3 = h_bar_ff\n",
    "            dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "            X = X + (dXdt * dt)\n",
    "        # Look at convergence pattern for first odor, assuming that it'll\n",
    "        # be similar across odors (since they are all independent)\n",
    "        #pts.append(torch.mean(dXdt, dim=0)[0].item())\n",
    "   \n",
    "    # On the last 2 iterations only, track the gradient\n",
    "    X.requires_grad_(True)\n",
    "    \n",
    "    for j in range(2):\n",
    "        part1 = -1 * X\n",
    "        part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "        part3 = h_bar_ff\n",
    "        dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "        X = X + (dXdt * dt)\n",
    "    \n",
    "    # The total input to the neuron at this last time step (should be equivalent to the resulting value of X after this time step, since dxdt = 0 after the recurrent network converges)\n",
    "    total_input = part2 + part3\n",
    "    threshold = compute_threshold(total_input, threshold_mult)\n",
    "    \n",
    "    # Plot derivatives to see if state converged\n",
    "    # plt.plot(torch.arange(T-2), pts)\n",
    "    # plt.show()\n",
    "    R = neuron_activations(X - threshold)\n",
    "    \n",
    "    return R\n",
    "\n",
    "# %%\n",
    "# Compute dimensionality of activity matrix R for either novel or familiar\n",
    "def compute_dim(R, odor_inds):\n",
    "    # Only compute for the excitatory neurons (b/c those are the ones that send signals to rest of brain\n",
    "    C = torch.cov(R[:num_e, odor_inds[0]:odor_inds[-1]])\n",
    "    dim = torch.trace(C) ** 2 / torch.trace(C @ C)\n",
    "    return dim\n",
    "\n",
    "# trace() is invariant for cyclic permutations of a matrix\n",
    "# Since C is symmetric, it can be orthogonally diagonalized into UDU^T where U is composed of orthonormal eigenvectors, U^T = U^-1, and D is a diagonal matrix of eigenvalues\n",
    "# therefore, trace(C) = trace(UDU^T) = trace(DU^TU) = trace(D) = sum(eigvals of C)\n",
    "# Similarly for the denominator, we need to compute the sum of the squared eigenvalues, which is trace(D^2). trace(D^2) = trace(D^2U^TU) = trace(UD^2U^T) = trace((UDU^T)^2)) [by the property of matrix exponentiation for a diagonalizable matrix] = trace(C^2) = trace(C @ C)\n",
    "\n",
    "# %%\n",
    "# Dimensionalities should be similar before performing plasticity, upper bound on dimensionality is 8 b/c there are 8 odors each for novel/familiar (this is the rank of the cov matrix)\n",
    "# Measures how many orthogonal directions in neuron space are needed to explain the set of odors (the max number of orthogonal directions is the number of neurons themselves)\n",
    "# print(C_novel0)\n",
    "# print(C_familiar0)\n",
    "\n",
    "# %%\n",
    "# Start and stop indices for the section of W_rec we want to update, respectively \n",
    "# Takes in R_alpha, a vector of neuron responses to a particular odor\n",
    "def compute_update(model: torch.nn.Sequential, R_alpha: torch.Tensor, update_inds) -> torch.Tensor:\n",
    "    # Compute the same pairs of R_i and R_j for every odor\n",
    "    # 1D vectors of the pre and postsynaptic neurons corresponding to the nonzero weights of W_rec\n",
    "    postsyn_responses = R_alpha[update_inds[0]]\n",
    "    presyn_responses = R_alpha[update_inds[1]]\n",
    "    model_input = torch.vstack((presyn_responses, postsyn_responses)).t()\n",
    "    slice_updates = model(model_input)\n",
    "    \n",
    "    updates = slice_updates.squeeze(dim=1)\n",
    "    \n",
    "    return updates\n",
    "\n",
    "# %%\n",
    "def odor_corrs(R):\n",
    "    # We don't care about the actual responses per odor, just about a neuron's fluctuations around its mean response across odors\n",
    "    R_adjusted = R - torch.mean(R, dim=1, keepdim=True)\n",
    "    # Each odor becomes a variable, because we want to calculate correlations between them across neurons\n",
    "    R_adjusted.t_()\n",
    "    # Like cov but divides by standard deviations, effectively normalizing the values (the diagonals of the resulting matrix become 1)\n",
    "    sigma_E = torch.corrcoef(R_adjusted)\n",
    "    # We only care about the correlations between the familiar odors\n",
    "    familiar_corrs = sigma_E[P//2:P, P//2:P] - torch.eye(P // 2, device=gpu)\n",
    "    corr_sum = torch.sum(familiar_corrs ** 2)\n",
    "    avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "    \n",
    "    return corr_sum, avg_corr\n",
    "\n",
    "# %%\n",
    "# Sparsity per odor, across all (E) neurons\n",
    "def sparsity_per_odor(R):\n",
    "    # Epsilon for if we have zero responses\n",
    "    eps = 1e-6\n",
    "    sp_per_odor = 1 - ((torch.sum(R[:num_e], dim=0) ** 2 + eps) / (num_e * (torch.sum(R[:num_e] ** 2, dim=0)) + eps))\n",
    "    # Sparsity nan means that the responses were all 0 for an odor, meaning that its max sparsity of 1\n",
    "    return sp_per_odor\n",
    "\n",
    "# Sparsity per (E) neuron, across a given odor family\n",
    "def sparsity_per_neuron(R, odor_inds):\n",
    "    sp_per_neuron = 1 - (\n",
    "                (torch.sum(R[:num_e, odor_inds], dim=1) ** 2) / ((P // 2) * torch.sum(R[:num_e, odor_inds] ** 2, dim=1)))\n",
    "    return sp_per_neuron\n",
    "\n",
    "# %%\n",
    "# Try to minimize the correlations between values\n",
    "def loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=True):\n",
    "    corr_sum, avg_corr = odor_corrs(R)\n",
    "    corr_loss = (1 / P) * corr_sum\n",
    "    corr_term = lambda_corr * corr_loss\n",
    "    \n",
    "    means = torch.mean(R, dim=0)\n",
    "    means_novel = torch.mean(means[novel_inds])\n",
    "    means_familiar = torch.mean(means[familiar_inds])\n",
    "    mu_term = lambda_mu * (((means_familiar - means_novel) / (means_novel + means_familiar)) ** 2)\n",
    "    \n",
    "    vars = torch.var(R, dim=0)\n",
    "    var_novel = torch.mean(vars[novel_inds])\n",
    "    var_familiar = torch.mean(vars[familiar_inds])\n",
    "    var_term = lambda_var * (((var_familiar - var_novel) / (var_novel + var_familiar)) ** 2)\n",
    "    \n",
    "    sparsities = sparsity_per_odor(R)\n",
    "    spars_novel = torch.mean(sparsities[novel_inds])\n",
    "    spars_familiar = torch.mean(sparsities[familiar_inds])\n",
    "    spars_term = lambda_sp * (((spars_familiar - spars_novel) / (spars_novel + spars_familiar)) ** 2)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Avg Corr: %.4f, Corr: %.4f, Mu: %.4f, Var: %.4f, Sparsity: %.4f\" % (avg_corr, corr_term, mu_term, var_term, spars_term))\n",
    "    loss = corr_term + mu_term + var_term + spars_term\n",
    "    return loss\n",
    "\n",
    "# %%\n",
    "def loss_after_odors(W_ff: torch.Tensor, I: torch.Tensor, W_rec: torch.Tensor, threshold_mult, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print):   \n",
    "    R_new = compute_piriform_response(W_ff, I, W_rec, threshold_mult)\n",
    "    loss = loss_fn(R_new, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print)\n",
    "    \n",
    "    return loss, R_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "epochs_inner = 10000\n",
    "# Number of standard deviations from mean, we are trying 0 b/c 1 and 2 is too sparse\n",
    "threshold_multiplier = 0\n",
    "\n",
    "lambda_corr, lambda_mu, lambda_var, lambda_sp = 1, 0, 0, 0\n",
    "\n",
    "mult = 100\n",
    "w_mitral = 1\n",
    "mitral_max = mult * w_mitral\n",
    "mitral_min = 0\n",
    "\n",
    "def train_model():\n",
    "    corrs = torch.zeros((epochs_inner,))\n",
    "    \n",
    "    W_rec = compute_initial_recurrent_weights()\n",
    "        \n",
    "    I = correlated_mitral_activity()\n",
    "    W_initial = compute_initial_feedforward_weights()\n",
    "    W_trained = W_initial.clone()\n",
    "    W_trained.requires_grad_(True)\n",
    "\n",
    "    update_inds = torch.nonzero(W_trained, as_tuple=True)\n",
    "\n",
    "    # Only update the relevant weights\n",
    "    def w_hook(grad):\n",
    "        new_grad = torch.zeros_like(grad)\n",
    "        new_grad[update_inds] = grad[update_inds]\n",
    "        return new_grad\n",
    "    \n",
    "    W_trained.register_hook(w_hook)\n",
    "    # Moved to LR -2 and 10000 epochs (I->E needs slower training)\n",
    "    # Move even more to LR -3\n",
    "    optimizer = optim.Adam([W_trained], lr=1e-3)\n",
    "   \n",
    "    R_initial = compute_piriform_response(W_trained, I, W_rec, threshold_multiplier)\n",
    "    R_trained = R_initial.clone()\n",
    "    print(f\"Initial loss: \\t\", end=\"\")\n",
    "    loss_fn(R_initial, lambda_corr, lambda_mu, lambda_var, lambda_sp)\n",
    "    \n",
    "    clamp_min = torch.zeros_like(W_trained)\n",
    "    clamp_min[update_inds] = mitral_min\n",
    "    clamp_max = torch.zeros_like(W_trained)\n",
    "    clamp_max[update_inds] = mitral_max\n",
    "    \n",
    "    # if track_weights:\n",
    "    #     # Number of weights to track across iterations\n",
    "    #     num_samples = 100\n",
    "    #     track_inds = torch.randint(0, len(update_inds), size=(num_samples,))\n",
    "    #     W_tracked = torch.empty((epochs_inner, num_samples))\n",
    "    \n",
    "    for i in range(epochs_inner):\n",
    "            \n",
    "        do_print=False\n",
    "        if (i % 100 == 0):\n",
    "            print(f\"Epoch {i}: \\t\", end=\"\")\n",
    "            do_print = True\n",
    "        loss, R_trained = loss_after_odors(W_trained, I, W_rec, threshold_multiplier, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print)\n",
    "        \n",
    "        corrs[i] = odor_corrs(R_trained)[1].item()\n",
    "        \n",
    "        # if track_weights:\n",
    "        #     W_tracked[i, :] = W_trained[update_inds][track_inds].detach()\n",
    "        \n",
    "        loss.backward()   \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            W_trained.clamp_(min=clamp_min, max=clamp_max)\n",
    "            \n",
    "    return corrs, update_inds, I, W_initial, W_trained, R_initial, R_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: \tAvg Corr: 0.2610, Corr: 0.3979, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 0: \tAvg Corr: 0.2610, Corr: 0.3979, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 100: \tAvg Corr: 0.1965, Corr: 0.2168, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 200: \tAvg Corr: 0.1653, Corr: 0.1574, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 300: \tAvg Corr: 0.1492, Corr: 0.1328, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 400: \tAvg Corr: 0.1432, Corr: 0.1235, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 500: \tAvg Corr: 0.1402, Corr: 0.1200, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 600: \tAvg Corr: 0.1382, Corr: 0.1182, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 700: \tAvg Corr: 0.1371, Corr: 0.1171, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 800: \tAvg Corr: 0.1362, Corr: 0.1163, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 900: \tAvg Corr: 0.1355, Corr: 0.1155, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1000: \tAvg Corr: 0.1350, Corr: 0.1152, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1100: \tAvg Corr: 0.1347, Corr: 0.1151, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1200: \tAvg Corr: 0.1343, Corr: 0.1149, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1300: \tAvg Corr: 0.1341, Corr: 0.1146, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1400: \tAvg Corr: 0.1340, Corr: 0.1143, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1500: \tAvg Corr: 0.1341, Corr: 0.1142, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1600: \tAvg Corr: 0.1343, Corr: 0.1143, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1700: \tAvg Corr: 0.1347, Corr: 0.1147, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1800: \tAvg Corr: 0.1352, Corr: 0.1152, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 1900: \tAvg Corr: 0.1358, Corr: 0.1157, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2000: \tAvg Corr: 0.1363, Corr: 0.1164, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2100: \tAvg Corr: 0.1368, Corr: 0.1170, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2200: \tAvg Corr: 0.1373, Corr: 0.1177, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2300: \tAvg Corr: 0.1378, Corr: 0.1185, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2400: \tAvg Corr: 0.1383, Corr: 0.1195, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2500: \tAvg Corr: 0.1388, Corr: 0.1204, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2600: \tAvg Corr: 0.1392, Corr: 0.1213, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2700: \tAvg Corr: 0.1395, Corr: 0.1221, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2800: \tAvg Corr: 0.1398, Corr: 0.1229, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 2900: \tAvg Corr: 0.1402, Corr: 0.1237, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Epoch 3000: \tAvg Corr: 0.1405, Corr: 0.1245, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model()\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m     do_print \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m loss, R_trained \u001b[39m=\u001b[39m loss_after_odors(W_trained, I, W_rec, threshold_multiplier, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print)\n\u001b[1;32m     60\u001b[0m corrs[i] \u001b[39m=\u001b[39m odor_corrs(R_trained)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     62\u001b[0m \u001b[39m# if track_weights:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39m#     W_tracked[i, :] = W_trained[update_inds][track_inds].detach()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 167\u001b[0m, in \u001b[0;36mloss_after_odors\u001b[0;34m(W_ff, I, W_rec, threshold_mult, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_after_odors\u001b[39m(W_ff: torch\u001b[39m.\u001b[39mTensor, I: torch\u001b[39m.\u001b[39mTensor, W_rec: torch\u001b[39m.\u001b[39mTensor, threshold_mult, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print):   \n\u001b[0;32m--> 167\u001b[0m     R_new \u001b[39m=\u001b[39m compute_piriform_response(W_ff, I, W_rec, threshold_mult)\n\u001b[1;32m    168\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(R_new, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print)\n\u001b[1;32m    170\u001b[0m     \u001b[39mreturn\u001b[39;00m loss, R_new\n",
      "Cell \u001b[0;32mIn[3], line 43\u001b[0m, in \u001b[0;36mcompute_piriform_response\u001b[0;34m(W_ff, I, W_rec, threshold_mult)\u001b[0m\n\u001b[1;32m     41\u001b[0m pts \u001b[39m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(T\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     44\u001b[0m         part1 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m*\u001b[39m X\n\u001b[1;32m     45\u001b[0m         part2 \u001b[39m=\u001b[39m (W_rec \u001b[39m@\u001b[39m neuron_activations(X)) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(k))\n",
      "File \u001b[0;32m~/Piriform-Cortex-Plasticity/venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:84\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[1;32m     82\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f64f3bee69275d7dadabcd164c00bee7a237ebc40dc30e8b43706029d0d9fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
