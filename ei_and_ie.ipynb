{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.115867Z",
     "start_time": "2024-07-29T02:19:02.059779Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.120544Z",
     "start_time": "2024-07-29T02:19:02.061094Z"
    }
   },
   "id": "cc4fbc6b56b8cbae"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "gpu = torch.device(\"mps\")\n",
    "\n",
    "# Use smaller network for testing - ex 2000 neurons\n",
    "# Even for the project, doing it for 10^6 neurons would take too long\n",
    "# Problem this creates: test network is denser than actual network b/c we have 10^3 neurons but 10^2 connections per neuron\n",
    "num_neurons = 2000\n",
    "num_i = int(0.1 * num_neurons)\n",
    "num_e = int(0.9 * num_neurons)\n",
    "\n",
    "# Epsilon value close to 0 to prevent nan in division by 0\n",
    "eps = 1e-6\n",
    "\n",
    "# Num excitatory inputs and inhibitory inputs to each neuron (in reality it should be 500 but we reduce it here to make things faster)\n",
    "k = 100\n",
    "\n",
    "# Number of olfactory bulb channels (glomeruli) to each neuron\n",
    "D = 10 ** 3\n",
    "# For each neuron, how many glomeruli inputs it receives (should be 10^2)\n",
    "num_channel_inputs = 100\n",
    "\n",
    "# Number of odors\n",
    "P = 16\n",
    "# Novel activity is up to P // 2, and familiar activity is after\n",
    "novel_inds = torch.arange(0, P // 2)\n",
    "familiar_inds = torch.arange(P // 2, P)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.140075Z",
     "start_time": "2024-07-29T02:19:03.131653Z"
    }
   },
   "id": "80a1147c4c0a6f95"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Creates sparse adjacency matrix with the given probability of edge connection and size mxn\n",
    "def create_adj_matrix(p, m, n):\n",
    "    # num_connections = int(p * m * n)\n",
    "    # m_coords = torch.randint(0, m, (num_connections,))\n",
    "    # n_coords = torch.randint(0, n, (num_connections,))\n",
    "    # indices = torch.vstack((m_coords, n_coords))\n",
    "    # values = torch.ones(num_connections)\n",
    "    # A_mn = torch.sparse_coo_tensor(indices, values, (m, n))\n",
    "    probs = torch.ones(m, n) * p\n",
    "    A_mn = torch.bernoulli(probs)\n",
    "    return A_mn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.158129Z",
     "start_time": "2024-07-29T02:19:03.134625Z"
    }
   },
   "id": "d4e13c61f25818ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# New way of generating correlations between odors: we want different sets of odors to be correlated differently, so that when we subtract each neuron's mean activity over odors, it doesn't cancel out the variation between odors (if all the odors are correlated the same, they will tend to produce similar values for a single neuron and therefore subtracting by the mean will remove these values and only leave small fluctuations)\n",
    "# So we sample a small set of odors P' and make them linearly independent, and then by multiplying by a P'x P gaussian matrix we project into mitral cell activity space for all P odors, basically making the P odors a linear combination of the set of P' odors (the smaller P' is, the more correlated the resulting set of P odors will be)\n",
    "# We also scale the variance depending on how small P' is, so we will maintain differently correlated odors, just with higher total correlation if P' is small"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.158232Z",
     "start_time": "2024-07-29T02:19:03.138241Z"
    }
   },
   "id": "c6b94961ca601610"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "P_prime = 4\n",
    "def correlated_mitral_activity():\n",
    "    # Each of the P' odors is independent (correlation of 0)\n",
    "    sigma_p_prime = torch.zeros((P_prime, P_prime)).fill_diagonal_(1)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(P_prime), sigma_p_prime)\n",
    "    p_prime_activity = dist.sample(torch.Size([D]))\n",
    "    var = 1 / P_prime\n",
    "    projection = torch.normal(torch.zeros((P_prime, P)), torch.ones(P_prime, P) * np.sqrt(var))\n",
    "    activity = p_prime_activity @ projection\n",
    "    return activity.to(gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.162069Z",
     "start_time": "2024-07-29T02:19:03.141143Z"
    }
   },
   "id": "28887b0933bdd67a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Takes in mitral activity I and computes feedforward activity h_bar_ff\n",
    "def compute_feedforward_activity(I):\n",
    "    # Probability that a channel weight will be nonzero\n",
    "    p = num_channel_inputs / D\n",
    "    # Only the first 0.9 * n rows should have this bernoulli number, the rest should be 0 b/c they don't receive a channel input\n",
    "    # Check whether each neuron still receives ~10^2 nonzero inputs or what the distribution actually looks like\n",
    "    # Because when we calculate the adjacency matrix we don't go by row (e.g ensuring each neuron has these ~10^2 connections)\n",
    "    # Alternative: sample from Binomial distribution w/ mean 100\n",
    "    # The output n for each row is the number of nonzero inputs, and you choose a random subset n of the indices for that row and make them 1\n",
    "    with torch.device(gpu):\n",
    "        a = create_adj_matrix(p, num_e, D)\n",
    "        # Inhibitory neurons don't receive channel input\n",
    "        # This is the first simplification, where we neglect the first inhibitory layer I_ff\n",
    "        b = torch.zeros(size=(num_i, D))\n",
    "        W_ff = torch.cat(tensors=(a, b), dim=0)\n",
    "        \n",
    "        h_ff = (W_ff @ I) * (1 / np.sqrt(num_channel_inputs))\n",
    "        h_bar_ff = torch.zeros_like(h_ff)\n",
    "        h_bar_ff[:num_e] = h_ff[:num_e] - torch.mean(h_ff[:num_e], dim=0, keepdim=True)\n",
    "    return h_bar_ff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.162115Z",
     "start_time": "2024-07-29T02:19:03.145034Z"
    }
   },
   "id": "67da2d3fe4b6a081"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_initial_recurrent_weights():\n",
    "    k_ee = k_ei = k_ie = k_ii = k\n",
    "    #p_ee = k_ee / num_e\n",
    "    # k inhibitory inputs to that e neuron, out of num_i total inhibitory neurons gives the connection probability per neuron\n",
    "    p_ei = k_ei / num_i\n",
    "    p_ie = k_ie / num_e\n",
    "    #p_ii = k_ii / num_i\n",
    "    \n",
    "    # Constants\n",
    "    #w_ee = 0.1\n",
    "    w_ei = 0.2\n",
    "    w_ie = 0.5\n",
    "    #w_ii = 0.3\n",
    "    # Ignore ee and ii weights for now:\n",
    "    p_ee = p_ii = w_ee = w_ii = 0\n",
    "    with torch.device(gpu):\n",
    "        W_ee = create_adj_matrix(p_ee, num_e, num_e) * w_ee\n",
    "        W_ei = create_adj_matrix(p_ei, num_e, num_i) * -w_ei\n",
    "        W_ie = create_adj_matrix(p_ie, num_i, num_e) * w_ie\n",
    "        W_ii = create_adj_matrix(p_ii, num_i, num_i) * -w_ii\n",
    "        \n",
    "        # Concat\n",
    "        W_1 = torch.cat(tensors=(W_ee, W_ei), dim=1)\n",
    "        W_2 = torch.cat(tensors=(W_ie, W_ii), dim=1)\n",
    "        W_rec = torch.cat(tensors=(W_1, W_2), dim=0)\n",
    "    \n",
    "    return W_rec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.162141Z",
     "start_time": "2024-07-29T02:19:03.148820Z"
    }
   },
   "id": "b23d7ccb6f4b0787"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Computes activation threshold for neurons, based on the standard deviation of their firing rates across odors\n",
    "# This average standard deviation, multiplied by theta=2, ensures that each neuron will fire for only 5% of odors\n",
    "def compute_threshold(total_input, theta):\n",
    "    # For now, use diff thresholds for each neuron\n",
    "    center = torch.mean(total_input, dim=1, keepdim=True)\n",
    "    shift = torch.std(total_input, dim=1, keepdim=True)\n",
    "    threshold = center + (theta * shift)\n",
    "    # Since inhibitory neurons are linear\n",
    "    threshold[num_e:, :] = 0\n",
    "    return threshold"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.162166Z",
     "start_time": "2024-07-29T02:19:03.151547Z"
    }
   },
   "id": "65ae3fd6edb6a55b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ReLU for excitatory, linear for inhibitory\n",
    "def neuron_activations(X):\n",
    "    # Mask to keep excitatory\n",
    "    mask1 = torch.ones((num_neurons, 1), device=gpu)\n",
    "    mask1[num_e:, :] = 0\n",
    "    # Mask to keep inhibitory\n",
    "    mask2 = torch.zeros((num_neurons, 1), device=gpu)\n",
    "    mask2[num_e:, :] = 1\n",
    "    activation = (torch.relu(X) * mask1) + (X * mask2)\n",
    "    return activation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.162192Z",
     "start_time": "2024-07-29T02:19:03.154413Z"
    }
   },
   "id": "1ce5f8670e8cb95e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Computes R for each odor, with the activation threshold theta\n",
    "def compute_piriform_response(h_bar_ff, W_rec, threshold_mult):\n",
    "    # The coefficient of x_bar\n",
    "    tau = 1\n",
    "    # time step\n",
    "    dt = 0.1\n",
    "    # Number of time steps\n",
    "    T = 200\n",
    "    \n",
    "    # Initial condition where states are gaussian\n",
    "    mu_0 = 0.\n",
    "    sigma_0 = 0.2\n",
    "    X_0 = torch.normal(mu_0, sigma_0, size=(num_neurons, P))\n",
    "    X = X_0\n",
    "    X = X.to(gpu)\n",
    "    \n",
    "    #pts = []\n",
    "    for i in range(T-4):\n",
    "        with torch.no_grad():\n",
    "            part1 = -1 * X\n",
    "            part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "            part3 = h_bar_ff\n",
    "            dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "            X = X + (dXdt * dt)\n",
    "        # Look at convergence pattern for first odor, assuming that it'll\n",
    "        # be similar across odors (since they are all independent)\n",
    "        #pts.append(torch.mean(dXdt, dim=0)[0].item())\n",
    "   \n",
    "    # On the last 2 iterations only, track the gradient\n",
    "    # TODO increased to 4 just for more coverage\n",
    "    X.requires_grad_(True)\n",
    "    \n",
    "    for j in range(4):\n",
    "        part1 = -1 * X\n",
    "        part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "        part3 = h_bar_ff\n",
    "        dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "        X = X + (dXdt * dt)\n",
    "    \n",
    "    # The total input to the neuron at this last time step (should be equivalent to the resulting value of X after this time step, since dxdt = 0 after the recurrent network converges)\n",
    "    total_input = part2 + part3\n",
    "    threshold = compute_threshold(total_input, threshold_mult)\n",
    "    \n",
    "    # Plot derivatives to see if state converged\n",
    "    # plt.plot(torch.arange(T-2), pts)\n",
    "    # plt.show()\n",
    "    R = neuron_activations(X - threshold)\n",
    "    \n",
    "    return R"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.178661Z",
     "start_time": "2024-07-29T02:19:03.159118Z"
    }
   },
   "id": "14aa733bbbadf1db"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Compute dimensionality of activity matrix R for either novel or familiar\n",
    "# def compute_dim(R, odor_inds):\n",
    "#     # Only compute for the excitatory neurons (b/c those are the ones that send signals to rest of brain\n",
    "#     C = torch.cov(R[:num_e, odor_inds[0]:odor_inds[-1]])\n",
    "#     dim = torch.trace(C) ** 2 / torch.trace(C @ C)\n",
    "#     return dim\n",
    "\n",
    "# trace() is invariant for cyclic permutations of a matrix\n",
    "# Since C is symmetric, it can be orthogonally diagonalized into UDU^T where U is composed of orthonormal eigenvectors, U^T = U^-1, and D is a diagonal matrix of eigenvalues\n",
    "# therefore, trace(C) = trace(UDU^T) = trace(DU^TU) = trace(D) = sum(eigvals of C)\n",
    "# Similarly for the denominator, we need to compute the sum of the squared eigenvalues, which is trace(D^2). trace(D^2) = trace(D^2U^TU) = trace(UD^2U^T) = trace((UDU^T)^2)) [by the property of matrix exponentiation for a diagonalizable matrix] = trace(C^2) = trace(C @ C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199360Z",
     "start_time": "2024-07-29T02:19:03.161635Z"
    }
   },
   "id": "defc0132008cbd72"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Dimensionalities should be similar before performing plasticity, upper bound on dimensionality is 8 b/c there are 8 odors each for novel/familiar (this is the rank of the cov matrix)\n",
    "# Measures how many orthogonal directions in neuron space are needed to explain the set of odors (the max number of orthogonal directions is the number of neurons themselves)\n",
    "# print(C_novel0)\n",
    "# print(C_familiar0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199460Z",
     "start_time": "2024-07-29T02:19:03.163725Z"
    }
   },
   "id": "9b2eddcd771e4eb3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Loop over odors and after computing R_alpha (stationary/convergence state) for each odor, perform weight update on W_rec\n",
    "# Compute increment for W_rec(i,j) which is only dependent on R_alpha(i) and R_alpha(j) (two different neurons in W_rec)\n",
    "# Do it only for the nonzero weights for now (because these are the only active connections between neurons)\n",
    "# - Later, we could explore whether new synapses etc are created/destroyed (the changing of a weight from zero to nonzero)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199780Z",
     "start_time": "2024-07-29T02:19:03.168354Z"
    }
   },
   "id": "152f0a7f13e8e541"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Set up MLP which takes in R_alpha(i) and R_alpha(j) and computes an output value delta_W_rec(i,j) which is the update value for W_rec(i,j)\n",
    "# Single MLP for each class of weights (W_ee, W_ei, etc), and we adjust the weights of this MLP\n",
    "# - Because plasticity is different between different kinds of neurons"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199816Z",
     "start_time": "2024-07-29T02:19:03.170679Z"
    }
   },
   "id": "f9a5a7b63771a7c3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Start w/ small MLP - 1 hidden layer (ex. 20 units), 2 inputs, 1 output\n",
    "# Each time step, we want to compute the weight update\n",
    "# Compute the responses using the existing weight matrix, run thru the MLP to get the weight update, and update the weight matrix\n",
    "# After n iterations, we want to calculate dimensionality (from the covariance matrix we generate from the neuron responses)\n",
    "# Loss function: dim Novel / dim Familiar\n",
    "# - Potential case: might make dim(Novel) as 0\n",
    "\n",
    "# Might have to regularize the firing rate - should be close to a value we set\n",
    "# Ex. could take average of neuron responses at beginning (before plasticity) and set that as target firing rate\n",
    "\n",
    "# To prevent exploding/vanishing gradients, might have to truncate the gradient (stop it from updating after a certain iteration) because when we accumulate the gradient over the time steps to compute the response, it might blow up or just vanish\n",
    "# - since the weight update is used to update the weight matrix and compute the next time step of neuron response, which in turn is fed into the MLP to compute the next weight update (very cyclical)\n",
    "# Another solution - only backprop thru the last time steps of the recurrent dynamics (the time steps to compute a neuron response), b/c these are where the neuron response converges to the actual value\n",
    "# Lookup how to truncate gradient PyTorch\n",
    "\n",
    "# Instead of having plasticity act on all weights, only have plasticity between certain cell types (ex. between E and E, it will create positive feedback loop on weight update and may cause gradient blowup)\n",
    "# For now, only use MLP for E -> I (minimize likelihood of gradient blowup), don't have to truncate gradient yet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199843Z",
     "start_time": "2024-07-29T02:19:03.173375Z"
    }
   },
   "id": "3effd76566c0c0b2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def create_model() -> torch.nn.Sequential:\n",
    "    # Number of neurons in hidden layer\n",
    "    hidden_size = 100\n",
    "    # Constrain output to certain values: nn.Hardtanh(-1, 1)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "    )\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.199880Z",
     "start_time": "2024-07-29T02:19:03.176411Z"
    }
   },
   "id": "24911045583425aa"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Start and stop indices for the section of W_rec we want to update, respectively \n",
    "# Takes in R matrix (neuron responses for each odor and tuple of update inds representing ie, then ei (each element in that tuple is itself a tuple of (post, pre))\n",
    "def compute_updates(R: torch.Tensor, models: tuple, update_inds: tuple) -> torch.Tensor:\n",
    "    # Compute the same pairs of R_i and R_j for every odor, per model\n",
    "    all_updates = []\n",
    "    for i in range(len(models)):\n",
    "        postsyn_responses = R[update_inds[i][0], :]\n",
    "        presyn_responses = R[update_inds[i][1], :]\n",
    "        model_input = torch.stack(tensors=(presyn_responses, postsyn_responses), dim=2).transpose(1, 0)\n",
    "        updates_per_odor = models[i](model_input)\n",
    "        model_updates = torch.sum(updates_per_odor, dim=0).squeeze(dim=1)\n",
    "        all_updates.append(model_updates)\n",
    "    \n",
    "    return all_updates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.222955Z",
     "start_time": "2024-07-29T02:19:03.179591Z"
    }
   },
   "id": "a1cd6e5a0aa03ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# ie_post = (num_e, num_neurons)\n",
    "# ie_pre = (0, num_e)\n",
    "# ei_post = (0, num_e)\n",
    "# ei_pre = (num_e, num_neurons)\n",
    "# ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "# ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "# # (16, 19953, 2) -> (16, 19953)\n",
    "# a1, a2 = compute_updates(R_random.to(gpu), (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "# #print(a1.shape, a2.shape)\n",
    "# print(a1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.223071Z",
     "start_time": "2024-07-29T02:19:03.181868Z"
    }
   },
   "id": "9f51d515fadf0381"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def odor_corrs(R):\n",
    "    # We don't care about the actual responses per odor, just about a neuron's fluctuations around its mean response across odors\n",
    "    R_adjusted = R[:num_e] - torch.mean(R[:num_e], dim=1, keepdim=True)\n",
    "    # Each odor becomes a variable, because we want to calculate correlations between them across neurons\n",
    "    R_adjusted.t_()\n",
    "    # Like cov but divides by standard deviations, effectively normalizing the values (the diagonals of the resulting matrix become 1)\n",
    "    corrcoefs = torch.corrcoef(R_adjusted)\n",
    "    # If the responses are 0, variances across neurons will be 0, so denominator of corrcoef is 0, so term becomes nan\n",
    "    # In this case, the responses are \"perfectly correlated\" (bc always same value of 0) so its maximum correlation\n",
    "    # TODO do we need to change this NaN formulation so we propagate grad correctly?\n",
    "    corrcoefs = torch.nan_to_num(corrcoefs, nan=1.0)\n",
    "    # We only care about the correlations between the familiar odors\n",
    "    familiar_corrs = corrcoefs[P//2:P, P//2:P] - torch.eye(P // 2, device=gpu)\n",
    "    corr_sum = torch.sum(familiar_corrs ** 2)\n",
    "    avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "    \n",
    "    return corr_sum, avg_corr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.223344Z",
     "start_time": "2024-07-29T02:19:03.184984Z"
    }
   },
   "id": "8887e40e8a4d89c4"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Sparsity per odor, across all (E) neurons\n",
    "def sparsity_per_odor(R):\n",
    "    # Epsilon for if we have zero responses\n",
    "    eps = 1e-6\n",
    "    sp_per_odor = 1 - ((torch.sum(R[:num_e], dim=0) ** 2 + eps) / (num_e * (torch.sum(R[:num_e] ** 2, dim=0)) + eps))\n",
    "    # Sparsity nan means that the responses were all 0 for an odor, meaning that its max sparsity of 1\n",
    "    return sp_per_odor\n",
    "\n",
    "# Sparsity per (E) neuron, across a given odor family\n",
    "def sparsity_per_neuron(R, odor_inds):\n",
    "    sp_per_neuron = 1 - (\n",
    "                (torch.sum(R[:num_e, odor_inds], dim=1) ** 2) / ((P // 2) * torch.sum(R[:num_e, odor_inds] ** 2, dim=1)))\n",
    "    return sp_per_neuron"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.224946Z",
     "start_time": "2024-07-29T02:19:03.188468Z"
    }
   },
   "id": "d4d37265e0e371e8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Try to minimize the correlations between values\n",
    "def loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=True):\n",
    "    corr_sum, avg_corr = odor_corrs(R)\n",
    "    corr_loss = (1 / P) * corr_sum\n",
    "    corr_term = lambda_corr * corr_loss\n",
    "    \n",
    "    means = torch.mean(R[:num_e], dim=0)\n",
    "    means_novel = torch.mean(means[novel_inds])\n",
    "    means_familiar = torch.mean(means[familiar_inds])\n",
    "    if torch.abs(means_novel + means_familiar) < eps:\n",
    "        # All means are the same so there's technically no loss\n",
    "        mu_term = 0\n",
    "    else:\n",
    "        mu_term = lambda_mu * (((means_familiar - means_novel + eps) / (means_novel + means_familiar)) ** 2)\n",
    "    \n",
    "    vars = torch.var(R[:num_e], dim=0)\n",
    "    var_novel = torch.mean(vars[novel_inds])\n",
    "    var_familiar = torch.mean(vars[familiar_inds])\n",
    "    if torch.abs(var_novel + var_familiar) < eps:\n",
    "        # All variances are the same so there's technically no loss\n",
    "        var_term = 0\n",
    "    else:\n",
    "        var_term = lambda_var * (((var_familiar - var_novel) / (var_novel + var_familiar)) ** 2)\n",
    "    \n",
    "    sparsities = sparsity_per_odor(R)\n",
    "    spars_novel = torch.mean(sparsities[novel_inds])\n",
    "    spars_familiar = torch.mean(sparsities[familiar_inds])\n",
    "    if torch.abs(spars_novel + spars_familiar) < eps:\n",
    "        # Sparsities are technically the same so the term shouldn't contribute to loss\n",
    "        spars_term = 0\n",
    "    else:\n",
    "        spars_term = lambda_sp * (((spars_familiar - spars_novel) / (spars_novel + spars_familiar)) ** 2)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Avg Corr: %.4f, Corr: %.4f, Mu: %.4f, Var: %.4f, Sparsity: %.4f\" % (avg_corr, corr_term, mu_term, var_term, spars_term))\n",
    " \n",
    "    loss = corr_term + mu_term + var_term + spars_term\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.225019Z",
     "start_time": "2024-07-29T02:19:03.193312Z"
    }
   },
   "id": "1003ae68e7009d39"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Detaching vs zero grad - should detach because we have a term dependent on the previous model iteration which isn't zero but some constant gradient, accumulated from that model output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.225050Z",
     "start_time": "2024-07-29T02:19:03.195589Z"
    }
   },
   "id": "b7e8cc5982ed854c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def loss_after_odors(ie_model: torch.nn.Sequential, ei_model: torch.nn.Sequential, ie_update_inds, ei_update_inds, W_rec: torch.Tensor, R_current: torch.Tensor, h_bar_ff: torch.Tensor, threshold_mult, plasticity_ie, plasticity_ei, weight_decay_rate, weight_range: tuple, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False):\n",
    "   # First, compute the respective weight updates through the novel and familiar odors from the current neural responses (which are from the current weight matrix)\n",
    "    \n",
    "    W_rec.requires_grad_(True)\n",
    "    \n",
    "    # TODO new paradigm: computing updates across odors and between models in parallel\n",
    "    # TODO regularize by penalizing high model update?\n",
    "    ie_model_updates, ei_model_updates = compute_updates(R_current, (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "   \n",
    "    # with torch.no_grad():\n",
    "    #     val_tensor = ((1 - plasticity_rate * weight_decay_rate) * W_rec[update_inds]) + updates + (plasticity_rate * odor_update)\n",
    "    #     condition = torch.logical_and(torch.gt(val_tensor, min_weight), torch.le(val_tensor, max_weight))\n",
    "    \n",
    "    ie_updates = plasticity_ie * (ie_model_updates - weight_decay_rate * W_rec[ie_update_inds])\n",
    "    ei_updates = plasticity_ei * (ei_model_updates - weight_decay_rate * W_rec[ei_update_inds])\n",
    "    \n",
    "    ie_cond = torch.logical_and(torch.ge(W_rec[ie_update_inds] + ie_updates, weight_range[0][ie_update_inds]), torch.le(W_rec[ie_update_inds] + ie_updates, weight_range[1][ie_update_inds]))\n",
    "    ei_cond = torch.logical_and(torch.ge(W_rec[ei_update_inds] + ei_updates, weight_range[0][ei_update_inds]), torch.le(W_rec[ei_update_inds] + ei_updates, weight_range[1][ei_update_inds]))\n",
    "    \n",
    "    # TODO clamping updates instead of the weights post-update - change in gradient?\n",
    "    ie_bounded_updates = torch.where(ie_cond, ie_updates, 0)\n",
    "    ei_bounded_updates = torch.where(ei_cond, ei_updates, 0)\n",
    "   \n",
    "    # TODO penalize model for causing weight to go over threshold\n",
    "    # Amount below min\n",
    "    ie_a = torch.relu(weight_range[0][ie_update_inds] - (W_rec[ie_update_inds] + ie_updates))\n",
    "    # Amount above max\n",
    "    ie_b = torch.relu((W_rec[ie_update_inds] + ie_updates) - weight_range[1][ie_update_inds])\n",
    "    ie_over_weight = torch.sum(ie_a + ie_b) / len(ie_update_inds)\n",
    "    ei_a = torch.relu(weight_range[0][ei_update_inds] - (W_rec[ei_update_inds] + ei_updates))\n",
    "    # Amount above max\n",
    "    ei_b = torch.relu((W_rec[ei_update_inds] + ei_updates) - weight_range[1][ei_update_inds])\n",
    "    ei_over_weight = torch.sum(ei_a + ei_b) / len(ei_update_inds)\n",
    "    \n",
    "    #W_rec = W_rec.clamp(min=weight_range[0], max=weight_range[1])\n",
    "    \n",
    "    W_rec = torch.index_put(W_rec, ie_update_inds, ie_bounded_updates, accumulate=True)\n",
    "    W_rec = torch.index_put(W_rec, ei_update_inds, ei_bounded_updates, accumulate=True)\n",
    "     \n",
    "    \n",
    "    R = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "    R_new = R\n",
    "    \n",
    "    if detach_grad:\n",
    "        # We want a new R response tensor which only has the previous weight update but not anything before that\n",
    "        W_rec = W_rec.detach()\n",
    "        R_new = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "\n",
    "    if with_loss:\n",
    "        print(f\"IE over: {ie_over_weight}\")\n",
    "        print(f\"EI over: {ei_over_weight}\")\n",
    "        loss = loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp)\n",
    "        overload = ie_over_weight + ei_over_weight\n",
    "    else:\n",
    "        loss = 0\n",
    "        overload = 0\n",
    "    \n",
    "    return loss, overload, W_rec, R_new"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.259925Z",
     "start_time": "2024-07-29T02:19:03.202175Z"
    }
   },
   "id": "1252ae4977d8dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# def verify_initial_activities():\n",
    "#     runs = 100\n",
    "#     avg_corrs = torch.empty((runs,))\n",
    "#     total_losses = torch.empty((runs,))\n",
    "#     for i in range(runs):\n",
    "#         with torch.no_grad():\n",
    "#             a = correlated_mitral_activity()\n",
    "#             h = compute_feedforward_activity(a)\n",
    "#             w = compute_initial_recurrent_weights()\n",
    "#             r = compute_piriform_response(h, w, 0)\n",
    "#         R_adjusted = r[:num_e] - torch.mean(r[:num_e], dim=1, keepdim=True)\n",
    "#         R_adjusted.t_()\n",
    "#         sigma_E = torch.corrcoef(R_adjusted)\n",
    "#         familiar_corrs = sigma_E[P//2:P, P//2:P] - torch.eye(P // 2)\n",
    "#         total_loss = torch.sum(familiar_corrs ** 2)\n",
    "#         avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "#         print(f\"Loss: {total_loss.item()}, Avg Corr: {avg_corr}\")\n",
    "#         total_losses[i] = total_loss\n",
    "#         avg_corrs[i] = avg_corr\n",
    "#     plt.hist(avg_corrs, bins=15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.260040Z",
     "start_time": "2024-07-29T02:19:03.204768Z"
    }
   },
   "id": "70c09c7202ebd3f2"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def get_update_inds(post, pre, W):\n",
    "    weights_slice = W[post[0]:post[1], pre[0]:pre[1]]\n",
    "    inds = torch.nonzero(weights_slice, as_tuple=True)\n",
    "    update_inds = (inds[0] + post[0], inds[1] + pre[0])\n",
    "    \n",
    "    return update_inds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.260068Z",
     "start_time": "2024-07-29T02:19:03.207270Z"
    }
   },
   "id": "22297cdeba801b81"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Number of independent \"sniffs\" of the 16 odors\n",
    "n_inner = 10\n",
    "# Number of different realizations of the odor sniffing process\n",
    "n_outer = 200\n",
    "\n",
    "# TODO change back to -3 if too noisy\n",
    "plasticity_rate = 1e-1\n",
    "# Same plasticity rate for EI and IE\n",
    "plasticity_ie = plasticity_ei = plasticity_rate\n",
    "\n",
    "# TODO experiment w/ diff gradient tracking numbers\n",
    "# Number of inner epochs between model updates, n_update <= n_inner\n",
    "n_update = 10\n",
    "# Number of inner epochs across which the gradient is tracked (right now we detach the gradient after each inner epoch), n_track <= n_inner\n",
    "n_track = n_update\n",
    "\n",
    "# n_track = n_update to test formulation where we track the gradient across a subset of the inner epochs and update the model\n",
    "# If n_update > n_track, the model will have multiple \"gradient\" paths of loss accumulated: ex. inner epochs 1-5 and a separate branch of 6-10\n",
    "# If n_update < n_track, it doesn't matter b/c the model will truncate the history past its previous update since the gradient is zeroed\n",
    "\n",
    "# Number of standard deviations from mean, we are trying 0 b/c 1 and 2 is too sparse\n",
    "threshold_multiplier = 0\n",
    "\n",
    "# TODO for now go to simple formulation, no weight decay\n",
    "weight_decay = 0\n",
    "# How much to weight each of the regularization terms\n",
    "# Sparsity = 1000 doesn't improve loss ratios\n",
    "# Sparsity = 100, 500 epochs, loss ratios 1.0-1.8, blows up sparsity\n",
    "# Go back to 500 epochs no sparsity reg, w/ new nan clipping formulation\n",
    "lambda_corr, lambda_mu, lambda_var, lambda_sp = 1, 0, 0, 0\n",
    "\n",
    "ie_model = create_model()\n",
    "ie_model.to(gpu)\n",
    "ei_model = create_model()\n",
    "ei_model.to(gpu)\n",
    "\n",
    "\n",
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def train_model():\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # TODO try LR -3\n",
    "    ie_optim = optim.SGD(ie_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    ei_optim = optim.SGD(ei_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    updates_per_outer = n_inner // n_update\n",
    "    num_losses = int(updates_per_outer * n_outer)\n",
    "    losses = torch.empty(size=(num_losses,))\n",
    "    # batch every 20 (ex. tried 50 and was slightly more noisy (but w/ similar mean))\n",
    "    # batch_every = 20\n",
    "    # batch_loss = 0\n",
    "    \n",
    "    for outer_e in range(n_outer):\n",
    "        i = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(i)\n",
    "        \n",
    "        W_initial = compute_initial_recurrent_weights()\n",
    "        W = W_initial.clone().to(gpu)\n",
    "        W.requires_grad_(True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            \n",
    "            clamp_min = torch.zeros_like(W)\n",
    "            clamp_min[ei_update_inds] = ei_min_weight\n",
    "            clamp_min[ie_update_inds] = ie_min_weight\n",
    "            clamp_max = torch.zeros_like(W)\n",
    "            clamp_max[ie_update_inds] = ie_max_weight\n",
    "            clamp_max[ei_update_inds] = ei_max_weight\n",
    "            weight_range = (clamp_min, clamp_max)\n",
    "        \n",
    "        # Initial neuron responses\n",
    "        R = compute_piriform_response(hbar_ff, W, threshold_multiplier)\n",
    "        \n",
    "        # TODO 10 inner epochs, small plasticity rate\n",
    "        for i in range(1, n_inner + 1):\n",
    "            with_loss = False\n",
    "            detach_grad = False\n",
    "            if i % n_update == 0:\n",
    "                with_loss = True\n",
    "                print(f\"Outer epoch {outer_e}, Inner epoch {i}, Loss: \\t\", end=\"\")\n",
    "            if i % n_track == 0:\n",
    "                detach_grad = True\n",
    "            \n",
    "            loss, overload, W, R = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W, R, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=detach_grad, with_loss=with_loss)\n",
    "            \n",
    "            if with_loss:\n",
    "                final_loss = loss\n",
    "\n",
    "                R_initial = compute_piriform_response(hbar_ff, W_initial, threshold_multiplier)\n",
    "                initial_loss = loss_fn(R_initial, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=False)\n",
    "                total_loss = final_loss / initial_loss\n",
    "                total_loss += overload\n",
    "                print(f\"Loss ratio: {total_loss}\")\n",
    "                losses[(outer_e * updates_per_outer)  + (i // n_update) - 1] = total_loss.item()\n",
    "                total_loss.backward()\n",
    "                #batch_loss += total_loss\n",
    "                # ie_grad = torch.nn.utils.clip_grad_norm_(ie_model.parameters(), max_norm = 1e5)\n",
    "                # ei_grad = torch.nn.utils.clip_grad_norm_(ei_model.parameters(), max_norm = 1e5)\n",
    "                # print(f\"ie model grad: {ie_grad}\")\n",
    "                # print(f\"ei model grad: {ei_grad}\")\n",
    "                ie_optim.step()  \n",
    "                ei_optim.step()\n",
    "                ie_optim.zero_grad()\n",
    "                ei_optim.zero_grad()\n",
    "            \n",
    "        # if outer_e % batch_every == 0:\n",
    "        #     batch_loss.backward()   \n",
    "        #     ie_optim.step()\n",
    "        #     ei_optim.step()\n",
    "        #     ie_optim.zero_grad()\n",
    "        #     ei_optim.zero_grad()\n",
    "        #     batch_loss = 0\n",
    "                \n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:19:03.292968Z",
     "start_time": "2024-07-29T02:19:03.209131Z"
    }
   },
   "id": "70723f2b6444f1b2"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer epoch 0, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 14416.55078125\n",
      "Avg Corr: 0.3860, Corr: 1.0614, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14418.380859375\n",
      "Outer epoch 1, Inner epoch 10, Loss: \tIE over: 6743152.0\n",
      "EI over: 27972917248.0\n",
      "Avg Corr: 0.2776, Corr: 0.4714, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 27979661312.0\n",
      "Outer epoch 2, Inner epoch 10, Loss: \tIE over: 122989.1875\n",
      "EI over: 64230744.0\n",
      "Avg Corr: 0.2992, Corr: 0.5847, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 64353732.0\n",
      "Outer epoch 3, Inner epoch 10, Loss: \tIE over: 325341.375\n",
      "EI over: 61741992.0\n",
      "Avg Corr: 0.3005, Corr: 0.5819, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 62067332.0\n",
      "Outer epoch 4, Inner epoch 10, Loss: \tIE over: 34245392.0\n",
      "EI over: 37992656.0\n",
      "Avg Corr: 0.3098, Corr: 0.5938, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 72238048.0\n",
      "Outer epoch 5, Inner epoch 10, Loss: \tIE over: 19265.013671875\n",
      "EI over: 22996488192.0\n",
      "Avg Corr: 0.3021, Corr: 0.5651, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22996506624.0\n",
      "Outer epoch 6, Inner epoch 10, Loss: \tIE over: 23729.3671875\n",
      "EI over: 17217524.0\n",
      "Avg Corr: 0.2439, Corr: 0.3551, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 17241254.0\n",
      "Outer epoch 7, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 11224320.0\n",
      "Avg Corr: 0.2965, Corr: 0.5548, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11224321.0\n",
      "Outer epoch 8, Inner epoch 10, Loss: \tIE over: 223424.0\n",
      "EI over: 12992395.0\n",
      "Avg Corr: 0.2793, Corr: 0.5264, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13215820.0\n",
      "Outer epoch 9, Inner epoch 10, Loss: \tIE over: 174910.0625\n",
      "EI over: 15799970.0\n",
      "Avg Corr: 0.3075, Corr: 0.5716, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 15974881.0\n",
      "Outer epoch 10, Inner epoch 10, Loss: \tIE over: 109096.546875\n",
      "EI over: 581459.875\n",
      "Avg Corr: 0.2677, Corr: 0.3896, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 690557.125\n",
      "Outer epoch 11, Inner epoch 10, Loss: \tIE over: 901915.8125\n",
      "EI over: 2011728.875\n",
      "Avg Corr: 0.2860, Corr: 0.5063, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 2913645.75\n",
      "Outer epoch 12, Inner epoch 10, Loss: \tIE over: 1551931.875\n",
      "EI over: 12956764.0\n",
      "Avg Corr: 0.3726, Corr: 0.7983, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14508697.0\n",
      "Outer epoch 13, Inner epoch 10, Loss: \tIE over: 1887114.25\n",
      "EI over: 7307009.0\n",
      "Avg Corr: 0.3018, Corr: 0.5198, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 9194124.0\n",
      "Outer epoch 14, Inner epoch 10, Loss: \tIE over: 1969377.0\n",
      "EI over: 16710676.0\n",
      "Avg Corr: 0.2920, Corr: 0.5711, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 18680052.0\n",
      "Outer epoch 15, Inner epoch 10, Loss: \tIE over: 1730920.25\n",
      "EI over: 19147092.0\n",
      "Avg Corr: 0.3783, Corr: 0.7617, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 20878012.0\n",
      "Outer epoch 16, Inner epoch 10, Loss: \tIE over: 1277912.0\n",
      "EI over: 559427.75\n",
      "Avg Corr: 0.2800, Corr: 0.4675, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1837340.75\n",
      "Outer epoch 17, Inner epoch 10, Loss: \tIE over: 622080.9375\n",
      "EI over: 35055008.0\n",
      "Avg Corr: 0.3179, Corr: 0.6373, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 35677088.0\n",
      "Outer epoch 18, Inner epoch 10, Loss: \tIE over: 159032.125\n",
      "EI over: 48022544.0\n",
      "Avg Corr: 0.2712, Corr: 0.4606, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 48181576.0\n",
      "Outer epoch 19, Inner epoch 10, Loss: \tIE over: 134760.59375\n",
      "EI over: 38859776.0\n",
      "Avg Corr: 0.2485, Corr: 0.4158, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 38994536.0\n",
      "Outer epoch 20, Inner epoch 10, Loss: \tIE over: 555163.25\n",
      "EI over: 9840142.0\n",
      "Avg Corr: 0.3328, Corr: 0.6431, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 10395306.0\n",
      "Outer epoch 21, Inner epoch 10, Loss: \tIE over: 989665.875\n",
      "EI over: 35076516.0\n",
      "Avg Corr: 0.3064, Corr: 0.6585, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 36066180.0\n",
      "Outer epoch 22, Inner epoch 10, Loss: \tIE over: 1120322.75\n",
      "EI over: 56575952.0\n",
      "Avg Corr: 0.3445, Corr: 0.7183, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57696276.0\n",
      "Outer epoch 23, Inner epoch 10, Loss: \tIE over: 992921.125\n",
      "EI over: 54851704.0\n",
      "Avg Corr: 0.4379, Corr: 0.9930, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 55844624.0\n",
      "Outer epoch 24, Inner epoch 10, Loss: \tIE over: 631367.3125\n",
      "EI over: 32729168.0\n",
      "Avg Corr: 0.3055, Corr: 0.5042, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 33360536.0\n",
      "Outer epoch 25, Inner epoch 10, Loss: \tIE over: 46695.375\n",
      "EI over: 6136091.0\n",
      "Avg Corr: 0.2862, Corr: 0.5113, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 6182787.5\n",
      "Outer epoch 26, Inner epoch 10, Loss: \tIE over: 239128.09375\n",
      "EI over: 21959780.0\n",
      "Avg Corr: 0.3043, Corr: 0.6087, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22198908.0\n",
      "Outer epoch 27, Inner epoch 10, Loss: \tIE over: 682091.625\n",
      "EI over: 15619922.0\n",
      "Avg Corr: 0.3062, Corr: 0.6005, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16302015.0\n",
      "Outer epoch 28, Inner epoch 10, Loss: \tIE over: 837501.125\n",
      "EI over: 9135667.0\n",
      "Avg Corr: 0.3030, Corr: 0.6032, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 9973169.0\n",
      "Outer epoch 29, Inner epoch 10, Loss: \tIE over: 711121.0\n",
      "EI over: 12370182.0\n",
      "Avg Corr: 0.2942, Corr: 0.5191, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13081304.0\n",
      "Outer epoch 30, Inner epoch 10, Loss: \tIE over: 336057.5\n",
      "EI over: 3638221.75\n",
      "Avg Corr: 0.2581, Corr: 0.4211, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 3974280.25\n",
      "Outer epoch 31, Inner epoch 10, Loss: \tIE over: 238871.25\n",
      "EI over: 137976.875\n",
      "Avg Corr: 0.1174, Corr: 0.1081, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 376848.40625\n",
      "Outer epoch 32, Inner epoch 10, Loss: \tIE over: 1057387.0\n",
      "EI over: 36469240.0\n",
      "Avg Corr: 0.3530, Corr: 0.7723, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37526628.0\n",
      "Outer epoch 33, Inner epoch 10, Loss: \tIE over: 1958025.5\n",
      "EI over: 49075224.0\n",
      "Avg Corr: 0.2640, Corr: 0.4816, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 51033248.0\n",
      "Outer epoch 34, Inner epoch 10, Loss: \tIE over: 2547899.0\n",
      "EI over: 39914760.0\n",
      "Avg Corr: 0.2855, Corr: 0.4847, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 42462660.0\n",
      "Outer epoch 35, Inner epoch 10, Loss: \tIE over: 2759210.0\n",
      "EI over: 10959132.0\n",
      "Avg Corr: 0.2386, Corr: 0.3993, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13718343.0\n",
      "Outer epoch 36, Inner epoch 10, Loss: \tIE over: 2773184.0\n",
      "EI over: 34269444.0\n",
      "Avg Corr: 0.2973, Corr: 0.5287, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37042628.0\n",
      "Outer epoch 37, Inner epoch 10, Loss: \tIE over: 2483592.5\n",
      "EI over: 55559388.0\n",
      "Avg Corr: 0.3389, Corr: 0.7430, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 58042980.0\n",
      "Outer epoch 38, Inner epoch 10, Loss: \tIE over: 1987182.0\n",
      "EI over: 54068936.0\n",
      "Avg Corr: 0.2887, Corr: 0.5165, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 56056120.0\n",
      "Outer epoch 39, Inner epoch 10, Loss: \tIE over: 1270785.75\n",
      "EI over: 31944736.0\n",
      "Avg Corr: 0.2580, Corr: 0.4274, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 33215524.0\n",
      "Outer epoch 40, Inner epoch 10, Loss: \tIE over: 362988.09375\n",
      "EI over: 6589650.0\n",
      "Avg Corr: 0.3315, Corr: 0.7614, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 6952639.0\n",
      "Outer epoch 41, Inner epoch 10, Loss: \tIE over: 204251.890625\n",
      "EI over: 22309120.0\n",
      "Avg Corr: 0.3007, Corr: 0.5458, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22513372.0\n",
      "Outer epoch 42, Inner epoch 10, Loss: \tIE over: 914398.25\n",
      "EI over: 15694776.0\n",
      "Avg Corr: 0.3217, Corr: 0.5936, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16609175.0\n",
      "Outer epoch 43, Inner epoch 10, Loss: \tIE over: 1296820.875\n",
      "EI over: 9174012.0\n",
      "Avg Corr: 0.3486, Corr: 0.7522, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 10470834.0\n",
      "Outer epoch 44, Inner epoch 10, Loss: \tIE over: 1377909.25\n",
      "EI over: 12412052.0\n",
      "Avg Corr: 0.3304, Corr: 0.6955, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13789962.0\n",
      "Outer epoch 45, Inner epoch 10, Loss: \tIE over: 1181769.0\n",
      "EI over: 3528208.5\n",
      "Avg Corr: 0.2795, Corr: 0.4835, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 4709978.5\n",
      "Outer epoch 46, Inner epoch 10, Loss: \tIE over: 758346.0\n",
      "EI over: 563648.625\n",
      "Avg Corr: 0.2595, Corr: 0.4229, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1321995.375\n",
      "Outer epoch 47, Inner epoch 10, Loss: \tIE over: 138489.03125\n",
      "EI over: 36009088.0\n",
      "Avg Corr: 0.2316, Corr: 0.3282, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 36147576.0\n",
      "Outer epoch 48, Inner epoch 10, Loss: \tIE over: 183981.3125\n",
      "EI over: 48614540.0\n",
      "Avg Corr: 0.2776, Corr: 0.4577, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 48798520.0\n",
      "Outer epoch 49, Inner epoch 10, Loss: \tIE over: 666178.375\n",
      "EI over: 39412140.0\n",
      "Avg Corr: 0.2770, Corr: 0.4297, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 40078320.0\n",
      "Outer epoch 50, Inner epoch 10, Loss: \tIE over: 842355.1875\n",
      "EI over: 10159026.0\n",
      "Avg Corr: 0.3059, Corr: 0.5941, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11001382.0\n",
      "Outer epoch 51, Inner epoch 10, Loss: \tIE over: 762939.8125\n",
      "EI over: 34950376.0\n",
      "Avg Corr: 0.2459, Corr: 0.3808, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 35713316.0\n",
      "Outer epoch 52, Inner epoch 10, Loss: \tIE over: 423503.375\n",
      "EI over: 56603808.0\n",
      "Avg Corr: 0.2707, Corr: 0.4832, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57027312.0\n",
      "Outer epoch 53, Inner epoch 10, Loss: \tIE over: 22894.822265625\n",
      "EI over: 55231704.0\n",
      "Avg Corr: 0.2520, Corr: 0.3783, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 55254600.0\n",
      "Outer epoch 54, Inner epoch 10, Loss: \tIE over: 378272.96875\n",
      "EI over: 33356684.0\n",
      "Avg Corr: 0.2518, Corr: 0.4217, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 33734956.0\n",
      "Outer epoch 55, Inner epoch 10, Loss: \tIE over: 580573.875\n",
      "EI over: 5283736.5\n",
      "Avg Corr: 0.3175, Corr: 0.5949, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5864311.5\n",
      "Outer epoch 56, Inner epoch 10, Loss: \tIE over: 511964.4375\n",
      "EI over: 20866312.0\n",
      "Avg Corr: 0.2916, Corr: 0.5161, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 21378276.0\n",
      "Outer epoch 57, Inner epoch 10, Loss: \tIE over: 187765.625\n",
      "EI over: 14314060.0\n",
      "Avg Corr: 0.2649, Corr: 0.4823, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14501827.0\n",
      "Outer epoch 58, Inner epoch 10, Loss: \tIE over: 211014.0\n",
      "EI over: 10617840.0\n",
      "Avg Corr: 0.2651, Corr: 0.4267, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 10828855.0\n",
      "Outer epoch 59, Inner epoch 10, Loss: \tIE over: 169897.5625\n",
      "EI over: 13828048.0\n",
      "Avg Corr: 0.2720, Corr: 0.4507, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13997947.0\n",
      "Outer epoch 60, Inner epoch 10, Loss: \tIE over: 199853.53125\n",
      "EI over: 2141260.5\n",
      "Avg Corr: 0.3900, Corr: 0.8324, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 2341115.0\n",
      "Outer epoch 61, Inner epoch 10, Loss: \tIE over: 416969.625\n",
      "EI over: 826755.125\n",
      "Avg Corr: 0.2574, Corr: 0.4514, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1243725.75\n",
      "Outer epoch 62, Inner epoch 10, Loss: \tIE over: 359892.0625\n",
      "EI over: 13824878.0\n",
      "Avg Corr: 0.2917, Corr: 0.5150, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14184771.0\n",
      "Outer epoch 63, Inner epoch 10, Loss: \tIE over: 60454.796875\n",
      "EI over: 7947913.5\n",
      "Avg Corr: 0.2218, Corr: 0.3705, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8008369.5\n",
      "Outer epoch 64, Inner epoch 10, Loss: \tIE over: 428546.5625\n",
      "EI over: 16305792.0\n",
      "Avg Corr: 0.2142, Corr: 0.2789, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16734340.0\n",
      "Outer epoch 65, Inner epoch 10, Loss: \tIE over: 350276.3125\n",
      "EI over: 19195856.0\n",
      "Avg Corr: 0.2802, Corr: 0.4628, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 19546132.0\n",
      "Outer epoch 66, Inner epoch 10, Loss: \tIE over: 119996.1015625\n",
      "EI over: 921375.375\n",
      "Avg Corr: 0.2887, Corr: 0.5379, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1041372.5\n",
      "Outer epoch 67, Inner epoch 10, Loss: \tIE over: 358057.125\n",
      "EI over: 34422920.0\n",
      "Avg Corr: 0.2744, Corr: 0.4445, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 34780976.0\n",
      "Outer epoch 68, Inner epoch 10, Loss: \tIE over: 323130.4375\n",
      "EI over: 47303440.0\n",
      "Avg Corr: 0.3116, Corr: 0.5977, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 47626572.0\n",
      "Outer epoch 69, Inner epoch 10, Loss: \tIE over: 30433.056640625\n",
      "EI over: 38087992.0\n",
      "Avg Corr: 0.2524, Corr: 0.4019, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 38118424.0\n",
      "Outer epoch 70, Inner epoch 10, Loss: \tIE over: 467006.15625\n",
      "EI over: 8862936.0\n",
      "Avg Corr: 0.2476, Corr: 0.4527, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 9329943.0\n",
      "Outer epoch 71, Inner epoch 10, Loss: \tIE over: 372953.625\n",
      "EI over: 36260696.0\n",
      "Avg Corr: 0.4369, Corr: 1.1201, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 36633648.0\n",
      "Outer epoch 72, Inner epoch 10, Loss: \tIE over: 116464.140625\n",
      "EI over: 57608524.0\n",
      "Avg Corr: 0.3341, Corr: 0.6573, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57724988.0\n",
      "Outer epoch 73, Inner epoch 10, Loss: \tIE over: 358552.3125\n",
      "EI over: 56264936.0\n",
      "Avg Corr: 0.2906, Corr: 0.4783, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 56623488.0\n",
      "Outer epoch 74, Inner epoch 10, Loss: \tIE over: 320757.0625\n",
      "EI over: 34204392.0\n",
      "Avg Corr: 0.2635, Corr: 0.4083, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 34525148.0\n",
      "Outer epoch 75, Inner epoch 10, Loss: \tIE over: 37970.3671875\n",
      "EI over: 4463857.0\n",
      "Avg Corr: 0.2679, Corr: 0.4762, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 4501828.5\n",
      "Outer epoch 76, Inner epoch 10, Loss: \tIE over: 442482.125\n",
      "EI over: 20157888.0\n",
      "Avg Corr: 0.2312, Corr: 0.3146, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 20600370.0\n",
      "Outer epoch 77, Inner epoch 10, Loss: \tIE over: 374758.0625\n",
      "EI over: 13632001.0\n",
      "Avg Corr: 0.3025, Corr: 0.5970, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14006760.0\n",
      "Outer epoch 78, Inner epoch 10, Loss: \tIE over: 111359.4375\n",
      "EI over: 11105426.0\n",
      "Avg Corr: 0.3276, Corr: 0.6162, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11216786.0\n",
      "Outer epoch 79, Inner epoch 10, Loss: \tIE over: 349050.875\n",
      "EI over: 14392692.0\n",
      "Avg Corr: 0.3135, Corr: 0.5990, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14741744.0\n",
      "Outer epoch 80, Inner epoch 10, Loss: \tIE over: 315024.5\n",
      "EI over: 1674443.75\n",
      "Avg Corr: 0.2718, Corr: 0.4508, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1989469.25\n",
      "Outer epoch 81, Inner epoch 10, Loss: \tIE over: 22105.412109375\n",
      "EI over: 1176910.0\n",
      "Avg Corr: 0.3259, Corr: 0.6689, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1199016.375\n",
      "Outer epoch 82, Inner epoch 10, Loss: \tIE over: 488757.3125\n",
      "EI over: 13616937.0\n",
      "Avg Corr: 0.2475, Corr: 0.4029, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14105695.0\n",
      "Outer epoch 83, Inner epoch 10, Loss: \tIE over: 408484.03125\n",
      "EI over: 7773218.5\n",
      "Avg Corr: 0.2764, Corr: 0.5385, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8181703.5\n",
      "Outer epoch 84, Inner epoch 10, Loss: \tIE over: 89093.59375\n",
      "EI over: 16429570.0\n",
      "Avg Corr: 0.2988, Corr: 0.5054, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16518665.0\n",
      "Outer epoch 85, Inner epoch 10, Loss: \tIE over: 319131.875\n",
      "EI over: 19199548.0\n",
      "Avg Corr: 0.3395, Corr: 0.7234, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 19518682.0\n",
      "Outer epoch 86, Inner epoch 10, Loss: \tIE over: 278798.5\n",
      "EI over: 874629.875\n",
      "Avg Corr: 0.2560, Corr: 0.4246, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1153429.375\n",
      "Outer epoch 87, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 34497096.0\n",
      "Avg Corr: 0.2948, Corr: 0.5419, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 34497096.0\n",
      "Outer epoch 88, Inner epoch 10, Loss: \tIE over: 33710.4140625\n",
      "EI over: 47195556.0\n",
      "Avg Corr: 0.3017, Corr: 0.6481, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 47229268.0\n",
      "Outer epoch 89, Inner epoch 10, Loss: \tIE over: 829.9490966796875\n",
      "EI over: 37842880.0\n",
      "Avg Corr: 0.3309, Corr: 0.6460, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37843708.0\n",
      "Outer epoch 90, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 8836477.0\n",
      "Avg Corr: 0.2652, Corr: 0.4291, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8836478.0\n",
      "Outer epoch 91, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 36244056.0\n",
      "Avg Corr: 0.2515, Corr: 0.3908, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 36244056.0\n",
      "Outer epoch 92, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 57545920.0\n",
      "Avg Corr: 0.3140, Corr: 0.5717, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57545920.0\n",
      "Outer epoch 93, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 56226020.0\n",
      "Avg Corr: 0.3239, Corr: 0.6233, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 56226020.0\n",
      "Outer epoch 94, Inner epoch 10, Loss: \tIE over: 0.0\n",
      "EI over: 34236308.0\n",
      "Avg Corr: 0.2995, Corr: 0.5307, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 34236308.0\n",
      "Outer epoch 95, Inner epoch 10, Loss: \tIE over: 10516.458984375\n",
      "EI over: 4564615.5\n",
      "Avg Corr: 0.2912, Corr: 0.5439, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 4575133.0\n",
      "Outer epoch 96, Inner epoch 10, Loss: \tIE over: 2236231.25\n",
      "EI over: 20249884.0\n",
      "Avg Corr: 0.3277, Corr: 0.6501, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22486118.0\n",
      "Outer epoch 97, Inner epoch 10, Loss: \tIE over: 4092010.5\n",
      "EI over: 13693456.0\n",
      "Avg Corr: 0.3944, Corr: 0.8736, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 17785466.0\n",
      "Outer epoch 98, Inner epoch 10, Loss: \tIE over: 5415861.0\n",
      "EI over: 11120692.0\n",
      "Avg Corr: 0.2613, Corr: 0.4664, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16536554.0\n",
      "Outer epoch 99, Inner epoch 10, Loss: \tIE over: 6394032.5\n",
      "EI over: 14391334.0\n",
      "Avg Corr: 0.3288, Corr: 0.6542, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 20785368.0\n",
      "Outer epoch 100, Inner epoch 10, Loss: \tIE over: 6979423.0\n",
      "EI over: 1544877.125\n",
      "Avg Corr: 0.3457, Corr: 0.7672, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8524301.0\n",
      "Outer epoch 101, Inner epoch 10, Loss: \tIE over: 7323132.5\n",
      "EI over: 1453621.375\n",
      "Avg Corr: 0.2770, Corr: 0.5449, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8776755.0\n",
      "Outer epoch 102, Inner epoch 10, Loss: \tIE over: 7404585.5\n",
      "EI over: 13132594.0\n",
      "Avg Corr: 0.2770, Corr: 0.4930, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 20537180.0\n",
      "Outer epoch 103, Inner epoch 10, Loss: \tIE over: 7093845.5\n",
      "EI over: 7274722.0\n",
      "Avg Corr: 0.3440, Corr: 0.7231, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14368569.0\n",
      "Outer epoch 104, Inner epoch 10, Loss: \tIE over: 6728571.0\n",
      "EI over: 17040286.0\n",
      "Avg Corr: 0.2816, Corr: 0.4669, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 23768858.0\n",
      "Outer epoch 105, Inner epoch 10, Loss: \tIE over: 6039844.0\n",
      "EI over: 19792582.0\n",
      "Avg Corr: 0.3373, Corr: 0.7095, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 25832428.0\n",
      "Outer epoch 106, Inner epoch 10, Loss: \tIE over: 5186400.5\n",
      "EI over: 1452237.0\n",
      "Avg Corr: 0.3207, Corr: 0.6607, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 6638638.5\n",
      "Outer epoch 107, Inner epoch 10, Loss: \tIE over: 4115837.5\n",
      "EI over: 33893220.0\n",
      "Avg Corr: 0.3132, Corr: 0.5950, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 38009056.0\n",
      "Outer epoch 108, Inner epoch 10, Loss: \tIE over: 2929325.5\n",
      "EI over: 46720980.0\n",
      "Avg Corr: 0.2860, Corr: 0.5846, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 49650304.0\n",
      "Outer epoch 109, Inner epoch 10, Loss: \tIE over: 1549634.25\n",
      "EI over: 37513056.0\n",
      "Avg Corr: 0.2574, Corr: 0.4177, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 39062692.0\n",
      "Outer epoch 110, Inner epoch 10, Loss: \tIE over: 114209.546875\n",
      "EI over: 8472079.0\n",
      "Avg Corr: 0.3307, Corr: 0.6450, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8586290.0\n",
      "Outer epoch 111, Inner epoch 10, Loss: \tIE over: 944516.0\n",
      "EI over: 36534048.0\n",
      "Avg Corr: 0.2983, Corr: 0.5392, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37478564.0\n",
      "Outer epoch 112, Inner epoch 10, Loss: \tIE over: 2093330.875\n",
      "EI over: 58124544.0\n",
      "Avg Corr: 0.3139, Corr: 0.7049, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 60217876.0\n",
      "Outer epoch 113, Inner epoch 10, Loss: \tIE over: 2860284.25\n",
      "EI over: 56774136.0\n",
      "Avg Corr: 0.2404, Corr: 0.3686, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 59634420.0\n",
      "Outer epoch 114, Inner epoch 10, Loss: \tIE over: 3349765.75\n",
      "EI over: 34577752.0\n",
      "Avg Corr: 0.2636, Corr: 0.4377, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37927516.0\n",
      "Outer epoch 115, Inner epoch 10, Loss: \tIE over: 3502297.25\n",
      "EI over: 4177487.0\n",
      "Avg Corr: 0.3080, Corr: 0.5782, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 7679785.0\n",
      "Outer epoch 116, Inner epoch 10, Loss: \tIE over: 3380648.0\n",
      "EI over: 19925972.0\n",
      "Avg Corr: 0.3599, Corr: 0.7444, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 23306622.0\n",
      "Outer epoch 117, Inner epoch 10, Loss: \tIE over: 2960486.0\n",
      "EI over: 13280580.0\n",
      "Avg Corr: 0.3802, Corr: 0.8314, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16241067.0\n",
      "Outer epoch 118, Inner epoch 10, Loss: \tIE over: 2401164.75\n",
      "EI over: 11660646.0\n",
      "Avg Corr: 0.3114, Corr: 0.5580, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14061812.0\n",
      "Outer epoch 119, Inner epoch 10, Loss: \tIE over: 1627066.5\n",
      "EI over: 14881764.0\n",
      "Avg Corr: 0.2914, Corr: 0.5155, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16508831.0\n",
      "Outer epoch 120, Inner epoch 10, Loss: \tIE over: 678065.25\n",
      "EI over: 1134560.75\n",
      "Avg Corr: 0.2749, Corr: 0.4923, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1812627.0\n",
      "Outer epoch 121, Inner epoch 10, Loss: \tIE over: 20899.693359375\n",
      "EI over: 1756546.875\n",
      "Avg Corr: 0.2850, Corr: 0.4890, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1777447.5\n",
      "Outer epoch 122, Inner epoch 10, Loss: \tIE over: 3549646.5\n",
      "EI over: 12932326.0\n",
      "Avg Corr: 0.2893, Corr: 0.5463, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16481973.0\n",
      "Outer epoch 123, Inner epoch 10, Loss: \tIE over: 6455702.0\n",
      "EI over: 7058514.0\n",
      "Avg Corr: 0.2701, Corr: 0.5077, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13514217.0\n",
      "Outer epoch 124, Inner epoch 10, Loss: \tIE over: 8855751.0\n",
      "EI over: 17014366.0\n",
      "Avg Corr: 0.2384, Corr: 0.3715, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 25870116.0\n",
      "Outer epoch 125, Inner epoch 10, Loss: \tIE over: 10760559.0\n",
      "EI over: 19672864.0\n",
      "Avg Corr: 0.3460, Corr: 0.7221, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 30433424.0\n",
      "Outer epoch 126, Inner epoch 10, Loss: \tIE over: 12308794.0\n",
      "EI over: 1316078.5\n",
      "Avg Corr: 0.3230, Corr: 0.6654, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13624873.0\n",
      "Outer epoch 127, Inner epoch 10, Loss: \tIE over: 13312842.0\n",
      "EI over: 34226048.0\n",
      "Avg Corr: 0.2804, Corr: 0.5360, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 47538888.0\n",
      "Outer epoch 128, Inner epoch 10, Loss: \tIE over: 14025188.0\n",
      "EI over: 47147408.0\n",
      "Avg Corr: 0.3058, Corr: 0.5392, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 61172596.0\n",
      "Outer epoch 129, Inner epoch 10, Loss: \tIE over: 14527442.0\n",
      "EI over: 37769952.0\n",
      "Avg Corr: 0.3733, Corr: 0.7887, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 52297392.0\n",
      "Outer epoch 130, Inner epoch 10, Loss: \tIE over: 14578540.0\n",
      "EI over: 8593610.0\n",
      "Avg Corr: 0.2889, Corr: 0.4895, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 23172152.0\n",
      "Outer epoch 131, Inner epoch 10, Loss: \tIE over: 14368621.0\n",
      "EI over: 36480808.0\n",
      "Avg Corr: 0.3601, Corr: 0.7641, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 50849428.0\n",
      "Outer epoch 132, Inner epoch 10, Loss: \tIE over: 13782790.0\n",
      "EI over: 58021244.0\n",
      "Avg Corr: 0.2940, Corr: 0.5246, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 71804032.0\n",
      "Outer epoch 133, Inner epoch 10, Loss: \tIE over: 13318004.0\n",
      "EI over: 56470112.0\n",
      "Avg Corr: 0.2858, Corr: 0.5024, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 69788112.0\n",
      "Outer epoch 134, Inner epoch 10, Loss: \tIE over: 12418548.0\n",
      "EI over: 34617624.0\n",
      "Avg Corr: 0.2996, Corr: 0.5647, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 47036172.0\n",
      "Outer epoch 135, Inner epoch 10, Loss: \tIE over: 11448700.0\n",
      "EI over: 4081070.0\n",
      "Avg Corr: 0.2826, Corr: 0.4760, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 15529771.0\n",
      "Outer epoch 136, Inner epoch 10, Loss: \tIE over: 10231644.0\n",
      "EI over: 19764284.0\n",
      "Avg Corr: 0.2873, Corr: 0.5092, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 29995930.0\n",
      "Outer epoch 137, Inner epoch 10, Loss: \tIE over: 8933379.0\n",
      "EI over: 13321076.0\n",
      "Avg Corr: 0.3561, Corr: 0.7467, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22254456.0\n",
      "Outer epoch 138, Inner epoch 10, Loss: \tIE over: 7518251.0\n",
      "EI over: 11616416.0\n",
      "Avg Corr: 0.2976, Corr: 0.6147, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 19134668.0\n",
      "Outer epoch 139, Inner epoch 10, Loss: \tIE over: 5945917.0\n",
      "EI over: 14731100.0\n",
      "Avg Corr: 0.2499, Corr: 0.4138, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 20677016.0\n",
      "Outer epoch 140, Inner epoch 10, Loss: \tIE over: 4375838.0\n",
      "EI over: 1300817.375\n",
      "Avg Corr: 0.3245, Corr: 0.6666, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5676656.5\n",
      "Outer epoch 141, Inner epoch 10, Loss: \tIE over: 2641349.0\n",
      "EI over: 1624149.125\n",
      "Avg Corr: 0.3180, Corr: 0.6050, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 4265499.0\n",
      "Outer epoch 142, Inner epoch 10, Loss: \tIE over: 842412.25\n",
      "EI over: 12966325.0\n",
      "Avg Corr: 0.3562, Corr: 0.7070, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 13808738.0\n",
      "Outer epoch 143, Inner epoch 10, Loss: \tIE over: 544534.125\n",
      "EI over: 7054607.0\n",
      "Avg Corr: 0.2534, Corr: 0.4072, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 7599142.0\n",
      "Outer epoch 144, Inner epoch 10, Loss: \tIE over: 1973400.75\n",
      "EI over: 17087316.0\n",
      "Avg Corr: 0.2838, Corr: 0.5234, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 19060716.0\n",
      "Outer epoch 145, Inner epoch 10, Loss: \tIE over: 3032323.0\n",
      "EI over: 19802168.0\n",
      "Avg Corr: 0.2673, Corr: 0.4389, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22834492.0\n",
      "Outer epoch 146, Inner epoch 10, Loss: \tIE over: 3661970.0\n",
      "EI over: 1509958.25\n",
      "Avg Corr: 0.3207, Corr: 0.6051, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5171929.0\n",
      "Outer epoch 147, Inner epoch 10, Loss: \tIE over: 4043387.75\n",
      "EI over: 33974128.0\n",
      "Avg Corr: 0.2790, Corr: 0.4614, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 38017516.0\n",
      "Outer epoch 148, Inner epoch 10, Loss: \tIE over: 4086847.5\n",
      "EI over: 46683912.0\n",
      "Avg Corr: 0.3154, Corr: 0.6065, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 50770760.0\n",
      "Outer epoch 149, Inner epoch 10, Loss: \tIE over: 3895833.5\n",
      "EI over: 37431640.0\n",
      "Avg Corr: 0.3659, Corr: 0.8123, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 41327472.0\n",
      "Outer epoch 150, Inner epoch 10, Loss: \tIE over: 3503861.25\n",
      "EI over: 8381220.0\n",
      "Avg Corr: 0.3258, Corr: 0.6491, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11885082.0\n",
      "Outer epoch 151, Inner epoch 10, Loss: \tIE over: 2853215.0\n",
      "EI over: 36614136.0\n",
      "Avg Corr: 0.3076, Corr: 0.6212, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 39467352.0\n",
      "Outer epoch 152, Inner epoch 10, Loss: \tIE over: 1993000.25\n",
      "EI over: 58132228.0\n",
      "Avg Corr: 0.2727, Corr: 0.4603, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 60125228.0\n",
      "Outer epoch 153, Inner epoch 10, Loss: \tIE over: 985023.3125\n",
      "EI over: 56732156.0\n",
      "Avg Corr: 0.2964, Corr: 0.5622, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57717180.0\n",
      "Outer epoch 154, Inner epoch 10, Loss: \tIE over: 161453.0625\n",
      "EI over: 34780040.0\n",
      "Avg Corr: 0.3037, Corr: 0.5550, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 34941492.0\n",
      "Outer epoch 155, Inner epoch 10, Loss: \tIE over: 1225811.125\n",
      "EI over: 4192395.0\n",
      "Avg Corr: 0.2823, Corr: 0.5021, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5418207.0\n",
      "Outer epoch 156, Inner epoch 10, Loss: \tIE over: 2376876.25\n",
      "EI over: 20022484.0\n",
      "Avg Corr: 0.2295, Corr: 0.3835, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 22399362.0\n",
      "Outer epoch 157, Inner epoch 10, Loss: \tIE over: 3118918.5\n",
      "EI over: 13628646.0\n",
      "Avg Corr: 0.3377, Corr: 0.7175, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16747565.0\n",
      "Outer epoch 158, Inner epoch 10, Loss: \tIE over: 3593417.75\n",
      "EI over: 11309828.0\n",
      "Avg Corr: 0.2707, Corr: 0.4675, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14903247.0\n",
      "Outer epoch 159, Inner epoch 10, Loss: \tIE over: 3733292.25\n",
      "EI over: 14534953.0\n",
      "Avg Corr: 0.2969, Corr: 0.5058, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 18268248.0\n",
      "Outer epoch 160, Inner epoch 10, Loss: \tIE over: 3567496.0\n",
      "EI over: 1545981.5\n",
      "Avg Corr: 0.2704, Corr: 0.4524, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5113478.5\n",
      "Outer epoch 161, Inner epoch 10, Loss: \tIE over: 3251737.75\n",
      "EI over: 1356447.375\n",
      "Avg Corr: 0.2634, Corr: 0.4375, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 4608186.0\n",
      "Outer epoch 162, Inner epoch 10, Loss: \tIE over: 2601684.5\n",
      "EI over: 13376556.0\n",
      "Avg Corr: 0.3574, Corr: 0.7410, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 15978241.0\n",
      "Outer epoch 163, Inner epoch 10, Loss: \tIE over: 1819444.25\n",
      "EI over: 7553596.0\n",
      "Avg Corr: 0.2736, Corr: 0.4867, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 9373041.0\n",
      "Outer epoch 164, Inner epoch 10, Loss: \tIE over: 895790.75\n",
      "EI over: 16589762.0\n",
      "Avg Corr: 0.3141, Corr: 0.6241, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 17485554.0\n",
      "Outer epoch 165, Inner epoch 10, Loss: \tIE over: 197217.28125\n",
      "EI over: 19094404.0\n",
      "Avg Corr: 0.1935, Corr: 0.2193, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 19291622.0\n",
      "Outer epoch 166, Inner epoch 10, Loss: \tIE over: 439758.6875\n",
      "EI over: 614857.25\n",
      "Avg Corr: 0.2520, Corr: 0.4064, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1054616.75\n",
      "Outer epoch 167, Inner epoch 10, Loss: \tIE over: 3494.7939453125\n",
      "EI over: 35048376.0\n",
      "Avg Corr: 0.3132, Corr: 0.5721, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 35051872.0\n",
      "Outer epoch 168, Inner epoch 10, Loss: \tIE over: 865318.25\n",
      "EI over: 47831284.0\n",
      "Avg Corr: 0.4446, Corr: 1.0660, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 48696604.0\n",
      "Outer epoch 169, Inner epoch 10, Loss: \tIE over: 1547017.5\n",
      "EI over: 38816376.0\n",
      "Avg Corr: 0.3160, Corr: 0.5687, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 40363392.0\n",
      "Outer epoch 170, Inner epoch 10, Loss: \tIE over: 1895930.375\n",
      "EI over: 9769900.0\n",
      "Avg Corr: 0.3050, Corr: 0.5454, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11665831.0\n",
      "Outer epoch 171, Inner epoch 10, Loss: \tIE over: 1967174.0\n",
      "EI over: 35261552.0\n",
      "Avg Corr: 0.2331, Corr: 0.3791, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 37228728.0\n",
      "Outer epoch 172, Inner epoch 10, Loss: \tIE over: 1744626.0\n",
      "EI over: 56663732.0\n",
      "Avg Corr: 0.2903, Corr: 0.5696, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 58408360.0\n",
      "Outer epoch 173, Inner epoch 10, Loss: \tIE over: 1309336.75\n",
      "EI over: 55339008.0\n",
      "Avg Corr: 0.3123, Corr: 0.6204, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 56648344.0\n",
      "Outer epoch 174, Inner epoch 10, Loss: \tIE over: 649527.0\n",
      "EI over: 33257644.0\n",
      "Avg Corr: 0.3897, Corr: 0.9092, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 33907172.0\n",
      "Outer epoch 175, Inner epoch 10, Loss: \tIE over: 59836.859375\n",
      "EI over: 5431923.0\n",
      "Avg Corr: 0.3669, Corr: 0.7513, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 5491761.0\n",
      "Outer epoch 176, Inner epoch 10, Loss: \tIE over: 22753.146484375\n",
      "EI over: 21248452.0\n",
      "Avg Corr: 0.3009, Corr: 0.5457, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 21271206.0\n",
      "Outer epoch 177, Inner epoch 10, Loss: \tIE over: 613911.75\n",
      "EI over: 14769663.0\n",
      "Avg Corr: 0.2972, Corr: 0.5354, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 15383576.0\n",
      "Outer epoch 178, Inner epoch 10, Loss: \tIE over: 1067102.5\n",
      "EI over: 10070282.0\n",
      "Avg Corr: 0.2836, Corr: 0.5102, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 11137385.0\n",
      "Outer epoch 179, Inner epoch 10, Loss: \tIE over: 1189094.75\n",
      "EI over: 13287836.0\n",
      "Avg Corr: 0.3069, Corr: 0.6183, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14476932.0\n",
      "Outer epoch 180, Inner epoch 10, Loss: \tIE over: 1070834.5\n",
      "EI over: 2678498.5\n",
      "Avg Corr: 0.3228, Corr: 0.6617, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 3749334.0\n",
      "Outer epoch 181, Inner epoch 10, Loss: \tIE over: 695229.125\n",
      "EI over: 185238.75\n",
      "Avg Corr: 0.3031, Corr: 0.5119, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 880468.875\n",
      "Outer epoch 182, Inner epoch 10, Loss: \tIE over: 99678.515625\n",
      "EI over: 14583932.0\n",
      "Avg Corr: 0.3305, Corr: 0.6868, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 14683612.0\n",
      "Outer epoch 183, Inner epoch 10, Loss: \tIE over: 191364.34375\n",
      "EI over: 8739996.0\n",
      "Avg Corr: 0.2489, Corr: 0.4850, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 8931361.0\n",
      "Outer epoch 184, Inner epoch 10, Loss: \tIE over: 646907.0625\n",
      "EI over: 15419108.0\n",
      "Avg Corr: 0.2919, Corr: 0.5522, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 16066016.0\n",
      "Outer epoch 185, Inner epoch 10, Loss: \tIE over: 804317.75\n",
      "EI over: 18047388.0\n",
      "Avg Corr: 0.2342, Corr: 0.4412, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 18851706.0\n",
      "Outer epoch 186, Inner epoch 10, Loss: \tIE over: 692833.625\n",
      "EI over: 1190816.75\n",
      "Avg Corr: 0.2814, Corr: 0.4775, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1883651.25\n",
      "Outer epoch 187, Inner epoch 10, Loss: \tIE over: 343479.71875\n",
      "EI over: 56711288.0\n",
      "Avg Corr: 0.3284, Corr: 0.6633, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 57054768.0\n",
      "Outer epoch 188, Inner epoch 10, Loss: \tIE over: 14471.0849609375\n",
      "EI over: 88168912.0\n",
      "Avg Corr: 0.2353, Corr: 0.4082, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 88183384.0\n",
      "Outer epoch 189, Inner epoch 10, Loss: \tIE over: 781262.0625\n",
      "EI over: 95905664.0\n",
      "Avg Corr: 0.2844, Corr: 0.4669, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 96686928.0\n",
      "Outer epoch 190, Inner epoch 10, Loss: \tIE over: 1453551.25\n",
      "EI over: 81903880.0\n",
      "Avg Corr: 0.3237, Corr: 0.6442, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 83357432.0\n",
      "Outer epoch 191, Inner epoch 10, Loss: \tIE over: 1792979.5\n",
      "EI over: 48543656.0\n",
      "Avg Corr: 0.3417, Corr: 0.6902, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 50336636.0\n",
      "Outer epoch 192, Inner epoch 10, Loss: \tIE over: 1846098.25\n",
      "EI over: 98494.5625\n",
      "Avg Corr: 0.2568, Corr: 0.4203, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1944593.75\n",
      "Outer epoch 193, Inner epoch 10, Loss: \tIE over: 1674359.75\n",
      "EI over: 24848666.0\n",
      "Avg Corr: 0.4335, Corr: 1.0427, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 26523028.0\n",
      "Outer epoch 194, Inner epoch 10, Loss: \tIE over: 1213976.375\n",
      "EI over: 26319344.0\n",
      "Avg Corr: 0.2432, Corr: 0.3523, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 27533322.0\n",
      "Outer epoch 195, Inner epoch 10, Loss: \tIE over: 557687.75\n",
      "EI over: 7061749.0\n",
      "Avg Corr: 0.2919, Corr: 0.5540, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 7619438.0\n",
      "Outer epoch 196, Inner epoch 10, Loss: \tIE over: 66716.15625\n",
      "EI over: 29310664.0\n",
      "Avg Corr: 0.3126, Corr: 0.6192, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 29377380.0\n",
      "Outer epoch 197, Inner epoch 10, Loss: \tIE over: 31495.5546875\n",
      "EI over: 42782664.0\n",
      "Avg Corr: 0.2679, Corr: 0.4426, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 42814160.0\n",
      "Outer epoch 198, Inner epoch 10, Loss: \tIE over: 492020.21875\n",
      "EI over: 34516000.0\n",
      "Avg Corr: 0.2971, Corr: 0.4979, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 35008020.0\n",
      "Outer epoch 199, Inner epoch 10, Loss: \tIE over: 38801.66796875\n",
      "EI over: 6130493.5\n",
      "Avg Corr: 0.3876, Corr: 0.8895, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 6169296.0\n"
     ]
    }
   ],
   "source": [
    "losses = train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:11.686188Z",
     "start_time": "2024-07-29T02:19:03.233257Z"
    }
   },
   "id": "e34a7ed21ff8c903"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(torch.arange(losses.shape[0]), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss ratio\")\n",
    "    plt.title(\"Loss ratio per realization over time\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:11.872512Z",
     "start_time": "2024-07-29T02:33:11.705045Z"
    }
   },
   "id": "1b8aabaa70054798"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "i = correlated_mitral_activity()\n",
    "hbar_ff = compute_feedforward_activity(i)\n",
    "W_random = compute_initial_recurrent_weights()\n",
    "R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:12.514161Z",
     "start_time": "2024-07-29T02:33:11.871351Z"
    }
   },
   "id": "a3843b2195b086d5"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Stats at initialization\n",
    "mu_familiar_0 = torch.mean(R_random[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_0 = torch.var(R_random[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_0 = torch.mean(R_random[:num_e, novel_inds], dim=1)\n",
    "sig_novel_0 = torch.var(R_random[:num_e, novel_inds], dim=1)\n",
    "#print(f\"Initialization: \\nFamiliar -  mean: {mu_familiar_0}, var: {sig_familiar_0}\\nNovel -  mean: {mu_novel_0}, var: {sig_novel_0}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:12.597965Z",
     "start_time": "2024-07-29T02:33:12.515087Z"
    }
   },
   "id": "4da719c5a4a5ebbd"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "R_random = R_random.to(gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:12.603224Z",
     "start_time": "2024-07-29T02:33:12.598676Z"
    }
   },
   "id": "67971e29f9bb692"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_inner):\n\u001B[0;32m---> 23\u001B[0m         _, W_plastic, R_plastic \u001B[38;5;241m=\u001B[39m loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, with_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     24\u001B[0m         W_tracked[i, :, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m=\u001B[39m W_plastic[ie_update_inds][ie_track_inds]\u001B[38;5;241m.\u001B[39mdetach()\n\u001B[1;32m     25\u001B[0m         W_tracked[i, :, \u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m W_plastic[ei_update_inds][ei_track_inds]\u001B[38;5;241m.\u001B[39mdetach()\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "W_plastic = W_random.detach().clone()\n",
    "R_plastic = R_random.detach().clone()\n",
    "with torch.no_grad():\n",
    "    ie_update_inds = get_update_inds(ie_post, ie_pre, W_plastic)\n",
    "    ei_update_inds = get_update_inds(ei_post, ei_pre, W_plastic)\n",
    "    clamp_min = torch.zeros_like(W_plastic)\n",
    "    clamp_min[ei_update_inds] = ei_min_weight\n",
    "    clamp_min[ie_update_inds] = ie_min_weight\n",
    "    clamp_max = torch.zeros_like(W_plastic)\n",
    "    clamp_max[ie_update_inds] = ie_max_weight\n",
    "    clamp_max[ei_update_inds] = ei_max_weight\n",
    "    weight_range = (clamp_min, clamp_max)\n",
    "\n",
    "\n",
    "# Number of weights from each group to track\n",
    "num_samples = 100\n",
    "W_tracked = torch.empty((n_inner, num_samples, 2))\n",
    "R_tracked = torch.empty((n_inner, num_i))\n",
    "ie_track_inds = torch.randint(0, len(ie_update_inds), size=(num_samples,))\n",
    "ei_track_inds = torch.randint(0, len(ei_update_inds), size=(num_samples,))\n",
    "with torch.no_grad():\n",
    "    for i in range(n_inner):\n",
    "        _, W_plastic, R_plastic = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        W_tracked[i, :, 0] = W_plastic[ie_update_inds][ie_track_inds].detach()\n",
    "        W_tracked[i, :, 1] = W_plastic[ei_update_inds][ei_track_inds].detach()\n",
    "        R_tracked[i, :] = torch.mean(R_plastic[num_e:, :], dim=1).detach()\n",
    "\n",
    "mu_familiar_f = torch.mean(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_f = torch.var(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_f = torch.mean(R_plastic[:num_e, novel_inds], dim=1)\n",
    "sig_novel_f = torch.var(R_plastic[:num_e, novel_inds], dim=1)\n",
    "mu_novel_diff = mu_novel_f - mu_novel_0\n",
    "mu_familiar_diff = mu_familiar_f - mu_familiar_0\n",
    "sig_novel_diff = sig_novel_f - sig_novel_0\n",
    "sig_familiar_diff = sig_familiar_f - sig_familiar_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.789699Z",
     "start_time": "2024-07-29T02:33:12.604656Z"
    }
   },
   "id": "8aea9b6d1ff27717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "R_random = R_random.cpu()\n",
    "R_plastic = R_plastic.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.789346Z"
    }
   },
   "id": "26d6bbd72d52ab77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(R_tracked)\n",
    "plt.title(\"I responses across inner epochs\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.794206Z",
     "start_time": "2024-07-29T02:33:13.790743Z"
    }
   },
   "id": "630eb42d255674cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look at how weights evolve over the course of inner epoch, for a random realization\n",
    "# Store a random set of 10 weights and see how the 10 weights change over the course of the iterations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.792280Z"
    }
   },
   "id": "ddea879c3b9927e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.hist(torch.flatten(R_random[:num_e]), density=True, label=\"E responses\")\n",
    "    ax1.hist(torch.flatten(R_random[num_e:]), density=True, label=\"I responses\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.hist(torch.flatten(R_plastic[:num_e]), density=True, label=\"E responses\")\n",
    "    ax2.hist(torch.flatten(R_plastic[num_e:]), density=True, label=\"I responses\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Responses\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.793680Z"
    }
   },
   "id": "b9dd48d9b5cdbe58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "R_random[num_e:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.804370Z",
     "start_time": "2024-07-29T02:33:13.795285Z"
    }
   },
   "id": "92f7a48ffa1db260"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO figure out why I responses keep getting blown up to 0\n",
    "R_plastic[num_e:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.796739Z"
    }
   },
   "id": "815a783a635f96f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ei_update_inds].cpu().flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.798322Z"
    }
   },
   "id": "c2901f2e35c4e550"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ie_update_inds].cpu().flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.799897Z"
    }
   },
   "id": "af71d4a512bc4881"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO model is learning to set all E->I weights to 0, and that's causing I responses to go to 0, then I->E weights can't do anything to fix that\n",
    "torch.unique((W_plastic[ie_update_inds].cpu().flatten()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.801644Z"
    }
   },
   "id": "547affea0c66ded8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    sparsities_0 = sparsity_per_odor(R_random)\n",
    "    spars_novel = sparsities_0[novel_inds]\n",
    "    spars_familiar = sparsities_0[familiar_inds]\n",
    "    ax1.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax1.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    sparsities_f = sparsity_per_odor(R_plastic)\n",
    "    spars_novel = sparsities_f[novel_inds]\n",
    "    spars_familiar = sparsities_f[familiar_inds]\n",
    "    ax2.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax2.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Sparsity per odor\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.803155Z"
    }
   },
   "id": "e970014f84261189"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def verification_set(ie_model, ei_model, runs=10):\n",
    "    corrs = torch.empty((runs,))\n",
    "    ratios = torch.empty((runs,))\n",
    "    spars_change = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        I_ff = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(I_ff)\n",
    "        W_random = compute_initial_recurrent_weights()\n",
    "        R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)\n",
    "        _, initial_corr = odor_corrs(R_random)\n",
    "        spars_initial = sparsity_per_odor(R_random)\n",
    "        initial_spars_diff = torch.abs(torch.mean(spars_initial[novel_inds]) - torch.mean(spars_initial[familiar_inds]))\n",
    "        \n",
    "        ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "        ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "        \n",
    "        for _ in range(n_inner):\n",
    "            _, W_random, R_random = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_random, R_random, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        \n",
    "        _, final_corr = odor_corrs(R_random)\n",
    "        ratio = final_corr / initial_corr\n",
    "        spars_final = sparsity_per_odor(R_random)\n",
    "        # Plot actual difference in the sparsity number, not the percent change ratio\n",
    "        # - Also see whether \n",
    "        final_spars_diff = torch.abs(torch.mean(spars_final[novel_inds]) - torch.mean(spars_final[familiar_inds]))\n",
    "        \n",
    "        print(f\"Corr: {initial_corr} -> {final_corr}, Sparsity: {initial_spars_diff} -> {final_spars_diff}, Loss Ratio: {ratio}\")\n",
    "        corrs[i] = final_corr.item()\n",
    "        ratios[i] = ratio.item()\n",
    "        spars_change[i] = (((final_spars_diff - initial_spars_diff) / initial_spars_diff) * 100).item()\n",
    "        \n",
    "    return corrs, ratios, spars_change"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.814173Z",
     "start_time": "2024-07-29T02:33:13.804653Z"
    }
   },
   "id": "8a68603cd329249b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrs, ratios, spars_change = verification_set(ie_model, ei_model, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.806121Z"
    }
   },
   "id": "94bafcc0031d6f22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(ratios, bins=20)\n",
    "plt.title(\"Loss ratios\")\n",
    "plt.show()\n",
    "plt.hist(spars_change, bins=20)\n",
    "plt.title(\"Percent change in sparsity per odor for novel vs. familiar families\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.807719Z"
    }
   },
   "id": "bfae243066bddc4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spars_change[spars_change < 100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.809240Z"
    }
   },
   "id": "9f1cf70906cd0175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# version = \"6\"\n",
    "# ie_path = f\"./joint_models/ie_models/ie_model_{version}\"\n",
    "# torch.save(ie_model.state_dict(), ie_path)\n",
    "# ei_path = f\"./joint_models/ei_models/ei_model_{version}\"\n",
    "# torch.save(ei_model.state_dict(), ei_path)\n",
    "\n",
    "# Model 2 - loss ratios between 0.75 and 1, most sparsity changes within 100%\n",
    "# Model 3 - loss ratios between 0.94 and 1.04, reduces sparsity close to 100% (but has NaN values) \n",
    "# Model 4 - reduces sparsity close to 100%, loss ratios between 0.85 and 1 (basically tries to set weights to 0)\n",
    "# Model 5 - loss ratios between 0.9 and 1.1, most sparsity changes go to 0 (negative 100% sparsity change)\n",
    "\n",
    "# Interesting result from model 1 - tries to set all weights to 0\n",
    "#ie_model = create_model()\n",
    "#ie_model.load_state_dict(torch.load(ie_path))\n",
    "#ei_model = create_model()\n",
    "#ei_model.load_state_dict(torch.load(ei_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.810736Z"
    }
   },
   "id": "5279dce2095c309d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_colormap(model, type=\"ie\"):\n",
    "    num_ticks = 100\n",
    "    E_min, E_max = torch.min(R_plastic[:num_e, :]), torch.max(R_plastic[:num_e, :])\n",
    "    I_min, I_max = torch.min(R_plastic[num_e:, :]), torch.max(R_plastic[num_e:, :])\n",
    "    E_vals = torch.linspace(E_min, E_max, num_ticks)\n",
    "    I_vals = torch.linspace(I_min, I_max, num_ticks)\n",
    "    E_coords, I_coords = torch.meshgrid(E_vals, I_vals, indexing=\"ij\")\n",
    "    R_plot = 0\n",
    "    if type == \"ie\":\n",
    "        R_plot = torch.stack((E_coords.flatten(), I_coords.flatten()), dim=1)\n",
    "    elif type == \"ei\":\n",
    "        R_plot = torch.stack((I_coords.flatten(), E_coords.flatten()), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        plasticity_vals = model(R_plot.to(gpu)).squeeze(0).cpu()\n",
    "        \n",
    "    plot = plt.scatter(R_plot[:, 0], R_plot[:, 1], c=plasticity_vals, cmap='rainbow')\n",
    "    clrbar = plt.colorbar(plot)\n",
    "    clrbar.set_label('Model plasticity')\n",
    "    plt.xlabel(\"E responses\")\n",
    "    plt.ylabel(\"I responses\")\n",
    "        \n",
    "    return E_min, E_max, I_min, I_max, R_plot, plasticity_vals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.812005Z"
    }
   },
   "id": "d0cb8ceeefdbfa5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def fit_model(model, type=\"ie\", degree=3):\n",
    "    E_min, E_max, I_min, I_max, R_plot, plasticity_vals = make_colormap(model, type)\n",
    "    \n",
    "\n",
    "    std = 2 # Take data within 2 std of responses mean\n",
    "    mu_e = torch.mean(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    sig_e = torch.std(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    mu_i = torch.mean(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    sig_i = torch.std(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    e_train_bounds = (torch.max(E_min, mu_e - std*sig_e), torch.min(E_max, mu_e + std*sig_e))\n",
    "    i_train_bounds = (torch.max(I_min, mu_i - std*sig_i), torch.min(I_max, mu_i + std*sig_i))\n",
    "    \n",
    "    reg = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "    \n",
    "    e_inds = torch.logical_and(torch.ge(R_plot[:, 0], e_train_bounds[0]), torch.le(R_plot[:, 0], e_train_bounds[1]))\n",
    "    i_inds = torch.logical_and(torch.ge(R_plot[:, 1], i_train_bounds[0]), torch.le(R_plot[:, 1], i_train_bounds[1]))\n",
    "    b = torch.logical_and(e_inds, i_inds)\n",
    "    X = R_plot[b]\n",
    "    y = plasticity_vals[b].squeeze(0)\n",
    "    \n",
    "    reg = reg.fit(X, y)\n",
    "    # Exclude bias term\n",
    "    coefs = reg.named_steps['linear'].coef_[0][1:]\n",
    "    # How many terms to consider\n",
    "    take = 3\n",
    "    # Take largest magnitude terms\n",
    "    inds = np.argsort(np.abs(coefs))[-take:]\n",
    "\n",
    "    # Exclude bias term\n",
    "    term_list = reg.named_steps['poly'].powers_[1:, :]\n",
    "    terms = f\"[Pre, Post] Powers:\\n\"\n",
    "    for i in range(take - 1, -1, -1):\n",
    "        term = f\"{term_list[inds][i]}: {coefs[inds][i]}\\n\"\n",
    "        terms += term\n",
    "    \n",
    "    print(terms)\n",
    "    \n",
    "    preds = reg.predict(X).ravel()\n",
    "    return X, preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.813050Z"
    }
   },
   "id": "7c28c037048e3b5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ie_X, ie_preds = fit_model(ie_model, type=\"ie\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ie_X[:, 0], ie_X[:, 1], c=ie_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"E-I plasticity based on neural responses\")\n",
    "plt.xlabel(\"E responses\")\n",
    "plt.ylabel(\"I responses\")\n",
    "plt.show()\n",
    "\n",
    "ei_X, ei_preds = fit_model(ei_model, type=\"ei\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ei_X[:, 0], ei_X[:, 1], c=ei_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"I-E plasticity based on neural responses\")\n",
    "plt.xlabel(\"I responses\")\n",
    "plt.ylabel(\"E responses\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.813833Z"
    }
   },
   "id": "b7ac6392aab0837e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do a plot of rho_0 vs rho_final (for ex. 100 random initializations) to see whether the model can decorrelate odors\n",
    "# Even though potentiation is always positive for the set of responses, since we are doing E to I, increasing the connection strength effectively increases inhibition too, so it balances out\n",
    "# Not directly hebbian because that would mean there is some constant c*r_i*r_j, but when one of the responses is decreasing, the resulting potentiation doesn't decrease\n",
    "# Instead, it will be an a * r_i + b * r_j term\n",
    "# These terms come from the polynomial expansion of the function defined by the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.814747Z"
    }
   },
   "id": "aa938bb7961846a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look at statistics of E neurons (mean firing rate across odors, variance, etc) at initialization compared to after plasticity\n",
    "# Also look at sparsity - one across odors and another across neurons (sparsity is essentially 1 minus the square of the coefficient of variation)\n",
    "# We care mostly about the sparsity across neurons (ex between neurons) and what it would be between odors (should be similar between the familiar odors b/c that's where we applied the plasticity) and we also know what it's like between novel odors\n",
    "# We don't want a change in firing rate, because it should be same for novel and familiar odors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.822408Z",
     "start_time": "2024-07-29T02:33:13.815606Z"
    }
   },
   "id": "6b1fe6484d9fbc92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pct_change = ((W_tracked[:, :, 0] - w_ie) / w_ie) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"E-I weight change\")\n",
    "    plt.show()\n",
    "    \n",
    "    pct_change = ((W_tracked[:, :, 1] - w_ei) / w_ei) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"I-E weight change\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.816446Z"
    }
   },
   "id": "92ed3911273faa8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_change = ((W_plastic[ie_update_inds].cpu() - w_ie) / w_ie) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"E-I Percent weight change from initialization\")\n",
    "plt.show()\n",
    "\n",
    "W_change = ((W_plastic[ei_update_inds].cpu() - w_ei) / w_ei) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"I-E Percent weight change from initialization\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.817209Z"
    }
   },
   "id": "804d8df91ba43350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(mu_familiar_0.detach().cpu() - mu_novel_0.detach().cpu(), bins=50, label=\"Before\")\n",
    "plt.hist(mu_familiar_f.detach().cpu() - mu_novel_f.detach().cpu(), bins=50, label=\"After\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.817977Z"
    }
   },
   "id": "d7d07fb09871512a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(mu_novel_diff, bins=50, label=\"Novel\")\n",
    "plt.hist(mu_familiar_diff, bins=50, label=\"Familiar\")\n",
    "plt.title(\"Difference in mean responses before and after plasticity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.818806Z"
    }
   },
   "id": "dd90208f664a85ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp_novel_0 = sparsity_per_neuron(R_random, novel_inds)\n",
    "sp_novel_f = sparsity_per_neuron(R_plastic, novel_inds)\n",
    "sp_familiar_0 = sparsity_per_neuron(R_random, familiar_inds)\n",
    "sp_familiar_f = sparsity_per_neuron(R_plastic, familiar_inds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.819720Z"
    }
   },
   "id": "cecc851bc8f43f35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.hist(sp_novel_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_0\")\n",
    "    plt.hist(sp_familiar_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_0\")\n",
    "    plt.hist(sp_novel_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_f\")\n",
    "    plt.hist(sp_familiar_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_f\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.820557Z"
    }
   },
   "id": "3ee33d1ae62b6cd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create scatter plot of odor correlation vs sparsity difference - odors that are highly positively correlated should have similar sparsities across neurons, and those that are highly negatively correlated should have different sparsities across neurons\n",
    "# Check - increase P' all the way to 16, and with lower correlations, there should be less variability between the sparsity for each odor\n",
    "# - would tell us how much the natural spread in sparsity is"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.821372Z"
    }
   },
   "id": "91ce75bd2a06f39a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Re-run model w/ lower threshold (0 stdev above mean)\n",
    "# 2. Figure out percentage-wise how much the weights actually change (do a hist)\n",
    "# - if it's 1-2% it's too small, we want the weights to change 10-100%, the weights could even change 10-fold\n",
    "# 3. Add metrics to training function - automatically compute sparsity etc (also look at the sparsity between odors, not just the sparsity between neurons, see if it changes for novel vs familiar)\n",
    "# Create bar plot for each number of odors, how many neurons respond to that number of odors (ideally, if we have a threshold of 0 stdev, most neurons should respond to ~4 odors, since on average a neuron will respond to half of the total odors, so 8 odors, and out of those, it should be equal between 4 novel and 4 familiar)\n",
    "# Hypothesis right now - weights aren't changing that much, so we can add more epoch_inner steps (since the gradient isn't blowing up)\n",
    "# Then, try removing plasticity ramp - keep 1e-3 (don't do epoch_inner increase and remove plasticity ramp at same time)\n",
    "# Goal: understand what mechanism the meta-learning discovered that makes correlations smaller (see whether it acts on sparsity etc)\n",
    "# Think of other metrics to quantify network behavior to understand change"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-07-29T02:33:13.822196Z"
    }
   },
   "id": "fdcd6174a08701aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T02:33:13.894626Z",
     "start_time": "2024-07-29T02:33:13.822939Z"
    }
   },
   "id": "c57e8808ab6eb092"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
