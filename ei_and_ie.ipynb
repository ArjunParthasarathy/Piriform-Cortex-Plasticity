{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.243331Z",
     "start_time": "2024-08-01T17:46:22.473598Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274764Z",
     "start_time": "2024-08-01T17:46:22.475263Z"
    }
   },
   "id": "cc4fbc6b56b8cbae"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "gpu = torch.device(\"mps\")\n",
    "\n",
    "# Use smaller network for testing - ex 2000 neurons\n",
    "# Even for the project, doing it for 10^6 neurons would take too long\n",
    "# Problem this creates: test network is denser than actual network b/c we have 10^3 neurons but 10^2 connections per neuron\n",
    "num_neurons = 2000\n",
    "num_i = int(0.1 * num_neurons)\n",
    "num_e = int(0.9 * num_neurons)\n",
    "\n",
    "# Epsilon value close to 0 to prevent nan in division by 0\n",
    "eps = 1e-6\n",
    "\n",
    "# Num excitatory inputs and inhibitory inputs to each neuron (in reality it should be 500 but we reduce it here to make things faster)\n",
    "k = 100\n",
    "\n",
    "# Number of olfactory bulb channels (glomeruli) to each neuron\n",
    "D = 10 ** 3\n",
    "# For each neuron, how many glomeruli inputs it receives (should be 10^2)\n",
    "num_channel_inputs = 100\n",
    "\n",
    "# Number of odors\n",
    "P = 16\n",
    "# Novel activity is up to P // 2, and familiar activity is after\n",
    "novel_inds = torch.arange(0, P // 2)\n",
    "familiar_inds = torch.arange(P // 2, P)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274897Z",
     "start_time": "2024-08-01T17:46:24.148740Z"
    }
   },
   "id": "80a1147c4c0a6f95"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Creates sparse adjacency matrix with the given probability of edge connection and size mxn\n",
    "def create_adj_matrix(p, m, n):\n",
    "    # num_connections = int(p * m * n)\n",
    "    # m_coords = torch.randint(0, m, (num_connections,))\n",
    "    # n_coords = torch.randint(0, n, (num_connections,))\n",
    "    # indices = torch.vstack((m_coords, n_coords))\n",
    "    # values = torch.ones(num_connections)\n",
    "    # A_mn = torch.sparse_coo_tensor(indices, values, (m, n))\n",
    "    probs = torch.ones(m, n) * p\n",
    "    A_mn = torch.bernoulli(probs)\n",
    "    return A_mn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274933Z",
     "start_time": "2024-08-01T17:46:24.155175Z"
    }
   },
   "id": "d4e13c61f25818ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# New way of generating correlations between odors: we want different sets of odors to be correlated differently, so that when we subtract each neuron's mean activity over odors, it doesn't cancel out the variation between odors (if all the odors are correlated the same, they will tend to produce similar values for a single neuron and therefore subtracting by the mean will remove these values and only leave small fluctuations)\n",
    "# So we sample a small set of odors P' and make them linearly independent, and then by multiplying by a P'x P gaussian matrix we project into mitral cell activity space for all P odors, basically making the P odors a linear combination of the set of P' odors (the smaller P' is, the more correlated the resulting set of P odors will be)\n",
    "# We also scale the variance depending on how small P' is, so we will maintain differently correlated odors, just with higher total correlation if P' is small"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274966Z",
     "start_time": "2024-08-01T17:46:24.159685Z"
    }
   },
   "id": "c6b94961ca601610"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "P_prime = 4\n",
    "def correlated_mitral_activity():\n",
    "    # Each of the P' odors is independent (correlation of 0)\n",
    "    sigma_p_prime = torch.zeros((P_prime, P_prime)).fill_diagonal_(1)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(P_prime), sigma_p_prime)\n",
    "    p_prime_activity = dist.sample(torch.Size([D]))\n",
    "    var = 1 / P_prime\n",
    "    projection = torch.normal(torch.zeros((P_prime, P)), torch.ones(P_prime, P) * np.sqrt(var))\n",
    "    activity = p_prime_activity @ projection\n",
    "    return activity.to(gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274997Z",
     "start_time": "2024-08-01T17:46:24.162068Z"
    }
   },
   "id": "28887b0933bdd67a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Takes in mitral activity I and computes feedforward activity h_bar_ff\n",
    "def compute_feedforward_activity(I):\n",
    "    # Probability that a channel weight will be nonzero\n",
    "    p = num_channel_inputs / D\n",
    "    # Only the first 0.9 * n rows should have this bernoulli number, the rest should be 0 b/c they don't receive a channel input\n",
    "    # Check whether each neuron still receives ~10^2 nonzero inputs or what the distribution actually looks like\n",
    "    # Because when we calculate the adjacency matrix we don't go by row (e.g ensuring each neuron has these ~10^2 connections)\n",
    "    # Alternative: sample from Binomial distribution w/ mean 100\n",
    "    # The output n for each row is the number of nonzero inputs, and you choose a random subset n of the indices for that row and make them 1\n",
    "    with torch.device(gpu):\n",
    "        a = create_adj_matrix(p, num_e, D)\n",
    "        # Inhibitory neurons don't receive channel input\n",
    "        # This is the first simplification, where we neglect the first inhibitory layer I_ff\n",
    "        b = torch.zeros(size=(num_i, D))\n",
    "        W_ff = torch.cat(tensors=(a, b), dim=0)\n",
    "        \n",
    "        h_ff = (W_ff @ I) * (1 / np.sqrt(num_channel_inputs))\n",
    "        h_bar_ff = torch.zeros_like(h_ff)\n",
    "        h_bar_ff[:num_e] = h_ff[:num_e] - torch.mean(h_ff[:num_e], dim=0, keepdim=True)\n",
    "    return h_bar_ff"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275027Z",
     "start_time": "2024-08-01T17:46:24.167675Z"
    }
   },
   "id": "67da2d3fe4b6a081"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def compute_initial_recurrent_weights():\n",
    "    k_ee = k_ei = k_ie = k_ii = k\n",
    "    #p_ee = k_ee / num_e\n",
    "    # k inhibitory inputs to that e neuron, out of num_i total inhibitory neurons gives the connection probability per neuron\n",
    "    p_ei = k_ei / num_i\n",
    "    p_ie = k_ie / num_e\n",
    "    #p_ii = k_ii / num_i\n",
    "    \n",
    "    # Constants\n",
    "    #w_ee = 0.1\n",
    "    w_ei = 0.2\n",
    "    w_ie = 0.5\n",
    "    #w_ii = 0.3\n",
    "    # Ignore ee and ii weights for now:\n",
    "    p_ee = p_ii = w_ee = w_ii = 0\n",
    "    with torch.device(gpu):\n",
    "        W_ee = create_adj_matrix(p_ee, num_e, num_e) * w_ee\n",
    "        W_ei = create_adj_matrix(p_ei, num_e, num_i) * -w_ei\n",
    "        W_ie = create_adj_matrix(p_ie, num_i, num_e) * w_ie\n",
    "        W_ii = create_adj_matrix(p_ii, num_i, num_i) * -w_ii\n",
    "        \n",
    "        # Concat\n",
    "        W_1 = torch.cat(tensors=(W_ee, W_ei), dim=1)\n",
    "        W_2 = torch.cat(tensors=(W_ie, W_ii), dim=1)\n",
    "        W_rec = torch.cat(tensors=(W_1, W_2), dim=0)\n",
    "    \n",
    "    return W_rec"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275058Z",
     "start_time": "2024-08-01T17:46:24.170963Z"
    }
   },
   "id": "b23d7ccb6f4b0787"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Computes activation threshold for neurons, based on the standard deviation of their firing rates across odors\n",
    "# This average standard deviation, multiplied by theta=2, ensures that each neuron will fire for only 5% of odors\n",
    "def compute_threshold(total_input, theta):\n",
    "    # For now, use diff thresholds for each neuron\n",
    "    center = torch.mean(total_input, dim=1, keepdim=True)\n",
    "    shift = torch.std(total_input, dim=1, keepdim=True)\n",
    "    threshold = center + (theta * shift)\n",
    "    # Since inhibitory neurons are linear\n",
    "    threshold[num_e:, :] = 0\n",
    "    return threshold"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275088Z",
     "start_time": "2024-08-01T17:46:24.176889Z"
    }
   },
   "id": "65ae3fd6edb6a55b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# ReLU for excitatory, linear for inhibitory\n",
    "def neuron_activations(X):\n",
    "    # Mask to keep excitatory\n",
    "    mask1 = torch.ones((num_neurons, 1), device=gpu)\n",
    "    mask1[num_e:, :] = 0\n",
    "    # Mask to keep inhibitory\n",
    "    mask2 = torch.zeros((num_neurons, 1), device=gpu)\n",
    "    mask2[num_e:, :] = 1\n",
    "    activation = (torch.relu(X) * mask1) + (X * mask2)\n",
    "    return activation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275118Z",
     "start_time": "2024-08-01T17:46:24.180482Z"
    }
   },
   "id": "1ce5f8670e8cb95e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Computes R for each odor, with the activation threshold theta\n",
    "def compute_piriform_response(h_bar_ff, W_rec, threshold_mult):\n",
    "    # The coefficient of x_bar\n",
    "    tau = 1\n",
    "    # time step\n",
    "    dt = 0.1\n",
    "    # Number of time steps\n",
    "    T = 200\n",
    "    \n",
    "    # Initial condition where states are gaussian\n",
    "    mu_0 = 0.\n",
    "    sigma_0 = 0.2\n",
    "    X_0 = torch.normal(mu_0, sigma_0, size=(num_neurons, P))\n",
    "    X = X_0\n",
    "    X = X.to(gpu)\n",
    "    \n",
    "    #pts = []\n",
    "    for i in range(T-4):\n",
    "        with torch.no_grad():\n",
    "            part1 = -1 * X\n",
    "            part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "            part3 = h_bar_ff\n",
    "            dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "            X = X + (dXdt * dt)\n",
    "        # Look at convergence pattern for first odor, assuming that it'll\n",
    "        # be similar across odors (since they are all independent)\n",
    "        #pts.append(torch.mean(dXdt, dim=0)[0].item())\n",
    "   \n",
    "    # On the last 2 iterations only, track the gradient\n",
    "    # TODO increased to 4 just for more coverage\n",
    "    X.requires_grad_(True)\n",
    "    \n",
    "    for j in range(4):\n",
    "        part1 = -1 * X\n",
    "        part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "        part3 = h_bar_ff\n",
    "        dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "        X = X + (dXdt * dt)\n",
    "    \n",
    "    # The total input to the neuron at this last time step (should be equivalent to the resulting value of X after this time step, since dxdt = 0 after the recurrent network converges)\n",
    "    total_input = part2 + part3\n",
    "    threshold = compute_threshold(total_input, threshold_mult)\n",
    "    \n",
    "    # Plot derivatives to see if state converged\n",
    "    # plt.plot(torch.arange(T-2), pts)\n",
    "    # plt.show()\n",
    "    R = neuron_activations(X - threshold)\n",
    "    \n",
    "    return R"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275148Z",
     "start_time": "2024-08-01T17:46:24.186449Z"
    }
   },
   "id": "14aa733bbbadf1db"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Compute dimensionality of activity matrix R for either novel or familiar\n",
    "# def compute_dim(R, odor_inds):\n",
    "#     # Only compute for the excitatory neurons (b/c those are the ones that send signals to rest of brain\n",
    "#     C = torch.cov(R[:num_e, odor_inds[0]:odor_inds[-1]])\n",
    "#     dim = torch.trace(C) ** 2 / torch.trace(C @ C)\n",
    "#     return dim\n",
    "\n",
    "# trace() is invariant for cyclic permutations of a matrix\n",
    "# Since C is symmetric, it can be orthogonally diagonalized into UDU^T where U is composed of orthonormal eigenvectors, U^T = U^-1, and D is a diagonal matrix of eigenvalues\n",
    "# therefore, trace(C) = trace(UDU^T) = trace(DU^TU) = trace(D) = sum(eigvals of C)\n",
    "# Similarly for the denominator, we need to compute the sum of the squared eigenvalues, which is trace(D^2). trace(D^2) = trace(D^2U^TU) = trace(UD^2U^T) = trace((UDU^T)^2)) [by the property of matrix exponentiation for a diagonalizable matrix] = trace(C^2) = trace(C @ C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275177Z",
     "start_time": "2024-08-01T17:46:24.189623Z"
    }
   },
   "id": "defc0132008cbd72"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Dimensionalities should be similar before performing plasticity, upper bound on dimensionality is 8 b/c there are 8 odors each for novel/familiar (this is the rank of the cov matrix)\n",
    "# Measures how many orthogonal directions in neuron space are needed to explain the set of odors (the max number of orthogonal directions is the number of neurons themselves)\n",
    "# print(C_novel0)\n",
    "# print(C_familiar0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275252Z",
     "start_time": "2024-08-01T17:46:24.192648Z"
    }
   },
   "id": "9b2eddcd771e4eb3"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Loop over odors and after computing R_alpha (stationary/convergence state) for each odor, perform weight update on W_rec\n",
    "# Compute increment for W_rec(i,j) which is only dependent on R_alpha(i) and R_alpha(j) (two different neurons in W_rec)\n",
    "# Do it only for the nonzero weights for now (because these are the only active connections between neurons)\n",
    "# - Later, we could explore whether new synapses etc are created/destroyed (the changing of a weight from zero to nonzero)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275295Z",
     "start_time": "2024-08-01T17:46:24.198975Z"
    }
   },
   "id": "152f0a7f13e8e541"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Set up MLP which takes in R_alpha(i) and R_alpha(j) and computes an output value delta_W_rec(i,j) which is the update value for W_rec(i,j)\n",
    "# Single MLP for each class of weights (W_ee, W_ei, etc), and we adjust the weights of this MLP\n",
    "# - Because plasticity is different between different kinds of neurons"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275329Z",
     "start_time": "2024-08-01T17:46:24.201213Z"
    }
   },
   "id": "f9a5a7b63771a7c3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Start w/ small MLP - 1 hidden layer (ex. 20 units), 2 inputs, 1 output\n",
    "# Each time step, we want to compute the weight update\n",
    "# Compute the responses using the existing weight matrix, run thru the MLP to get the weight update, and update the weight matrix\n",
    "# After n iterations, we want to calculate dimensionality (from the covariance matrix we generate from the neuron responses)\n",
    "# Loss function: dim Novel / dim Familiar\n",
    "# - Potential case: might make dim(Novel) as 0\n",
    "\n",
    "# Might have to regularize the firing rate - should be close to a value we set\n",
    "# Ex. could take average of neuron responses at beginning (before plasticity) and set that as target firing rate\n",
    "\n",
    "# To prevent exploding/vanishing gradients, might have to truncate the gradient (stop it from updating after a certain iteration) because when we accumulate the gradient over the time steps to compute the response, it might blow up or just vanish\n",
    "# - since the weight update is used to update the weight matrix and compute the next time step of neuron response, which in turn is fed into the MLP to compute the next weight update (very cyclical)\n",
    "# Another solution - only backprop thru the last time steps of the recurrent dynamics (the time steps to compute a neuron response), b/c these are where the neuron response converges to the actual value\n",
    "# Lookup how to truncate gradient PyTorch\n",
    "\n",
    "# Instead of having plasticity act on all weights, only have plasticity between certain cell types (ex. between E and E, it will create positive feedback loop on weight update and may cause gradient blowup)\n",
    "# For now, only use MLP for E -> I (minimize likelihood of gradient blowup), don't have to truncate gradient yet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275361Z",
     "start_time": "2024-08-01T17:46:24.220461Z"
    }
   },
   "id": "3effd76566c0c0b2"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def create_model() -> torch.nn.Sequential:\n",
    "    # Number of neurons in hidden layer\n",
    "    hidden_size = 100\n",
    "    # Constrain output to certain values: nn.Hardtanh(-1, 1)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "    )\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275393Z",
     "start_time": "2024-08-01T17:46:24.224277Z"
    }
   },
   "id": "24911045583425aa"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Start and stop indices for the section of W_rec we want to update, respectively \n",
    "# Takes in R matrix (neuron responses for each odor and tuple of update inds representing ie, then ei (each element in that tuple is itself a tuple of (post, pre))\n",
    "def compute_updates(R: torch.Tensor, models: tuple, update_inds: tuple) -> torch.Tensor:\n",
    "    # Compute the same pairs of R_i and R_j for every odor, per model\n",
    "    all_updates = []\n",
    "    for i in range(len(models)):\n",
    "        postsyn_responses = R[update_inds[i][0], :]\n",
    "        presyn_responses = R[update_inds[i][1], :]\n",
    "        model_input = torch.stack(tensors=(presyn_responses, postsyn_responses), dim=2).transpose(1, 0)\n",
    "        updates_per_odor = models[i](model_input)\n",
    "        model_updates = torch.mean(updates_per_odor, dim=0).squeeze(dim=1)\n",
    "        all_updates.append(model_updates)\n",
    "    \n",
    "    return all_updates"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275425Z",
     "start_time": "2024-08-01T17:46:24.228522Z"
    }
   },
   "id": "a1cd6e5a0aa03ae7"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# ie_post = (num_e, num_neurons)\n",
    "# ie_pre = (0, num_e)\n",
    "# ei_post = (0, num_e)\n",
    "# ei_pre = (num_e, num_neurons)\n",
    "# ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "# ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "# # (16, 19953, 2) -> (16, 19953)\n",
    "# a1, a2 = compute_updates(R_random.to(gpu), (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "# #print(a1.shape, a2.shape)\n",
    "# print(a1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275457Z",
     "start_time": "2024-08-01T17:46:24.232958Z"
    }
   },
   "id": "9f51d515fadf0381"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def odor_corrs(R):\n",
    "    # We don't care about the actual responses per odor, just about a neuron's fluctuations around its mean response across odors\n",
    "    R_adjusted = R[:num_e] - torch.mean(R[:num_e], dim=1, keepdim=True)\n",
    "    # Each odor becomes a variable, because we want to calculate correlations between them across neurons\n",
    "    R_adjusted.t_()\n",
    "    # Like cov but divides by standard deviations, effectively normalizing the values (the diagonals of the resulting matrix become 1)\n",
    "    corrcoefs = torch.corrcoef(R_adjusted)\n",
    "    # If the responses are 0, variances across neurons will be 0, so denominator of corrcoef is 0, so term becomes nan\n",
    "    # In this case, the responses are \"perfectly correlated\" (bc always same value of 0) so its maximum correlation\n",
    "    # TODO do we need to change this NaN formulation so we propagate grad correctly?\n",
    "    corrcoefs = torch.nan_to_num(corrcoefs, nan=1.0)\n",
    "    # We only care about the correlations between the familiar odors\n",
    "    familiar_corrs = corrcoefs[P//2:P, P//2:P] - torch.eye(P // 2, device=gpu)\n",
    "    corr_sum = torch.sum(familiar_corrs ** 2)\n",
    "    avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "    \n",
    "    return corr_sum, avg_corr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275488Z",
     "start_time": "2024-08-01T17:46:24.235295Z"
    }
   },
   "id": "8887e40e8a4d89c4"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Sparsity per odor, across all (E) neurons\n",
    "def sparsity_per_odor(R):\n",
    "    # Epsilon for if we have zero responses\n",
    "    eps = 1e-6\n",
    "    sp_per_odor = 1 - ((torch.sum(R[:num_e], dim=0) ** 2 + eps) / (num_e * (torch.sum(R[:num_e] ** 2, dim=0)) + eps))\n",
    "    # Sparsity nan means that the responses were all 0 for an odor, meaning that its max sparsity of 1\n",
    "    return sp_per_odor\n",
    "\n",
    "# Sparsity per (E) neuron, across a given odor family\n",
    "def sparsity_per_neuron(R, odor_inds):\n",
    "    sp_per_neuron = 1 - (\n",
    "                (torch.sum(R[:num_e, odor_inds], dim=1) ** 2) / ((P // 2) * torch.sum(R[:num_e, odor_inds] ** 2, dim=1)))\n",
    "    return sp_per_neuron"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275519Z",
     "start_time": "2024-08-01T17:46:24.241695Z"
    }
   },
   "id": "d4d37265e0e371e8"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Try to minimize the correlations between values\n",
    "def loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=True):\n",
    "    corr_sum, avg_corr = odor_corrs(R)\n",
    "    corr_loss = (1 / P) * corr_sum\n",
    "    corr_term = lambda_corr * corr_loss\n",
    "    \n",
    "    means = torch.mean(R[:num_e], dim=0)\n",
    "    means_novel = torch.mean(means[novel_inds])\n",
    "    means_familiar = torch.mean(means[familiar_inds])\n",
    "    if torch.abs(means_novel + means_familiar) < eps:\n",
    "        # All means are the same so there's technically no loss\n",
    "        mu_term = 0\n",
    "    else:\n",
    "        mu_term = lambda_mu * (((means_familiar - means_novel + eps) / (means_novel + means_familiar)) ** 2)\n",
    "    \n",
    "    vars = torch.var(R[:num_e], dim=0)\n",
    "    var_novel = torch.mean(vars[novel_inds])\n",
    "    var_familiar = torch.mean(vars[familiar_inds])\n",
    "    if torch.abs(var_novel + var_familiar) < eps:\n",
    "        # All variances are the same so there's technically no loss\n",
    "        var_term = 0\n",
    "    else:\n",
    "        var_term = lambda_var * (((var_familiar - var_novel) / (var_novel + var_familiar)) ** 2)\n",
    "    \n",
    "    sparsities = sparsity_per_odor(R)\n",
    "    spars_novel = torch.mean(sparsities[novel_inds])\n",
    "    spars_familiar = torch.mean(sparsities[familiar_inds])\n",
    "    if torch.abs(spars_novel + spars_familiar) < eps:\n",
    "        # Sparsities are technically the same so the term shouldn't contribute to loss\n",
    "        spars_term = 0\n",
    "    else:\n",
    "        spars_term = lambda_sp * (((spars_familiar - spars_novel) / (spars_novel + spars_familiar)) ** 2)\n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Avg Corr: %.4f, Corr: %.4f, Mu: %.4f, Var: %.4f, Sparsity: %.4f\" % (avg_corr, corr_term, mu_term, var_term, spars_term))\n",
    " \n",
    "    loss = corr_term + mu_term + var_term + spars_term\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275588Z",
     "start_time": "2024-08-01T17:46:24.248644Z"
    }
   },
   "id": "1003ae68e7009d39"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Detaching vs zero grad - should detach because we have a term dependent on the previous model iteration which isn't zero but some constant gradient, accumulated from that model output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275650Z",
     "start_time": "2024-08-01T17:46:24.251548Z"
    }
   },
   "id": "b7e8cc5982ed854c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def loss_after_odors(ie_model: torch.nn.Sequential, ei_model: torch.nn.Sequential, ie_update_inds, ei_update_inds, W_rec: torch.Tensor, R_current: torch.Tensor, h_bar_ff: torch.Tensor, threshold_mult, plasticity_ie, plasticity_ei, weight_decay_rate, weight_range: tuple, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False):\n",
    "   # First, compute the respective weight updates through the novel and familiar odors from the current neural responses (which are from the current weight matrix)\n",
    "    \n",
    "    W_rec.requires_grad_(True)\n",
    "    \n",
    "    # TODO new paradigm: computing updates across odors and between models in parallel\n",
    "    # TODO regularize by penalizing high model update?\n",
    "    ie_model_updates, ei_model_updates = compute_updates(R_current, (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "   \n",
    "    # with torch.no_grad():\n",
    "    #     val_tensor = ((1 - plasticity_rate * weight_decay_rate) * W_rec[update_inds]) + updates + (plasticity_rate * odor_update)\n",
    "    #     condition = torch.logical_and(torch.gt(val_tensor, min_weight), torch.le(val_tensor, max_weight))\n",
    "    \n",
    "    ie_updates = plasticity_ie * (ie_model_updates - weight_decay_rate * W_rec[ie_update_inds])\n",
    "    ei_updates = plasticity_ei * (ei_model_updates - weight_decay_rate * W_rec[ei_update_inds])\n",
    "    \n",
    "    ie_cond = torch.logical_and(torch.ge(W_rec[ie_update_inds] + ie_updates, weight_range[0][ie_update_inds]), torch.le(W_rec[ie_update_inds] + ie_updates, weight_range[1][ie_update_inds]))\n",
    "    ei_cond = torch.logical_and(torch.ge(W_rec[ei_update_inds] + ei_updates, weight_range[0][ei_update_inds]), torch.le(W_rec[ei_update_inds] + ei_updates, weight_range[1][ei_update_inds]))\n",
    "    \n",
    "    # TODO clamping updates instead of the weights post-update - change in gradient?\n",
    "    # TODO if we clamp to bounds then we'll see changes in correlation, b/c there is a change in weight instead of none at all\n",
    "    ie_bounded_updates = torch.where(ie_cond, ie_updates, 0)\n",
    "    ei_bounded_updates = torch.where(ei_cond, ei_updates, 0)\n",
    "   \n",
    "    # TODO penalize model for causing weight to go over threshold\n",
    "    # Amount below min\n",
    "    ie_a = torch.relu(weight_range[0][ie_update_inds] - (W_rec[ie_update_inds] + ie_updates))\n",
    "    # Amount above max\n",
    "    ie_b = torch.relu((W_rec[ie_update_inds] + ie_updates) - weight_range[1][ie_update_inds])\n",
    "    ie_over_weight = torch.sum(ie_a + ie_b) / len(ie_update_inds)\n",
    "    ei_a = torch.relu(weight_range[0][ei_update_inds] - (W_rec[ei_update_inds] + ei_updates))\n",
    "    # Amount above max\n",
    "    ei_b = torch.relu((W_rec[ei_update_inds] + ei_updates) - weight_range[1][ei_update_inds])\n",
    "    ei_over_weight = torch.sum(ei_a + ei_b) / len(ei_update_inds)\n",
    "    \n",
    "    #W_rec = W_rec.clamp(min=weight_range[0], max=weight_range[1])\n",
    "    \n",
    "    W_rec = torch.index_put(W_rec, ie_update_inds, ie_bounded_updates, accumulate=True)\n",
    "    W_rec = torch.index_put(W_rec, ei_update_inds, ei_bounded_updates, accumulate=True)\n",
    "     \n",
    "    \n",
    "    R = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "    R_new = R\n",
    "    \n",
    "    if detach_grad:\n",
    "        # We want a new R response tensor which only has the previous weight update but not anything before that\n",
    "        W_rec = W_rec.detach()\n",
    "        R_new = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "\n",
    "    if with_loss:\n",
    "        print(f\"IE over: {ie_over_weight}\")\n",
    "        print(f\"EI over: {ei_over_weight}\")\n",
    "        loss = loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp)\n",
    "        overload = ie_over_weight + ei_over_weight\n",
    "    else:\n",
    "        loss = 0\n",
    "        overload = 0\n",
    "    \n",
    "    return loss, overload, W_rec, R_new"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275684Z",
     "start_time": "2024-08-01T17:46:24.260339Z"
    }
   },
   "id": "1252ae4977d8dbf"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# def verify_initial_activities():\n",
    "#     runs = 100\n",
    "#     avg_corrs = torch.empty((runs,))\n",
    "#     total_losses = torch.empty((runs,))\n",
    "#     for i in range(runs):\n",
    "#         with torch.no_grad():\n",
    "#             a = correlated_mitral_activity()\n",
    "#             h = compute_feedforward_activity(a)\n",
    "#             w = compute_initial_recurrent_weights()\n",
    "#             r = compute_piriform_response(h, w, 0)\n",
    "#         R_adjusted = r[:num_e] - torch.mean(r[:num_e], dim=1, keepdim=True)\n",
    "#         R_adjusted.t_()\n",
    "#         sigma_E = torch.corrcoef(R_adjusted)\n",
    "#         familiar_corrs = sigma_E[P//2:P, P//2:P] - torch.eye(P // 2)\n",
    "#         total_loss = torch.sum(familiar_corrs ** 2)\n",
    "#         avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "#         print(f\"Loss: {total_loss.item()}, Avg Corr: {avg_corr}\")\n",
    "#         total_losses[i] = total_loss\n",
    "#         avg_corrs[i] = avg_corr\n",
    "#     plt.hist(avg_corrs, bins=15)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275720Z",
     "start_time": "2024-08-01T17:46:24.263582Z"
    }
   },
   "id": "70c09c7202ebd3f2"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def get_update_inds(post, pre, W):\n",
    "    weights_slice = W[post[0]:post[1], pre[0]:pre[1]]\n",
    "    inds = torch.nonzero(weights_slice, as_tuple=True)\n",
    "    update_inds = (inds[0] + post[0], inds[1] + pre[0])\n",
    "    \n",
    "    return update_inds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275753Z",
     "start_time": "2024-08-01T17:46:24.267072Z"
    }
   },
   "id": "22297cdeba801b81"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Number of independent \"sniffs\" of the 16 odors\n",
    "n_inner = 1\n",
    "# Number of different realizations of the odor sniffing process\n",
    "n_outer = 200\n",
    "\n",
    "# TODO change back to -3 if too noisy\n",
    "plasticity_rate = 1e-1\n",
    "# Same plasticity rate for EI and IE\n",
    "plasticity_ie = plasticity_ei = plasticity_rate\n",
    "\n",
    "# TODO experiment w/ diff gradient tracking numbers\n",
    "# Number of inner epochs between model updates, n_update <= n_inner\n",
    "n_update = 1\n",
    "# Number of inner epochs across which the gradient is tracked (right now we detach the gradient after each inner epoch), n_track <= n_inner\n",
    "n_track = n_update\n",
    "\n",
    "# n_track = n_update to test formulation where we track the gradient across a subset of the inner epochs and update the model\n",
    "# If n_update > n_track, the model will have multiple \"gradient\" paths of loss accumulated: ex. inner epochs 1-5 and a separate branch of 6-10\n",
    "# If n_update < n_track, it doesn't matter b/c the model will truncate the history past its previous update since the gradient is zeroed\n",
    "\n",
    "# Number of standard deviations from mean, we are trying 0 b/c 1 and 2 is too sparse\n",
    "threshold_multiplier = 0\n",
    "\n",
    "# TODO for now go to simple formulation, no weight decay\n",
    "weight_decay = 0\n",
    "# How much to weight each of the regularization terms\n",
    "# Sparsity = 1000 doesn't improve loss ratios\n",
    "# Sparsity = 100, 500 epochs, loss ratios 1.0-1.8, blows up sparsity\n",
    "# Go back to 500 epochs no sparsity reg, w/ new nan clipping formulation\n",
    "lambda_corr, lambda_mu, lambda_var, lambda_sp = 1, 0, 0, 0\n",
    "\n",
    "ie_model = create_model()\n",
    "ie_model.to(gpu)\n",
    "ei_model = create_model()\n",
    "ei_model.to(gpu)\n",
    "\n",
    "\n",
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def train_model():\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # TODO try LR -3\n",
    "    # TODO try w/o momentum\n",
    "    ie_optim = optim.SGD(ie_model.parameters(), lr=1e-3, momentum=0)\n",
    "    ei_optim = optim.SGD(ei_model.parameters(), lr=1e-3, momentum=0)\n",
    "    \n",
    "    updates_per_outer = n_inner // n_update\n",
    "    num_losses = int(updates_per_outer * n_outer)\n",
    "    losses = torch.empty(size=(num_losses,))\n",
    "    # batch every 20 (ex. tried 50 and was slightly more noisy (but w/ similar mean))\n",
    "    # batch_every = 20\n",
    "    # batch_loss = 0\n",
    "    \n",
    "    for outer_e in range(n_outer):\n",
    "        i = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(i)\n",
    "        \n",
    "        W_initial = compute_initial_recurrent_weights()\n",
    "        W = W_initial.clone().to(gpu)\n",
    "        W.requires_grad_(True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            \n",
    "            clamp_min = torch.zeros_like(W)\n",
    "            clamp_min[ei_update_inds] = ei_min_weight\n",
    "            clamp_min[ie_update_inds] = ie_min_weight\n",
    "            clamp_max = torch.zeros_like(W)\n",
    "            clamp_max[ie_update_inds] = ie_max_weight\n",
    "            clamp_max[ei_update_inds] = ei_max_weight\n",
    "            weight_range = (clamp_min, clamp_max)\n",
    "        \n",
    "        # Initial neuron responses\n",
    "        R = compute_piriform_response(hbar_ff, W, threshold_multiplier)\n",
    "        \n",
    "        # TODO look at loss ratios when we set weights to 0 (so the responses converge to the feedforward input value)\n",
    "        # Intuition: should do worse at decorrelation (or maybe nothing)\n",
    "        # If it does do worse, then there's an error in how we compute losses, if it doesn't do anything, the loss is being computed correctly\n",
    "        # Check intuition of cranking up E->I and I->E to max value (for the update inds) and see what losses are like\n",
    "        # - First only increase E to I, then only I to E, then both\n",
    "        for i in range(1, n_inner + 1):\n",
    "            with_loss = False\n",
    "            detach_grad = False\n",
    "            if i % n_update == 0:\n",
    "                with_loss = True\n",
    "                print(f\"Outer epoch {outer_e}, Inner epoch {i}, Loss: \\t\", end=\"\")\n",
    "            if i % n_track == 0:\n",
    "                detach_grad = True\n",
    "            \n",
    "            loss, overload, W, R = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W, R, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=detach_grad, with_loss=with_loss)\n",
    "            \n",
    "            if with_loss:\n",
    "                final_loss = loss\n",
    "\n",
    "                R_initial = compute_piriform_response(hbar_ff, W_initial, threshold_multiplier)\n",
    "                initial_loss = loss_fn(R_initial, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=False)\n",
    "                total_loss = final_loss / initial_loss\n",
    "                #total_loss += overload\n",
    "                print(f\"Loss ratio: {total_loss}\")\n",
    "                losses[(outer_e * updates_per_outer)  + (i // n_update) - 1] = total_loss.item()\n",
    "                total_loss.backward()\n",
    "                #batch_loss += total_loss\n",
    "                # ie_grad = torch.nn.utils.clip_grad_norm_(ie_model.parameters(), max_norm = 1e5)\n",
    "                # ei_grad = torch.nn.utils.clip_grad_norm_(ei_model.parameters(), max_norm = 1e5)\n",
    "                # print(f\"ie model grad: {ie_grad}\")\n",
    "                # print(f\"ei model grad: {ei_grad}\")\n",
    "                ie_optim.step()  \n",
    "                ei_optim.step()\n",
    "                ie_optim.zero_grad()\n",
    "                ei_optim.zero_grad()\n",
    "            \n",
    "        # if outer_e % batch_every == 0:\n",
    "        #     batch_loss.backward()   \n",
    "        #     ie_optim.step()\n",
    "        #     ei_optim.step()\n",
    "        #     ie_optim.zero_grad()\n",
    "        #     ei_optim.zero_grad()\n",
    "        #     batch_loss = 0\n",
    "                \n",
    "    return losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:20:56.821190Z",
     "start_time": "2024-08-01T18:20:56.742882Z"
    }
   },
   "id": "70723f2b6444f1b2"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer epoch 0, Inner epoch 1, Loss: \tIE over: 0.00013074278831481934\n",
      "EI over: 1104.5550537109375\n",
      "Avg Corr: 0.2978, Corr: 0.5275, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0321122407913208\n",
      "Outer epoch 1, Inner epoch 1, Loss: \tIE over: 0.34556591510772705\n",
      "EI over: 1866.9979248046875\n",
      "Avg Corr: 0.3058, Corr: 0.5614, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9292868375778198\n",
      "Outer epoch 2, Inner epoch 1, Loss: \tIE over: 2.463599443435669\n",
      "EI over: 1658.8349609375\n",
      "Avg Corr: 0.3019, Corr: 0.6041, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9730696082115173\n",
      "Outer epoch 3, Inner epoch 1, Loss: \tIE over: 41.034454345703125\n",
      "EI over: 2678.68896484375\n",
      "Avg Corr: 0.2507, Corr: 0.4236, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0459035634994507\n",
      "Outer epoch 4, Inner epoch 1, Loss: \tIE over: 29.260894775390625\n",
      "EI over: 2078.13037109375\n",
      "Avg Corr: 0.2346, Corr: 0.3959, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9819177389144897\n",
      "Outer epoch 5, Inner epoch 1, Loss: \tIE over: 10.534219741821289\n",
      "EI over: 1014.9340209960938\n",
      "Avg Corr: 0.2990, Corr: 0.5996, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9878153204917908\n",
      "Outer epoch 6, Inner epoch 1, Loss: \tIE over: 343.06591796875\n",
      "EI over: 3105.119140625\n",
      "Avg Corr: 0.3220, Corr: 0.5910, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9998303055763245\n",
      "Outer epoch 7, Inner epoch 1, Loss: \tIE over: 370.2633056640625\n",
      "EI over: 3311.015869140625\n",
      "Avg Corr: 0.3193, Corr: 0.5597, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0011205673217773\n",
      "Outer epoch 8, Inner epoch 1, Loss: \tIE over: 31.928781509399414\n",
      "EI over: 1297.9810791015625\n",
      "Avg Corr: 0.2935, Corr: 0.5196, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9949547648429871\n",
      "Outer epoch 9, Inner epoch 1, Loss: \tIE over: 147.5279541015625\n",
      "EI over: 1971.0908203125\n",
      "Avg Corr: 0.3219, Corr: 0.6130, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9884709119796753\n",
      "Outer epoch 10, Inner epoch 1, Loss: \tIE over: 299.2922058105469\n",
      "EI over: 2489.795166015625\n",
      "Avg Corr: 0.2953, Corr: 0.5640, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0010324716567993\n",
      "Outer epoch 11, Inner epoch 1, Loss: \tIE over: 737.5218505859375\n",
      "EI over: 3990.059814453125\n",
      "Avg Corr: 0.3413, Corr: 0.6351, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0001944303512573\n",
      "Outer epoch 12, Inner epoch 1, Loss: \tIE over: 227.75418090820312\n",
      "EI over: 2129.31201171875\n",
      "Avg Corr: 0.2664, Corr: 0.4215, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.0067272186279297\n",
      "Outer epoch 13, Inner epoch 1, Loss: \tIE over: 229.56964111328125\n",
      "EI over: 2066.7841796875\n",
      "Avg Corr: 0.3290, Corr: 0.6769, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9968949556350708\n",
      "Outer epoch 14, Inner epoch 1, Loss: \tIE over: 119.67192840576172\n",
      "EI over: 1531.8765869140625\n",
      "Avg Corr: 0.3292, Corr: 0.6075, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9948016405105591\n",
      "Outer epoch 15, Inner epoch 1, Loss: \tIE over: 52.31849670410156\n",
      "EI over: 965.5860595703125\n",
      "Avg Corr: 0.3698, Corr: 0.8431, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9926413893699646\n",
      "Outer epoch 16, Inner epoch 1, Loss: \tIE over: 205.71405029296875\n",
      "EI over: 1796.0794677734375\n",
      "Avg Corr: 0.2883, Corr: 0.5534, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9951289296150208\n",
      "Outer epoch 17, Inner epoch 1, Loss: \tIE over: 392.62774658203125\n",
      "EI over: 2399.273681640625\n",
      "Avg Corr: 0.2773, Corr: 0.4605, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 1.000062108039856\n",
      "Outer epoch 18, Inner epoch 1, Loss: \tIE over: 221.69491577148438\n",
      "EI over: 1892.9820556640625\n",
      "Avg Corr: 0.3783, Corr: 0.8044, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9872415065765381\n",
      "Outer epoch 19, Inner epoch 1, Loss: \tIE over: 228.62905883789062\n",
      "EI over: 1794.14697265625\n",
      "Avg Corr: 0.3827, Corr: 0.8338, Mu: 0.0000, Var: 0.0000, Sparsity: 0.0000\n",
      "Loss ratio: 0.9951226711273193\n",
      "Outer epoch 20, Inner epoch 1, Loss: \t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[37], line 104\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m()\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m n_track \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    102\u001B[0m     detach_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 104\u001B[0m loss, overload, W, R \u001B[38;5;241m=\u001B[39m \u001B[43mloss_after_odors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mie_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mei_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mie_update_inds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mei_update_inds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhbar_ff\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreshold_multiplier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplasticity_ie\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplasticity_ei\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_range\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_corr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_mu\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlambda_sp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetach_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdetach_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwith_loss\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_loss:\n\u001B[1;32m    107\u001B[0m     final_loss \u001B[38;5;241m=\u001B[39m loss\n",
      "Cell \u001B[0;32mIn[24], line 8\u001B[0m, in \u001B[0;36mloss_after_odors\u001B[0;34m(ie_model, ei_model, ie_update_inds, ei_update_inds, W_rec, R_current, h_bar_ff, threshold_mult, plasticity_ie, plasticity_ei, weight_decay_rate, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad, with_loss)\u001B[0m\n\u001B[1;32m      4\u001B[0m W_rec\u001B[38;5;241m.\u001B[39mrequires_grad_(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# TODO new paradigm: computing updates across odors and between models in parallel\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# TODO regularize by penalizing high model update?\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m ie_model_updates, ei_model_updates \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_updates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mR_current\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mie_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mei_model\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mie_update_inds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mei_update_inds\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m#     val_tensor = ((1 - plasticity_rate * weight_decay_rate) * W_rec[update_inds]) + updates + (plasticity_rate * odor_update)\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#     condition = torch.logical_and(torch.gt(val_tensor, min_weight), torch.le(val_tensor, max_weight))\u001B[39;00m\n\u001B[1;32m     14\u001B[0m ie_updates \u001B[38;5;241m=\u001B[39m plasticity_ie \u001B[38;5;241m*\u001B[39m (ie_model_updates \u001B[38;5;241m-\u001B[39m weight_decay_rate \u001B[38;5;241m*\u001B[39m W_rec[ie_update_inds])\n",
      "Cell \u001B[0;32mIn[18], line 10\u001B[0m, in \u001B[0;36mcompute_updates\u001B[0;34m(R, models, update_inds)\u001B[0m\n\u001B[1;32m      8\u001B[0m presyn_responses \u001B[38;5;241m=\u001B[39m R[update_inds[i][\u001B[38;5;241m1\u001B[39m], :]\n\u001B[1;32m      9\u001B[0m model_input \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(tensors\u001B[38;5;241m=\u001B[39m(presyn_responses, postsyn_responses), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m---> 10\u001B[0m updates_per_odor \u001B[38;5;241m=\u001B[39m \u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m model_updates \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(updates_per_odor, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39msqueeze(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     12\u001B[0m all_updates\u001B[38;5;241m.\u001B[39mappend(model_updates)\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/modules/activation.py:103\u001B[0m, in \u001B[0;36mReLU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/GitHub/Piriform-Cortex-Plasticity/venv/lib/python3.9/site-packages/torch/nn/functional.py:1500\u001B[0m, in \u001B[0;36mrelu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   1498\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m   1499\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1500\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "losses = train_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T18:21:43.717186Z",
     "start_time": "2024-08-01T18:20:57.000500Z"
    }
   },
   "id": "e34a7ed21ff8c903"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(torch.arange(losses.shape[0]), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss ratio\")\n",
    "    plt.title(\"Loss ratio per realization over time\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.960642Z",
     "start_time": "2024-08-01T17:46:56.958505Z"
    }
   },
   "id": "1b8aabaa70054798"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = correlated_mitral_activity()\n",
    "hbar_ff = compute_feedforward_activity(i)\n",
    "W_random = compute_initial_recurrent_weights()\n",
    "R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.959377Z"
    }
   },
   "id": "a3843b2195b086d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stats at initialization\n",
    "mu_familiar_0 = torch.mean(R_random[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_0 = torch.var(R_random[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_0 = torch.mean(R_random[:num_e, novel_inds], dim=1)\n",
    "sig_novel_0 = torch.var(R_random[:num_e, novel_inds], dim=1)\n",
    "#print(f\"Initialization: \\nFamiliar -  mean: {mu_familiar_0}, var: {sig_familiar_0}\\nNovel -  mean: {mu_novel_0}, var: {sig_novel_0}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.976483Z",
     "start_time": "2024-08-01T17:46:56.961173Z"
    }
   },
   "id": "4da719c5a4a5ebbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "R_random = R_random.to(gpu)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.962883Z"
    }
   },
   "id": "67971e29f9bb692"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_plastic = W_random.detach().clone()\n",
    "R_plastic = R_random.detach().clone()\n",
    "with torch.no_grad():\n",
    "    ie_update_inds = get_update_inds(ie_post, ie_pre, W_plastic)\n",
    "    ei_update_inds = get_update_inds(ei_post, ei_pre, W_plastic)\n",
    "    clamp_min = torch.zeros_like(W_plastic)\n",
    "    clamp_min[ei_update_inds] = ei_min_weight\n",
    "    clamp_min[ie_update_inds] = ie_min_weight\n",
    "    clamp_max = torch.zeros_like(W_plastic)\n",
    "    clamp_max[ie_update_inds] = ie_max_weight\n",
    "    clamp_max[ei_update_inds] = ei_max_weight\n",
    "    weight_range = (clamp_min, clamp_max)\n",
    "\n",
    "\n",
    "# Number of weights from each group to track\n",
    "num_samples = 100\n",
    "W_tracked = torch.empty((n_inner, num_samples, 2))\n",
    "R_tracked = torch.empty((n_inner, num_i))\n",
    "ie_track_inds = torch.randint(0, len(ie_update_inds), size=(num_samples,))\n",
    "ei_track_inds = torch.randint(0, len(ei_update_inds), size=(num_samples,))\n",
    "with torch.no_grad():\n",
    "    for i in range(n_inner):\n",
    "        _, W_plastic, R_plastic = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        W_tracked[i, :, 0] = W_plastic[ie_update_inds][ie_track_inds].detach()\n",
    "        W_tracked[i, :, 1] = W_plastic[ei_update_inds][ei_track_inds].detach()\n",
    "        R_tracked[i, :] = torch.mean(R_plastic[num_e:, :], dim=1).detach()\n",
    "\n",
    "mu_familiar_f = torch.mean(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_f = torch.var(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_f = torch.mean(R_plastic[:num_e, novel_inds], dim=1)\n",
    "sig_novel_f = torch.var(R_plastic[:num_e, novel_inds], dim=1)\n",
    "mu_novel_diff = mu_novel_f - mu_novel_0\n",
    "mu_familiar_diff = mu_familiar_f - mu_familiar_0\n",
    "sig_novel_diff = sig_novel_f - sig_novel_0\n",
    "sig_familiar_diff = sig_familiar_f - sig_familiar_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.964663Z"
    }
   },
   "id": "8aea9b6d1ff27717"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "R_random = R_random.cpu()\n",
    "R_plastic = R_plastic.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.966417Z"
    }
   },
   "id": "26d6bbd72d52ab77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(R_tracked)\n",
    "plt.title(\"I responses across inner epochs\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.968115Z"
    }
   },
   "id": "630eb42d255674cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look at how weights evolve over the course of inner epoch, for a random realization\n",
    "# Store a random set of 10 weights and see how the 10 weights change over the course of the iterations"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.970104Z"
    }
   },
   "id": "ddea879c3b9927e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.hist(torch.flatten(R_random[:num_e]), density=True, label=\"E responses\")\n",
    "    ax1.hist(torch.flatten(R_random[num_e:]), density=True, label=\"I responses\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.hist(torch.flatten(R_plastic[:num_e]), density=True, label=\"E responses\")\n",
    "    ax2.hist(torch.flatten(R_plastic[num_e:]), density=True, label=\"I responses\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Responses\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.971685Z"
    }
   },
   "id": "b9dd48d9b5cdbe58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "R_random[num_e:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.973328Z"
    }
   },
   "id": "92f7a48ffa1db260"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO figure out why I responses keep getting blown up to 0\n",
    "R_plastic[num_e:, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.975049Z"
    }
   },
   "id": "815a783a635f96f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ei_update_inds].cpu().flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.986239Z",
     "start_time": "2024-08-01T17:46:56.976720Z"
    }
   },
   "id": "c2901f2e35c4e550"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ie_update_inds].cpu().flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.978392Z"
    }
   },
   "id": "af71d4a512bc4881"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO model is learning to set all E->I weights to 0, and that's causing I responses to go to 0, then I->E weights can't do anything to fix that\n",
    "torch.unique((W_plastic[ie_update_inds].cpu().flatten()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.979980Z"
    }
   },
   "id": "547affea0c66ded8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    sparsities_0 = sparsity_per_odor(R_random)\n",
    "    spars_novel = sparsities_0[novel_inds]\n",
    "    spars_familiar = sparsities_0[familiar_inds]\n",
    "    ax1.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax1.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    sparsities_f = sparsity_per_odor(R_plastic)\n",
    "    spars_novel = sparsities_f[novel_inds]\n",
    "    spars_familiar = sparsities_f[familiar_inds]\n",
    "    ax2.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax2.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Sparsity per odor\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.981629Z"
    }
   },
   "id": "e970014f84261189"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def verification_set(ie_model, ei_model, runs=10):\n",
    "    corrs = torch.empty((runs,))\n",
    "    ratios = torch.empty((runs,))\n",
    "    spars_change = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        I_ff = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(I_ff)\n",
    "        W_random = compute_initial_recurrent_weights()\n",
    "        R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)\n",
    "        _, initial_corr = odor_corrs(R_random)\n",
    "        spars_initial = sparsity_per_odor(R_random)\n",
    "        initial_spars_diff = torch.abs(torch.mean(spars_initial[novel_inds]) - torch.mean(spars_initial[familiar_inds]))\n",
    "        \n",
    "        ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "        ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "        \n",
    "        for _ in range(n_inner):\n",
    "            _, W_random, R_random = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_random, R_random, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        \n",
    "        _, final_corr = odor_corrs(R_random)\n",
    "        ratio = final_corr / initial_corr\n",
    "        spars_final = sparsity_per_odor(R_random)\n",
    "        # Plot actual difference in the sparsity number, not the percent change ratio\n",
    "        # - Also see whether \n",
    "        final_spars_diff = torch.abs(torch.mean(spars_final[novel_inds]) - torch.mean(spars_final[familiar_inds]))\n",
    "        \n",
    "        print(f\"Corr: {initial_corr} -> {final_corr}, Sparsity: {initial_spars_diff} -> {final_spars_diff}, Loss Ratio: {ratio}\")\n",
    "        corrs[i] = final_corr.item()\n",
    "        ratios[i] = ratio.item()\n",
    "        spars_change[i] = (((final_spars_diff - initial_spars_diff) / initial_spars_diff) * 100).item()\n",
    "        \n",
    "    return corrs, ratios, spars_change"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.983228Z"
    }
   },
   "id": "8a68603cd329249b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrs, ratios, spars_change = verification_set(ie_model, ei_model, runs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.984704Z"
    }
   },
   "id": "94bafcc0031d6f22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(ratios, bins=20)\n",
    "plt.title(\"Loss ratios\")\n",
    "plt.show()\n",
    "plt.hist(spars_change, bins=20)\n",
    "plt.title(\"Percent change in sparsity per odor for novel vs. familiar families\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.986194Z"
    }
   },
   "id": "bfae243066bddc4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spars_change[spars_change < 100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.987518Z"
    }
   },
   "id": "9f1cf70906cd0175"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# version = \"6\"\n",
    "# ie_path = f\"./joint_models/ie_models/ie_model_{version}\"\n",
    "# torch.save(ie_model.state_dict(), ie_path)\n",
    "# ei_path = f\"./joint_models/ei_models/ei_model_{version}\"\n",
    "# torch.save(ei_model.state_dict(), ei_path)\n",
    "\n",
    "# Model 2 - loss ratios between 0.75 and 1, most sparsity changes within 100%\n",
    "# Model 3 - loss ratios between 0.94 and 1.04, reduces sparsity close to 100% (but has NaN values) \n",
    "# Model 4 - reduces sparsity close to 100%, loss ratios between 0.85 and 1 (basically tries to set weights to 0)\n",
    "# Model 5 - loss ratios between 0.9 and 1.1, most sparsity changes go to 0 (negative 100% sparsity change)\n",
    "\n",
    "# Interesting result from model 1 - tries to set all weights to 0\n",
    "#ie_model = create_model()\n",
    "#ie_model.load_state_dict(torch.load(ie_path))\n",
    "#ei_model = create_model()\n",
    "#ei_model.load_state_dict(torch.load(ei_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.988670Z"
    }
   },
   "id": "5279dce2095c309d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_colormap(model, type=\"ie\"):\n",
    "    num_ticks = 100\n",
    "    E_min, E_max = torch.min(R_plastic[:num_e, :]), torch.max(R_plastic[:num_e, :])\n",
    "    I_min, I_max = torch.min(R_plastic[num_e:, :]), torch.max(R_plastic[num_e:, :])\n",
    "    E_vals = torch.linspace(E_min, E_max, num_ticks)\n",
    "    I_vals = torch.linspace(I_min, I_max, num_ticks)\n",
    "    E_coords, I_coords = torch.meshgrid(E_vals, I_vals, indexing=\"ij\")\n",
    "    R_plot = 0\n",
    "    if type == \"ie\":\n",
    "        R_plot = torch.stack((E_coords.flatten(), I_coords.flatten()), dim=1)\n",
    "    elif type == \"ei\":\n",
    "        R_plot = torch.stack((I_coords.flatten(), E_coords.flatten()), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        plasticity_vals = model(R_plot.to(gpu)).squeeze(0).cpu()\n",
    "        \n",
    "    plot = plt.scatter(R_plot[:, 0], R_plot[:, 1], c=plasticity_vals, cmap='rainbow')\n",
    "    clrbar = plt.colorbar(plot)\n",
    "    clrbar.set_label('Model plasticity')\n",
    "    plt.xlabel(\"E responses\")\n",
    "    plt.ylabel(\"I responses\")\n",
    "        \n",
    "    return E_min, E_max, I_min, I_max, R_plot, plasticity_vals"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.990202Z"
    }
   },
   "id": "d0cb8ceeefdbfa5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def fit_model(model, type=\"ie\", degree=3):\n",
    "    E_min, E_max, I_min, I_max, R_plot, plasticity_vals = make_colormap(model, type)\n",
    "    \n",
    "\n",
    "    std = 2 # Take data within 2 std of responses mean\n",
    "    mu_e = torch.mean(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    sig_e = torch.std(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    mu_i = torch.mean(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    sig_i = torch.std(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    e_train_bounds = (torch.max(E_min, mu_e - std*sig_e), torch.min(E_max, mu_e + std*sig_e))\n",
    "    i_train_bounds = (torch.max(I_min, mu_i - std*sig_i), torch.min(I_max, mu_i + std*sig_i))\n",
    "    \n",
    "    reg = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "    \n",
    "    e_inds = torch.logical_and(torch.ge(R_plot[:, 0], e_train_bounds[0]), torch.le(R_plot[:, 0], e_train_bounds[1]))\n",
    "    i_inds = torch.logical_and(torch.ge(R_plot[:, 1], i_train_bounds[0]), torch.le(R_plot[:, 1], i_train_bounds[1]))\n",
    "    b = torch.logical_and(e_inds, i_inds)\n",
    "    X = R_plot[b]\n",
    "    y = plasticity_vals[b].squeeze(0)\n",
    "    \n",
    "    reg = reg.fit(X, y)\n",
    "    # Exclude bias term\n",
    "    coefs = reg.named_steps['linear'].coef_[0][1:]\n",
    "    # How many terms to consider\n",
    "    take = 3\n",
    "    # Take largest magnitude terms\n",
    "    inds = np.argsort(np.abs(coefs))[-take:]\n",
    "\n",
    "    # Exclude bias term\n",
    "    term_list = reg.named_steps['poly'].powers_[1:, :]\n",
    "    terms = f\"[Pre, Post] Powers:\\n\"\n",
    "    for i in range(take - 1, -1, -1):\n",
    "        term = f\"{term_list[inds][i]}: {coefs[inds][i]}\\n\"\n",
    "        terms += term\n",
    "    \n",
    "    print(terms)\n",
    "    \n",
    "    preds = reg.predict(X).ravel()\n",
    "    return X, preds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.991543Z"
    }
   },
   "id": "7c28c037048e3b5a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ie_X, ie_preds = fit_model(ie_model, type=\"ie\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ie_X[:, 0], ie_X[:, 1], c=ie_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"E-I plasticity based on neural responses\")\n",
    "plt.xlabel(\"E responses\")\n",
    "plt.ylabel(\"I responses\")\n",
    "plt.show()\n",
    "\n",
    "ei_X, ei_preds = fit_model(ei_model, type=\"ei\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ei_X[:, 0], ei_X[:, 1], c=ei_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"I-E plasticity based on neural responses\")\n",
    "plt.xlabel(\"I responses\")\n",
    "plt.ylabel(\"E responses\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.992481Z"
    }
   },
   "id": "b7ac6392aab0837e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Do a plot of rho_0 vs rho_final (for ex. 100 random initializations) to see whether the model can decorrelate odors\n",
    "# Even though potentiation is always positive for the set of responses, since we are doing E to I, increasing the connection strength effectively increases inhibition too, so it balances out\n",
    "# Not directly hebbian because that would mean there is some constant c*r_i*r_j, but when one of the responses is decreasing, the resulting potentiation doesn't decrease\n",
    "# Instead, it will be an a * r_i + b * r_j term\n",
    "# These terms come from the polynomial expansion of the function defined by the model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:57.060814Z",
     "start_time": "2024-08-01T17:46:56.993604Z"
    }
   },
   "id": "aa938bb7961846a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Look at statistics of E neurons (mean firing rate across odors, variance, etc) at initialization compared to after plasticity\n",
    "# Also look at sparsity - one across odors and another across neurons (sparsity is essentially 1 minus the square of the coefficient of variation)\n",
    "# We care mostly about the sparsity across neurons (ex between neurons) and what it would be between odors (should be similar between the familiar odors b/c that's where we applied the plasticity) and we also know what it's like between novel odors\n",
    "# We don't want a change in firing rate, because it should be same for novel and familiar odors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.994660Z"
    }
   },
   "id": "6b1fe6484d9fbc92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pct_change = ((W_tracked[:, :, 0] - w_ie) / w_ie) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"E-I weight change\")\n",
    "    plt.show()\n",
    "    \n",
    "    pct_change = ((W_tracked[:, :, 1] - w_ei) / w_ei) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"I-E weight change\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.995682Z"
    }
   },
   "id": "92ed3911273faa8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "W_change = ((W_plastic[ie_update_inds].cpu() - w_ie) / w_ie) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"E-I Percent weight change from initialization\")\n",
    "plt.show()\n",
    "\n",
    "W_change = ((W_plastic[ei_update_inds].cpu() - w_ei) / w_ei) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"I-E Percent weight change from initialization\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.996672Z"
    }
   },
   "id": "804d8df91ba43350"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(mu_familiar_0.detach().cpu() - mu_novel_0.detach().cpu(), bins=50, label=\"Before\")\n",
    "plt.hist(mu_familiar_f.detach().cpu() - mu_novel_f.detach().cpu(), bins=50, label=\"After\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.997803Z"
    }
   },
   "id": "d7d07fb09871512a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(mu_novel_diff, bins=50, label=\"Novel\")\n",
    "plt.hist(mu_familiar_diff, bins=50, label=\"Familiar\")\n",
    "plt.title(\"Difference in mean responses before and after plasticity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.998838Z"
    }
   },
   "id": "dd90208f664a85ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sp_novel_0 = sparsity_per_neuron(R_random, novel_inds)\n",
    "sp_novel_f = sparsity_per_neuron(R_plastic, novel_inds)\n",
    "sp_familiar_0 = sparsity_per_neuron(R_random, familiar_inds)\n",
    "sp_familiar_f = sparsity_per_neuron(R_plastic, familiar_inds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.999877Z"
    }
   },
   "id": "cecc851bc8f43f35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.hist(sp_novel_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_0\")\n",
    "    plt.hist(sp_familiar_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_0\")\n",
    "    plt.hist(sp_novel_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_f\")\n",
    "    plt.hist(sp_familiar_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_f\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.000850Z"
    }
   },
   "id": "3ee33d1ae62b6cd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create scatter plot of odor correlation vs sparsity difference - odors that are highly positively correlated should have similar sparsities across neurons, and those that are highly negatively correlated should have different sparsities across neurons\n",
    "# Check - increase P' all the way to 16, and with lower correlations, there should be less variability between the sparsity for each odor\n",
    "# - would tell us how much the natural spread in sparsity is"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.001884Z"
    }
   },
   "id": "91ce75bd2a06f39a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. Re-run model w/ lower threshold (0 stdev above mean)\n",
    "# 2. Figure out percentage-wise how much the weights actually change (do a hist)\n",
    "# - if it's 1-2% it's too small, we want the weights to change 10-100%, the weights could even change 10-fold\n",
    "# 3. Add metrics to training function - automatically compute sparsity etc (also look at the sparsity between odors, not just the sparsity between neurons, see if it changes for novel vs familiar)\n",
    "# Create bar plot for each number of odors, how many neurons respond to that number of odors (ideally, if we have a threshold of 0 stdev, most neurons should respond to ~4 odors, since on average a neuron will respond to half of the total odors, so 8 odors, and out of those, it should be equal between 4 novel and 4 familiar)\n",
    "# Hypothesis right now - weights aren't changing that much, so we can add more epoch_inner steps (since the gradient isn't blowing up)\n",
    "# Then, try removing plasticity ramp - keep 1e-3 (don't do epoch_inner increase and remove plasticity ramp at same time)\n",
    "# Goal: understand what mechanism the meta-learning discovered that makes correlations smaller (see whether it acts on sparsity etc)\n",
    "# Think of other metrics to quantify network behavior to understand change"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.003211Z"
    }
   },
   "id": "fdcd6174a08701aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.004349Z"
    }
   },
   "id": "c57e8808ab6eb092"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
