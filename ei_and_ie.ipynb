{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.243331Z",
     "start_time": "2024-08-01T17:46:22.473598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4fbc6b56b8cbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274764Z",
     "start_time": "2024-08-01T17:46:22.475263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a1147c4c0a6f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274897Z",
     "start_time": "2024-08-01T17:46:24.148740Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Use smaller network for testing - ex 2000 neurons\n",
    "# Even for the project, doing it for 10^6 neurons would take too long\n",
    "# Problem this creates: test network is denser than actual network b/c we have 10^3 neurons but 10^2 connections per neuron\n",
    "num_neurons = 2000\n",
    "num_i = int(0.1 * num_neurons)\n",
    "num_e = int(0.9 * num_neurons)\n",
    "\n",
    "# Epsilon value close to 0 to prevent nan in division by 0\n",
    "eps = 1e-6\n",
    "\n",
    "# Num excitatory inputs and inhibitory inputs to each neuron (in reality it should be 500 but we reduce it here to make things faster)\n",
    "k = 100\n",
    "\n",
    "# Number of olfactory bulb channels (glomeruli) to each neuron\n",
    "D = 10 ** 3\n",
    "# For each neuron, how many glomeruli inputs it receives (should be 10^2)\n",
    "num_channel_inputs = 100\n",
    "\n",
    "# Number of odors\n",
    "P = 16\n",
    "# Novel activity is up to P // 2, and familiar activity is after\n",
    "novel_inds = torch.arange(0, P // 2)\n",
    "familiar_inds = torch.arange(P // 2, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e13c61f25818ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274933Z",
     "start_time": "2024-08-01T17:46:24.155175Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates sparse adjacency matrix with the given probability of edge connection and size mxn\n",
    "def create_adj_matrix(p, m, n):\n",
    "    # num_connections = int(p * m * n)\n",
    "    # m_coords = torch.randint(0, m, (num_connections,))\n",
    "    # n_coords = torch.randint(0, n, (num_connections,))\n",
    "    # indices = torch.vstack((m_coords, n_coords))\n",
    "    # values = torch.ones(num_connections)\n",
    "    # A_mn = torch.sparse_coo_tensor(indices, values, (m, n))\n",
    "    probs = torch.ones(m, n) * p\n",
    "    A_mn = torch.bernoulli(probs)\n",
    "    return A_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b94961ca601610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274966Z",
     "start_time": "2024-08-01T17:46:24.159685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New way of generating correlations between odors: we want different sets of odors to be correlated differently, so that when we subtract each neuron's mean activity over odors, it doesn't cancel out the variation between odors (if all the odors are correlated the same, they will tend to produce similar values for a single neuron and therefore subtracting by the mean will remove these values and only leave small fluctuations)\n",
    "# So we sample a small set of odors P' and make them linearly independent, and then by multiplying by a P'x P gaussian matrix we project into mitral cell activity space for all P odors, basically making the P odors a linear combination of the set of P' odors (the smaller P' is, the more correlated the resulting set of P odors will be)\n",
    "# We also scale the variance depending on how small P' is, so we will maintain differently correlated odors, just with higher total correlation if P' is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28887b0933bdd67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274997Z",
     "start_time": "2024-08-01T17:46:24.162068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_prime = 4\n",
    "def correlated_mitral_activity():\n",
    "    # Each of the P' odors is independent (correlation of 0)\n",
    "    sigma_p_prime = torch.zeros((P_prime, P_prime)).fill_diagonal_(1)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(P_prime), sigma_p_prime)\n",
    "    p_prime_activity = dist.sample(torch.Size([D]))\n",
    "    var = 1 / P_prime\n",
    "    projection = torch.normal(torch.zeros((P_prime, P)), torch.ones(P_prime, P) * np.sqrt(var))\n",
    "    activity = p_prime_activity @ projection\n",
    "    return activity.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67da2d3fe4b6a081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275027Z",
     "start_time": "2024-08-01T17:46:24.167675Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes in mitral activity I and computes feedforward activity h_bar_ff\n",
    "def compute_feedforward_activity(I):\n",
    "    # Probability that a channel weight will be nonzero\n",
    "    p = num_channel_inputs / D\n",
    "    # Only the first 0.9 * n rows should have this bernoulli number, the rest should be 0 b/c they don't receive a channel input\n",
    "    # Check whether each neuron still receives ~10^2 nonzero inputs or what the distribution actually looks like\n",
    "    # Because when we calculate the adjacency matrix we don't go by row (e.g ensuring each neuron has these ~10^2 connections)\n",
    "    # Alternative: sample from Binomial distribution w/ mean 100\n",
    "    # The output n for each row is the number of nonzero inputs, and you choose a random subset n of the indices for that row and make them 1\n",
    "    with torch.device(gpu):\n",
    "        a = create_adj_matrix(p, num_e, D)\n",
    "        # Inhibitory neurons don't receive channel input\n",
    "        # This is the first simplification, where we neglect the first inhibitory layer I_ff\n",
    "        b = torch.zeros(size=(num_i, D))\n",
    "        W_ff = torch.cat(tensors=(a, b), dim=0)\n",
    "        \n",
    "        h_ff = (W_ff @ I) * (1 / np.sqrt(num_channel_inputs))\n",
    "        h_bar_ff = torch.zeros_like(h_ff)\n",
    "        h_bar_ff[:num_e] = h_ff[:num_e] - torch.mean(h_ff[:num_e], dim=0, keepdim=True)\n",
    "    return h_bar_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23d7ccb6f4b0787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275058Z",
     "start_time": "2024-08-01T17:46:24.170963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_initial_recurrent_weights():\n",
    "    k_ee = k_ei = k_ie = k_ii = k\n",
    "    #p_ee = k_ee / num_e\n",
    "    # k inhibitory inputs to that e neuron, out of num_i total inhibitory neurons gives the connection probability per neuron\n",
    "    p_ei = k_ei / num_i\n",
    "    p_ie = k_ie / num_e\n",
    "    #p_ii = k_ii / num_i\n",
    "    \n",
    "    # Constants\n",
    "    #w_ee = 0.1\n",
    "    w_ei = 0.2\n",
    "    w_ie = 0.5\n",
    "    #w_ii = 0.3\n",
    "    # Ignore ee and ii weights for now:\n",
    "    p_ee = p_ii = w_ee = w_ii = 0\n",
    "    with torch.device(gpu):\n",
    "        W_ee = create_adj_matrix(p_ee, num_e, num_e) * w_ee\n",
    "        W_ei = create_adj_matrix(p_ei, num_e, num_i) * -w_ei\n",
    "        W_ie = create_adj_matrix(p_ie, num_i, num_e) * w_ie\n",
    "        W_ii = create_adj_matrix(p_ii, num_i, num_i) * -w_ii\n",
    "        \n",
    "        # Concat\n",
    "        W_1 = torch.cat(tensors=(W_ee, W_ei), dim=1)\n",
    "        W_2 = torch.cat(tensors=(W_ie, W_ii), dim=1)\n",
    "        W_rec = torch.cat(tensors=(W_1, W_2), dim=0)\n",
    "    \n",
    "    return W_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ae3fd6edb6a55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275088Z",
     "start_time": "2024-08-01T17:46:24.176889Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes activation threshold for neurons, based on the standard deviation of their firing rates across odors\n",
    "# This average standard deviation, multiplied by theta=2, ensures that each neuron will fire for only 5% of odors\n",
    "def compute_threshold(total_input, theta):\n",
    "    #center = torch.mean(torch.mean(total_input, dim=1, keepdim=True), dim=0, keepdim=True)\n",
    "    #shift = torch.std(total_input, dim=1, keepdim=True)\n",
    "    threshold = torch.ones_like(total_input) * theta\n",
    "    # Since inhibitory neurons are linear\n",
    "    threshold[num_e:, :] = 0\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce5f8670e8cb95e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275118Z",
     "start_time": "2024-08-01T17:46:24.180482Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ReLU for excitatory, linear for inhibitory\n",
    "def neuron_activations(X):\n",
    "    # Mask to keep excitatory\n",
    "    mask1 = torch.ones((num_neurons, 1), device=gpu)\n",
    "    mask1[num_e:, :] = 0\n",
    "    # Mask to keep inhibitory\n",
    "    mask2 = torch.zeros((num_neurons, 1), device=gpu)\n",
    "    mask2[num_e:, :] = 1\n",
    "    activation = (torch.relu(X) * mask1) + (X * mask2)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14aa733bbbadf1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275148Z",
     "start_time": "2024-08-01T17:46:24.186449Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes R for each odor, with the activation threshold theta\n",
    "def compute_piriform_response(h_bar_ff, W_rec, threshold_mult):\n",
    "    # The coefficient of x_bar\n",
    "    tau = 1\n",
    "    # time step\n",
    "    dt = 0.1\n",
    "    # Number of time steps\n",
    "    T = 200\n",
    "    \n",
    "    # Initial condition where states are gaussian\n",
    "    mu_0 = 0.\n",
    "    sigma_0 = 0.2\n",
    "    X_0 = torch.normal(mu_0, sigma_0, size=(num_neurons, P))\n",
    "    X = X_0\n",
    "    X = X.to(gpu)\n",
    "    \n",
    "    #pts = []\n",
    "    for i in range(T-2):\n",
    "        with torch.no_grad():\n",
    "            part1 = -1 * X\n",
    "            part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "            part3 = h_bar_ff\n",
    "            dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "            X = X + (dXdt * dt)\n",
    "        # Look at convergence pattern for first odor, assuming that it'll\n",
    "        # be similar across odors (since they are all independent)\n",
    "        #pts.append(torch.mean(dXdt, dim=0)[0].item())\n",
    "   \n",
    "    # On the last 2 iterations only, track the gradient\n",
    "    # TODO increased to 4 just for more coverage\n",
    "    X.requires_grad_(True)\n",
    "    # Turn this into matrix exponential since during convergent dynamics the ReLU activates the same neurons (no longer becomes nonlinearity)\n",
    "    for j in range(2):\n",
    "        part1 = -1 * X\n",
    "        part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "        part3 = h_bar_ff\n",
    "        dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "        X = X + (dXdt * dt)\n",
    "    \n",
    "    # The total input to the neuron at this last time step (should be equivalent to the resulting value of X after this time step, since dxdt = 0 after the recurrent network converges)\n",
    "    total_input = part2 + part3\n",
    "\n",
    "    # TODO fixed threshold across neurons - theta is now the threshold value\n",
    "    threshold = compute_threshold(total_input, 0)\n",
    "    \n",
    "    # Plot derivatives to see if state converged\n",
    "    # plt.plot(torch.arange(T-2), pts)\n",
    "    # plt.show()\n",
    "    R = neuron_activations(X - threshold)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defc0132008cbd72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275177Z",
     "start_time": "2024-08-01T17:46:24.189623Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute dimensionality of activity matrix R for either novel or familiar\n",
    "# def compute_dim(R, odor_inds):\n",
    "#     # Only compute for the excitatory neurons (b/c those are the ones that send signals to rest of brain\n",
    "#     C = torch.cov(R[:num_e, odor_inds[0]:odor_inds[-1]])\n",
    "#     dim = torch.trace(C) ** 2 / torch.trace(C @ C)\n",
    "#     return dim\n",
    "\n",
    "# trace() is invariant for cyclic permutations of a matrix\n",
    "# Since C is symmetric, it can be orthogonally diagonalized into UDU^T where U is composed of orthonormal eigenvectors, U^T = U^-1, and D is a diagonal matrix of eigenvalues\n",
    "# therefore, trace(C) = trace(UDU^T) = trace(DU^TU) = trace(D) = sum(eigvals of C)\n",
    "# Similarly for the denominator, we need to compute the sum of the squared eigenvalues, which is trace(D^2). trace(D^2) = trace(D^2U^TU) = trace(UD^2U^T) = trace((UDU^T)^2)) [by the property of matrix exponentiation for a diagonalizable matrix] = trace(C^2) = trace(C @ C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24911045583425aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275393Z",
     "start_time": "2024-08-01T17:46:24.224277Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def create_model() -> torch.nn.Sequential:\n",
    "    # Number of neurons in hidden layer\n",
    "    hidden_size = 100\n",
    "    # Constrain output to certain values: nn.Hardtanh(-1, 1)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1cd6e5a0aa03ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275425Z",
     "start_time": "2024-08-01T17:46:24.228522Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start and stop indices for the section of W_rec we want to update, respectively \n",
    "# Takes in R matrix (neuron responses for each odor and tuple of update inds representing ie, then ei (each element in that tuple is itself a tuple of (post, pre))\n",
    "def compute_updates(R: torch.Tensor, models: tuple, update_inds: tuple) -> torch.Tensor:\n",
    "    # Compute the same pairs of R_i and R_j for every odor, per model\n",
    "    all_updates = []\n",
    "    for i in range(len(models)):\n",
    "        postsyn_responses = R[update_inds[i][0], :]\n",
    "        presyn_responses = R[update_inds[i][1], :]\n",
    "        model_input = torch.stack(tensors=(presyn_responses, postsyn_responses), dim=2).transpose(1, 0)\n",
    "        updates_per_odor = models[i](model_input)\n",
    "        model_updates = torch.mean(updates_per_odor, dim=0).squeeze(dim=1)\n",
    "        all_updates.append(model_updates)\n",
    "    \n",
    "    return all_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f51d515fadf0381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275457Z",
     "start_time": "2024-08-01T17:46:24.232958Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ie_post = (num_e, num_neurons)\n",
    "# ie_pre = (0, num_e)\n",
    "# ei_post = (0, num_e)\n",
    "# ei_pre = (num_e, num_neurons)\n",
    "# ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "# ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "# # (16, 19953, 2) -> (16, 19953)\n",
    "# a1, a2 = compute_updates(R_random.to(gpu), (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "# #print(a1.shape, a2.shape)\n",
    "# print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8887e40e8a4d89c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275488Z",
     "start_time": "2024-08-01T17:46:24.235295Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def odor_corrs(R):\n",
    "    # We don't care about the actual responses per odor, just about a neuron's fluctuations around its mean response across odors\n",
    "    R_adjusted = R[:num_e] - torch.mean(R[:num_e], dim=1, keepdim=True)\n",
    "    # Each odor becomes a variable, because we want to calculate correlations between them across neurons\n",
    "    R_adjusted.t_()\n",
    "    # Like cov but divides by standard deviations, effectively normalizing the values (the diagonals of the resulting matrix become 1)\n",
    "    corrcoefs = torch.corrcoef(R_adjusted)\n",
    "    # If the responses are 0, variances across neurons will be 0, so denominator of corrcoef is 0, so term becomes nan\n",
    "    # In this case, the responses are \"perfectly correlated\" (bc always same value of 0) so its maximum correlation\n",
    "    # TODO do we need to change this NaN formulation so we propagate grad correctly?\n",
    "    corrcoefs = torch.nan_to_num(corrcoefs, nan=1.0)\n",
    "    # We only care about the correlations between the familiar odors\n",
    "    familiar_corrs = corrcoefs[P//2:P, P//2:P] - torch.eye(P // 2, device=gpu)\n",
    "    corr_sum = torch.sum(familiar_corrs ** 2)\n",
    "    avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "    \n",
    "    return corr_sum, avg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d37265e0e371e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275519Z",
     "start_time": "2024-08-01T17:46:24.241695Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sparsity per odor, across all (E) neurons\n",
    "def sparsity_per_odor(R):\n",
    "    # Epsilon for if we have zero responses\n",
    "    eps = 1e-6\n",
    "    sp_per_odor = 1 - ((torch.sum(R[:num_e], dim=0) ** 2 + eps) / (num_e * (torch.sum(R[:num_e] ** 2, dim=0)) + eps))\n",
    "    # Sparsity nan means that the responses were all 0 for an odor, meaning that its max sparsity of 1\n",
    "    return sp_per_odor\n",
    "\n",
    "# Sparsity per (E) neuron, across a given odor family\n",
    "def sparsity_per_neuron(R, odor_inds):\n",
    "    sp_per_neuron = 1 - (\n",
    "                (torch.sum(R[:num_e, odor_inds], dim=1) ** 2) / ((P // 2) * torch.sum(R[:num_e, odor_inds] ** 2, dim=1)))\n",
    "    return sp_per_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1003ae68e7009d39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275588Z",
     "start_time": "2024-08-01T17:46:24.248644Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to minimize the correlations between values\n",
    "def loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=True):\n",
    "    corr_sum, avg_corr = odor_corrs(R)\n",
    "    corr_loss = (1 / P) * corr_sum\n",
    "    corr_term = lambda_corr * corr_loss\n",
    "    \n",
    "    means = torch.mean(R[:num_e], dim=0)\n",
    "    means_novel = torch.mean(means[novel_inds])\n",
    "    means_familiar = torch.mean(means[familiar_inds])\n",
    "    if torch.abs(means_novel + means_familiar) < eps:\n",
    "        # All means are the same so there's technically no loss\n",
    "        mu_term = 0\n",
    "    else:\n",
    "        mu_term = lambda_mu * (((means_familiar - means_novel + eps) / (means_novel + means_familiar)) ** 2)\n",
    "    \n",
    "    vars = torch.var(R[:num_e], dim=0)\n",
    "    var_novel = torch.mean(vars[novel_inds])\n",
    "    var_familiar = torch.mean(vars[familiar_inds])\n",
    "    if torch.abs(var_novel + var_familiar) < eps:\n",
    "        # All variances are the same so there's technically no loss\n",
    "        var_term = 0\n",
    "    else:\n",
    "        var_term = lambda_var * (((var_familiar - var_novel) / (var_novel + var_familiar)) ** 2)\n",
    "    \n",
    "    sparsities = sparsity_per_odor(R)\n",
    "    spars_novel = torch.mean(sparsities[novel_inds])\n",
    "    spars_familiar = torch.mean(sparsities[familiar_inds])\n",
    "    if torch.abs(spars_novel + spars_familiar) < eps:\n",
    "        # Sparsities are technically the same so the term shouldn't contribute to loss\n",
    "        spars_term = 0\n",
    "    else:\n",
    "        spars_term = lambda_sp * (((spars_familiar - spars_novel) / (spars_novel + spars_familiar)) ** 2)\n",
    "\n",
    "    # Multiply by 1/P^2 for the decorrelation term and by 1 / num_e*K for the (EI) weight regularization term and 1/num_i*K for the (IE) weight regularization term\n",
    "    # Track each term independently to make sure they're on the same scale\n",
    "    # Do it for backprop too \n",
    "    \n",
    "    if do_print:\n",
    "        print(\"Avg Corr: %.4f, Corr: %.4f, Mu: %.4f, Var: %.4f, Sparsity: %.4f\" % (avg_corr, corr_term, mu_term, var_term, spars_term))\n",
    " \n",
    "    loss = corr_term + mu_term + var_term + spars_term\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e8cc5982ed854c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275650Z",
     "start_time": "2024-08-01T17:46:24.251548Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Detaching vs zero grad - should detach because we have a term dependent on the previous model iteration which isn't zero but some constant gradient, accumulated from that model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1252ae4977d8dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275684Z",
     "start_time": "2024-08-01T17:46:24.260339Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_after_odors(ie_model: torch.nn.Sequential, ei_model: torch.nn.Sequential, ie_update_inds, ei_update_inds, W_rec: torch.Tensor, R_current: torch.Tensor, h_bar_ff: torch.Tensor, threshold_mult, plasticity_ie, plasticity_ei, weight_decay_rate, weight_range: tuple, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False):\n",
    "   # First, compute the respective weight updates through the novel and familiar odors from the current neural responses (which are from the current weight matrix)\n",
    "    \n",
    "    W_rec.requires_grad_(True)\n",
    "    \n",
    "    # TODO new paradigm: computing updates across odors and between models in parallel\n",
    "    # TODO regularize by penalizing high model update?\n",
    "    ie_model_updates, ei_model_updates = compute_updates(R_current, (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     val_tensor = ((1 - plasticity_rate * weight_decay_rate) * W_rec[update_inds]) + updates + (plasticity_rate * odor_update)\n",
    "    #     condition = torch.logical_and(torch.gt(val_tensor, min_weight), torch.le(val_tensor, max_weight))\n",
    "    \n",
    "    ie_updates = plasticity_ie * (ie_model_updates - weight_decay_rate * W_rec[ie_update_inds])\n",
    "    ei_updates = plasticity_ei * (ei_model_updates - weight_decay_rate * W_rec[ei_update_inds])\n",
    "    \n",
    "    \n",
    "    ie_cond = torch.logical_and(torch.ge(W_rec[ie_update_inds] + ie_updates, weight_range[0][ie_update_inds]), torch.le(W_rec[ie_update_inds] + ie_updates, weight_range[1][ie_update_inds]))\n",
    "    ei_cond = torch.logical_and(torch.ge(W_rec[ei_update_inds] + ei_updates, weight_range[0][ei_update_inds]), torch.le(W_rec[ei_update_inds] + ei_updates, weight_range[1][ei_update_inds]))\n",
    "    \n",
    "    # TODO clamping updates instead of the weights post-update - change in gradient?\n",
    "    # TODO if we clamp to bounds then we'll see changes in correlation, b/c there is a change in weight instead of none at all\n",
    "    ie_bounded_updates = torch.where(ie_cond, ie_updates, 0)\n",
    "    ei_bounded_updates = torch.where(ei_cond, ei_updates, 0)\n",
    "   \n",
    "    # # TODO penalize model for causing weight to go over threshold\n",
    "    # # Amount below min\n",
    "    # ie_a = torch.relu(weight_range[0][ie_update_inds] - (W_rec[ie_update_inds] + ie_updates))\n",
    "    # # Amount above max\n",
    "    # ie_b = torch.relu((W_rec[ie_update_inds] + ie_updates) - weight_range[1][ie_update_inds])\n",
    "    # ie_over_weight = torch.mean(ie_a + ie_b)\n",
    "    # # Amount below min\n",
    "    # ei_a = torch.relu(weight_range[0][ei_update_inds] - (W_rec[ei_update_inds] + ei_updates))\n",
    "    # # Amount above max\n",
    "    # ei_b = torch.relu((W_rec[ei_update_inds] + ei_updates) - weight_range[1][ei_update_inds])\n",
    "    # ei_over_weight = torch.mean(ei_a + ei_b)\n",
    "\n",
    "    # print(f\"IE: {torch.mean(ie_updates)}: {ie_over_weight}\")\n",
    "    # print(f\"EI: {torch.mean(ei_updates)}: {ei_over_weight}\")\n",
    "    \n",
    "    # #W_rec = W_rec.clamp(min=weight_range[0], max=weight_range[1])\n",
    "    \n",
    "    W_rec = torch.index_put(W_rec, ie_update_inds, ie_bounded_updates, accumulate=True)\n",
    "    W_rec = torch.index_put(W_rec, ei_update_inds, ei_bounded_updates, accumulate=True)\n",
    "     \n",
    "    \n",
    "    R = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "    R_new = R\n",
    "    \n",
    "    if detach_grad:\n",
    "        # We want a new R response tensor which only has the previous weight update but not anything before that\n",
    "        W_rec = W_rec.detach()\n",
    "        R_new = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "\n",
    "    if with_loss:\n",
    "        #print(f\"IE over: {ie_over_weight}\")\n",
    "        #print(f\"EI over: {ei_over_weight}\")\n",
    "        loss = loss_fn(R, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=False)\n",
    "        # overload = ie_over_weight + ei_over_weight\n",
    "    else:\n",
    "        loss = 0\n",
    "        # overload = 0\n",
    "    \n",
    "    return loss, overload, W_rec, R_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22297cdeba801b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275753Z",
     "start_time": "2024-08-01T17:46:24.267072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_update_inds(post, pre, W):\n",
    "    weights_slice = W[post[0]:post[1], pre[0]:pre[1]]\n",
    "    inds = torch.nonzero(weights_slice, as_tuple=True)\n",
    "    update_inds = (inds[0] + post[0], inds[1] + pre[0])\n",
    "    \n",
    "    return update_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c09c7202ebd3f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275720Z",
     "start_time": "2024-08-01T17:46:24.263582Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def test_regime(ie_val, ei_val):\n",
    "    runs = 100\n",
    "    loss_ratios = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        with torch.no_grad():\n",
    "            I = correlated_mitral_activity()\n",
    "            hbar_ff = compute_feedforward_activity(I)\n",
    "            W = compute_initial_recurrent_weights()\n",
    "            R = compute_piriform_response(hbar_ff, W, 0)\n",
    "            initial_loss = loss_fn(R, 1, 0, 0, 0, do_print=False)\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            W[ie_update_inds] = ie_val\n",
    "            W[ei_update_inds] = ei_val\n",
    "            R_0 = compute_piriform_response(hbar_ff, W, 0)\n",
    "            final_loss = loss_fn(R_0, 1, 0, 0, 0, do_print=False)\n",
    "        \n",
    "        total_loss = final_loss / initial_loss\n",
    "        #print(f\"Loss Ratio: {total_loss.item()}\")\n",
    "        loss_ratios[i] = total_loss\n",
    "        \n",
    "    plt.hist(loss_ratios, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cdbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both zero\n",
    "# test_regime(0., 0.)\n",
    "# plt.title(\"Loss Ratios - both 0\")\n",
    "# plt.show()\n",
    "# # IE cranked\n",
    "# test_regime(ie_max_weight, w_ei)\n",
    "# plt.title(\"Loss Ratios - E->I cranked\")\n",
    "# plt.show()\n",
    "# # EI cranked\n",
    "# test_regime(w_ie, ei_min_weight)\n",
    "# plt.title(\"Loss Ratios - I->E cranked\")\n",
    "# plt.show()\n",
    "# # Both cranked\n",
    "# test_regime(ie_max_weight, ei_min_weight)\n",
    "# plt.title(\"Loss Ratios - both cranked\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70723f2b6444f1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:20:56.821190Z",
     "start_time": "2024-08-01T18:20:56.742882Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of independent \"sniffs\" of the 16 odors\n",
    "n_inner = 1\n",
    "# Number of different realizations of the odor sniffing process\n",
    "n_outer = 1000\n",
    "\n",
    "# TODO change back to -3 if too noisy\n",
    "plasticity_rate = 1e-1\n",
    "# Same plasticity rate for EI and IE\n",
    "plasticity_ie = plasticity_ei = plasticity_rate\n",
    "\n",
    "# TODO experiment w/ diff gradient tracking numbers\n",
    "# Number of inner epochs between model updates, n_update <= n_inner\n",
    "n_update = 1\n",
    "# Number of inner epochs across which the gradient is tracked (right now we detach the gradient after each inner epoch), n_track <= n_inner\n",
    "n_track = n_update\n",
    "\n",
    "# n_track = n_update to test formulation where we track the gradient across a subset of the inner epochs and update the model\n",
    "# If n_update > n_track, the model will have multiple \"gradient\" paths of loss accumulated: ex. inner epochs 1-5 and a separate branch of 6-10\n",
    "# If n_update < n_track, it doesn't matter b/c the model will truncate the history past its previous update since the gradient is zeroed\n",
    "\n",
    "# Number of standard deviations from mean, we are trying 0 b/c 1 and 2 is too sparse\n",
    "threshold_multiplier = 0\n",
    "\n",
    "# TODO for now go to simple formulation, no weight decay\n",
    "weight_decay = 0\n",
    "# How much to weight each of the regularization terms\n",
    "# Sparsity = 1000 doesn't improve loss ratios\n",
    "# Sparsity = 100, 500 epochs, loss ratios 1.0-1.8, blows up sparsity\n",
    "# Go back to 500 epochs no sparsity reg, w/ new nan clipping formulation\n",
    "lambda_corr, lambda_mu, lambda_var, lambda_sp = 1, 0, 0, 0\n",
    "\n",
    "ie_model = create_model()\n",
    "ie_model.to(gpu)\n",
    "ei_model = create_model()\n",
    "ei_model.to(gpu)\n",
    "\n",
    "\n",
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def train_model():\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # TODO try larger learning rate\n",
    "    # TODO problem was adaptive threshold - as we tried to boost the weights, the threshold for the response would change too\n",
    "    # So it would negate the effect of reducing the response\n",
    "    ie_optim = optim.SGD(ie_model.parameters(), lr=1e-3, momentum=0)\n",
    "    ei_optim = optim.SGD(ei_model.parameters(), lr=1e-3, momentum=0)\n",
    "    \n",
    "    updates_per_outer = n_inner // n_update\n",
    "    num_losses = int(updates_per_outer * n_outer)\n",
    "    losses = torch.empty(size=(num_losses,))\n",
    "    # batch every 20 (ex. tried 50 and was slightly more noisy (but w/ similar mean))\n",
    "    # batch_every = 20\n",
    "    # batch_loss = 0\n",
    "    \n",
    "    for outer_e in range(n_outer):\n",
    "        i = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(i)\n",
    "        \n",
    "        W_initial = compute_initial_recurrent_weights()\n",
    "        W = W_initial.clone().to(gpu)\n",
    "        W.requires_grad_(True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            \n",
    "            clamp_min = torch.zeros_like(W)\n",
    "            clamp_min[ei_update_inds] = ei_min_weight\n",
    "            clamp_min[ie_update_inds] = ie_min_weight\n",
    "            clamp_max = torch.zeros_like(W)\n",
    "            clamp_max[ie_update_inds] = ie_max_weight\n",
    "            clamp_max[ei_update_inds] = ei_max_weight\n",
    "            weight_range = (clamp_min, clamp_max)\n",
    "        \n",
    "        # Initial neuron responses\n",
    "        R = compute_piriform_response(hbar_ff, W, threshold_multiplier)\n",
    "        \n",
    "        for i in range(1, n_inner + 1):\n",
    "            with_loss = False\n",
    "            detach_grad = False\n",
    "            if i % n_update == 0:\n",
    "                with_loss = True\n",
    "                print(f\"Outer epoch {outer_e}, Inner epoch {i}, Loss: \\t\", end=\"\")\n",
    "            if i % n_track == 0:\n",
    "                detach_grad = True\n",
    "            \n",
    "            loss, overload, W, R = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W, R, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=detach_grad, with_loss=with_loss)\n",
    "            if with_loss:\n",
    "                final_loss = loss\n",
    "\n",
    "                R_initial = compute_piriform_response(hbar_ff, W_initial, threshold_multiplier)\n",
    "                initial_loss = loss_fn(R_initial, lambda_corr, lambda_mu, lambda_var, lambda_sp, do_print=False)\n",
    "                total_loss = final_loss / initial_loss\n",
    "                #total_loss += overload\n",
    "                #print(f\"Loss ratio: {total_loss}\")\n",
    "                losses[(outer_e * updates_per_outer)  + (i // n_update) - 1] = total_loss.item()\n",
    "                total_loss.backward()\n",
    "                #batch_loss += total_loss\n",
    "                # ie_grad = torch.nn.utils.clip_grad_norm_(ie_model.parameters(), max_norm = 1e5)\n",
    "                # ei_grad = torch.nn.utils.clip_grad_norm_(ei_model.parameters(), max_norm = 1e5)\n",
    "                # print(f\"ie model grad: {ie_grad}\")\n",
    "                # print(f\"ei model grad: {ei_grad}\")\n",
    "                ie_optim.step()  \n",
    "                ei_optim.step()\n",
    "                ie_optim.zero_grad()\n",
    "                ei_optim.zero_grad()\n",
    "            \n",
    "        # if outer_e % batch_every == 0:\n",
    "        #     batch_loss.backward()   \n",
    "        #     ie_optim.step()\n",
    "        #     ei_optim.step()\n",
    "        #     ie_optim.zero_grad()\n",
    "        #     ei_optim.zero_grad()\n",
    "        #     batch_loss = 0\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e34a7ed21ff8c903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:21:43.717186Z",
     "start_time": "2024-08-01T18:20:57.000500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer epoch 0, Inner epoch 1, Loss: \tIE: -0.007662446238100529: 0.0\n",
      "EI: 0.029748069122433662: 0.0\n",
      "Outer epoch 1, Inner epoch 1, Loss: \tIE: -0.008025513030588627: 0.0\n",
      "EI: 0.029622573405504227: 0.0\n",
      "Outer epoch 2, Inner epoch 1, Loss: \tIE: -0.00766872288659215: 0.0\n",
      "EI: 0.02948566898703575: 0.0\n",
      "Outer epoch 3, Inner epoch 1, Loss: \tIE: -0.007893997244536877: 0.0\n",
      "EI: 0.02942775748670101: 0.0\n",
      "Outer epoch 4, Inner epoch 1, Loss: \tIE: -0.00785160157829523: 0.0\n",
      "EI: 0.02920854277908802: 0.0\n",
      "Outer epoch 5, Inner epoch 1, Loss: \tIE: -0.008064324036240578: 0.0\n",
      "EI: 0.02921067178249359: 0.0\n",
      "Outer epoch 6, Inner epoch 1, Loss: \tIE: -0.007661957759410143: 0.0\n",
      "EI: 0.02934003621339798: 0.0\n",
      "Outer epoch 7, Inner epoch 1, Loss: \tIE: -0.007480925414711237: 0.0\n",
      "EI: 0.029037805274128914: 0.0\n",
      "Outer epoch 8, Inner epoch 1, Loss: \tIE: -0.008000245317816734: 0.0\n",
      "EI: 0.0292110163718462: 0.0\n",
      "Outer epoch 9, Inner epoch 1, Loss: \tIE: -0.0076034837402403355: 0.0\n",
      "EI: 0.028932448476552963: 0.0\n",
      "Outer epoch 10, Inner epoch 1, Loss: \tIE: -0.007537625730037689: 0.0\n",
      "EI: 0.028726767748594284: 0.0\n",
      "Outer epoch 11, Inner epoch 1, Loss: \tIE: -0.007617908995598555: 0.0\n",
      "EI: 0.028347022831439972: 0.0\n",
      "Outer epoch 12, Inner epoch 1, Loss: \tIE: -0.00798046961426735: 0.0\n",
      "EI: 0.028812484815716743: 0.0\n",
      "Outer epoch 13, Inner epoch 1, Loss: \tIE: -0.008395364508032799: 0.0\n",
      "EI: 0.02849736250936985: 0.0\n",
      "Outer epoch 14, Inner epoch 1, Loss: \tIE: -0.0076767271384596825: 0.0\n",
      "EI: 0.028518324717879295: 0.0\n",
      "Outer epoch 15, Inner epoch 1, Loss: \tIE: -0.007725276052951813: 0.0\n",
      "EI: 0.028464321047067642: 0.0\n",
      "Outer epoch 16, Inner epoch 1, Loss: \tIE: -0.007908765226602554: 0.0\n",
      "EI: 0.028392182663083076: 0.0\n",
      "Outer epoch 17, Inner epoch 1, Loss: \tIE: -0.007657905109226704: 0.0\n",
      "EI: 0.02807672880589962: 0.0\n",
      "Outer epoch 18, Inner epoch 1, Loss: \tIE: -0.007935909554362297: 0.0\n",
      "EI: 0.02826589345932007: 0.0\n",
      "Outer epoch 19, Inner epoch 1, Loss: \tIE: -0.007795845158398151: 0.0\n",
      "EI: 0.028099823743104935: 0.0\n",
      "Outer epoch 20, Inner epoch 1, Loss: \tIE: -0.007708752993494272: 0.0\n",
      "EI: 0.027855848893523216: 0.0\n",
      "Outer epoch 21, Inner epoch 1, Loss: \tIE: -0.00773839745670557: 0.0\n",
      "EI: 0.027867354452610016: 0.0\n",
      "Outer epoch 22, Inner epoch 1, Loss: \tIE: -0.007692540995776653: 0.0\n",
      "EI: 0.02794613689184189: 0.0\n",
      "Outer epoch 23, Inner epoch 1, Loss: \tIE: -0.00801426824182272: 0.0\n",
      "EI: 0.02769777551293373: 0.0\n",
      "Outer epoch 24, Inner epoch 1, Loss: \tIE: -0.0074563343077898026: 0.0\n",
      "EI: 0.027446715161204338: 0.0\n",
      "Outer epoch 25, Inner epoch 1, Loss: \tIE: -0.008055714890360832: 0.0\n",
      "EI: 0.027295874431729317: 0.0\n",
      "Outer epoch 26, Inner epoch 1, Loss: \tIE: -0.007885335944592953: 0.0\n",
      "EI: 0.0274095069617033: 0.0\n",
      "Outer epoch 27, Inner epoch 1, Loss: \tIE: -0.008252370171248913: 0.0\n",
      "EI: 0.027168380096554756: 0.0\n",
      "Outer epoch 28, Inner epoch 1, Loss: \tIE: -0.007526190485805273: 0.0\n",
      "EI: 0.02721248008310795: 0.0\n",
      "Outer epoch 29, Inner epoch 1, Loss: \tIE: -0.007944081909954548: 0.0\n",
      "EI: 0.02706609107553959: 0.0\n",
      "Outer epoch 30, Inner epoch 1, Loss: \tIE: -0.007737201172858477: 0.0\n",
      "EI: 0.027018696069717407: 0.0\n",
      "Outer epoch 31, Inner epoch 1, Loss: \tIE: -0.007607871200889349: 0.0\n",
      "EI: 0.026921266689896584: 0.0\n",
      "Outer epoch 32, Inner epoch 1, Loss: \tIE: -0.008222490549087524: 0.0\n",
      "EI: 0.02700606919825077: 0.0\n",
      "Outer epoch 33, Inner epoch 1, Loss: \tIE: -0.007774931378662586: 0.0\n",
      "EI: 0.02681438811123371: 0.0\n",
      "Outer epoch 34, Inner epoch 1, Loss: \tIE: -0.008039748296141624: 0.0\n",
      "EI: 0.02666598930954933: 0.0\n",
      "Outer epoch 35, Inner epoch 1, Loss: \tIE: -0.0076561798341572285: 0.0\n",
      "EI: 0.026559406891465187: 0.0\n",
      "Outer epoch 36, Inner epoch 1, Loss: \tIE: -0.007700780872255564: 0.0\n",
      "EI: 0.026370471343398094: 0.0\n",
      "Outer epoch 37, Inner epoch 1, Loss: \tIE: -0.00800777692347765: 0.0\n",
      "EI: 0.026597222313284874: 0.0\n",
      "Outer epoch 38, Inner epoch 1, Loss: \tIE: -0.00781303085386753: 0.0\n",
      "EI: 0.026464121416211128: 0.0\n",
      "Outer epoch 39, Inner epoch 1, Loss: \tIE: -0.007744705770164728: 0.0\n",
      "EI: 0.026457421481609344: 0.0\n",
      "Outer epoch 40, Inner epoch 1, Loss: \tIE: -0.008144479244947433: 0.0\n",
      "EI: 0.026200545951724052: 0.0\n",
      "Outer epoch 41, Inner epoch 1, Loss: \tIE: -0.007707357406616211: 0.0\n",
      "EI: 0.026272257789969444: 0.0\n",
      "Outer epoch 42, Inner epoch 1, Loss: \tIE: -0.007152589038014412: 0.0\n",
      "EI: 0.026006493717432022: 0.0\n",
      "Outer epoch 43, Inner epoch 1, Loss: \tIE: -0.0076927002519369125: 0.0\n",
      "EI: 0.02600007876753807: 0.0\n",
      "Outer epoch 44, Inner epoch 1, Loss: \tIE: -0.00799684040248394: 0.0\n",
      "EI: 0.0258907712996006: 0.0\n",
      "Outer epoch 45, Inner epoch 1, Loss: \tIE: -0.00707857683300972: 0.0\n",
      "EI: 0.025658518075942993: 0.0\n",
      "Outer epoch 46, Inner epoch 1, Loss: \tIE: -0.007275036070495844: 0.0\n",
      "EI: 0.025615237653255463: 0.0\n",
      "Outer epoch 47, Inner epoch 1, Loss: \tIE: -0.007897692732512951: 0.0\n",
      "EI: 0.02559106983244419: 0.0\n",
      "Outer epoch 48, Inner epoch 1, Loss: \tIE: -0.007400829344987869: 0.0\n",
      "EI: 0.025698497891426086: 0.0\n",
      "Outer epoch 49, Inner epoch 1, Loss: \tIE: -0.007736408617347479: 0.0\n",
      "EI: 0.02555292844772339: 0.0\n",
      "Outer epoch 50, Inner epoch 1, Loss: \tIE: -0.007614451926201582: 0.0\n",
      "EI: 0.025487421080470085: 0.0\n",
      "Outer epoch 51, Inner epoch 1, Loss: \tIE: -0.008059072308242321: 0.0\n",
      "EI: 0.02537495456635952: 0.0\n",
      "Outer epoch 52, Inner epoch 1, Loss: \tIE: -0.007572929374873638: 0.0\n",
      "EI: 0.025288602337241173: 0.0\n",
      "Outer epoch 53, Inner epoch 1, Loss: \tIE: -0.00751097546890378: 0.0\n",
      "EI: 0.025203272700309753: 0.0\n",
      "Outer epoch 54, Inner epoch 1, Loss: \tIE: -0.007749123964458704: 0.0\n",
      "EI: 0.024878736585378647: 0.0\n",
      "Outer epoch 55, Inner epoch 1, Loss: \tIE: -0.007606903091073036: 0.0\n",
      "EI: 0.025236112996935844: 0.0\n",
      "Outer epoch 56, Inner epoch 1, Loss: \tIE: -0.007816988974809647: 0.0\n",
      "EI: 0.02474270947277546: 0.0\n",
      "Outer epoch 57, Inner epoch 1, Loss: \tIE: -0.0075822798535227776: 0.0\n",
      "EI: 0.024984294548630714: 0.0\n",
      "Outer epoch 58, Inner epoch 1, Loss: \tIE: -0.007757606916129589: 0.0\n",
      "EI: 0.024983832612633705: 0.0\n",
      "Outer epoch 59, Inner epoch 1, Loss: \tIE: -0.007730791345238686: 0.0\n",
      "EI: 0.024809325113892555: 0.0\n",
      "Outer epoch 60, Inner epoch 1, Loss: \tIE: -0.007695096544921398: 0.0\n",
      "EI: 0.024722889065742493: 0.0\n",
      "Outer epoch 61, Inner epoch 1, Loss: \tIE: -0.007400614209473133: 0.0\n",
      "EI: 0.02466011606156826: 0.0\n",
      "Outer epoch 62, Inner epoch 1, Loss: \tIE: -0.007989875040948391: 0.0\n",
      "EI: 0.024618545547127724: 0.0\n",
      "Outer epoch 63, Inner epoch 1, Loss: \tIE: -0.007542388513684273: 0.0\n",
      "EI: 0.02437167428433895: 0.0\n",
      "Outer epoch 64, Inner epoch 1, Loss: \tIE: -0.007179771084338427: 0.0\n",
      "EI: 0.024294408038258553: 0.0\n",
      "Outer epoch 65, Inner epoch 1, Loss: \tIE: -0.007299996446818113: 0.0\n",
      "EI: 0.024140141904354095: 0.0\n",
      "Outer epoch 66, Inner epoch 1, Loss: \tIE: -0.008382572792470455: 0.0\n",
      "EI: 0.023856135085225105: 0.0\n",
      "Outer epoch 67, Inner epoch 1, Loss: \tIE: -0.008210881613194942: 0.0\n",
      "EI: 0.02394777722656727: 0.0\n",
      "Outer epoch 68, Inner epoch 1, Loss: \tIE: -0.007936467416584492: 0.0\n",
      "EI: 0.023991163820028305: 0.0\n",
      "Outer epoch 69, Inner epoch 1, Loss: \tIE: -0.007855670526623726: 0.0\n",
      "EI: 0.02393963560461998: 0.0\n",
      "Outer epoch 70, Inner epoch 1, Loss: \tIE: -0.007444604299962521: 0.0\n",
      "EI: 0.02361883781850338: 0.0\n",
      "Outer epoch 71, Inner epoch 1, Loss: \tIE: -0.007603860460221767: 0.0\n",
      "EI: 0.02367982640862465: 0.0\n",
      "Outer epoch 72, Inner epoch 1, Loss: \tIE: -0.00760863209143281: 0.0\n",
      "EI: 0.02379816398024559: 0.0\n",
      "Outer epoch 73, Inner epoch 1, Loss: \tIE: -0.0073941731825470924: 0.0\n",
      "EI: 0.023513054475188255: 0.0\n",
      "Outer epoch 74, Inner epoch 1, Loss: \tIE: -0.0075135864317417145: 0.0\n",
      "EI: 0.023282233625650406: 0.0\n",
      "Outer epoch 75, Inner epoch 1, Loss: \tIE: -0.0077544343657791615: 0.0\n",
      "EI: 0.02339588850736618: 0.0\n",
      "Outer epoch 76, Inner epoch 1, Loss: \tIE: -0.007498461287468672: 0.0\n",
      "EI: 0.023150337859988213: 0.0\n",
      "Outer epoch 77, Inner epoch 1, Loss: \tIE: -0.007856249809265137: 0.0\n",
      "EI: 0.023312216624617577: 0.0\n",
      "Outer epoch 78, Inner epoch 1, Loss: \tIE: -0.00838105846196413: 0.0\n",
      "EI: 0.022777341306209564: 0.0\n",
      "Outer epoch 79, Inner epoch 1, Loss: \tIE: -0.007753109093755484: 0.0\n",
      "EI: 0.023245632648468018: 0.0\n",
      "Outer epoch 80, Inner epoch 1, Loss: \tIE: -0.0076844231225550175: 0.0\n",
      "EI: 0.02317631058394909: 0.0\n",
      "Outer epoch 81, Inner epoch 1, Loss: \tIE: -0.007903176359832287: 0.0\n",
      "EI: 0.022746654227375984: 0.0\n",
      "Outer epoch 82, Inner epoch 1, Loss: \tIE: -0.007654011715203524: 0.0\n",
      "EI: 0.022420115768909454: 0.0\n",
      "Outer epoch 83, Inner epoch 1, Loss: \tIE: -0.0075676096603274345: 0.0\n",
      "EI: 0.02274451218545437: 0.0\n",
      "Outer epoch 84, Inner epoch 1, Loss: \tIE: -0.007795793004333973: 0.0\n",
      "EI: 0.022754378616809845: 0.0\n",
      "Outer epoch 85, Inner epoch 1, Loss: \tIE: -0.007343861274421215: 0.0\n",
      "EI: 0.022687554359436035: 0.0\n",
      "Outer epoch 86, Inner epoch 1, Loss: \tIE: -0.007464020512998104: 0.0\n",
      "EI: 0.0226767398416996: 0.0\n",
      "Outer epoch 87, Inner epoch 1, Loss: \tIE: -0.008134742267429829: 0.0\n",
      "EI: 0.02238180860877037: 0.0\n",
      "Outer epoch 88, Inner epoch 1, Loss: \tIE: -0.007741723209619522: 0.0\n",
      "EI: 0.022518878802657127: 0.0\n",
      "Outer epoch 89, Inner epoch 1, Loss: \tIE: -0.0071052201092243195: 0.0\n",
      "EI: 0.022475648671388626: 0.0\n",
      "Outer epoch 90, Inner epoch 1, Loss: \tIE: -0.007501684594899416: 0.0\n",
      "EI: 0.022461723536252975: 0.0\n",
      "Outer epoch 91, Inner epoch 1, Loss: \tIE: -0.0076760719530284405: 0.0\n",
      "EI: 0.022347846999764442: 0.0\n",
      "Outer epoch 92, Inner epoch 1, Loss: \tIE: -0.007450583856552839: 0.0\n",
      "EI: 0.022062810137867928: 0.0\n",
      "Outer epoch 93, Inner epoch 1, Loss: \tIE: -0.007620940450578928: 0.0\n",
      "EI: 0.021880444139242172: 0.0\n",
      "Outer epoch 94, Inner epoch 1, Loss: \tIE: -0.0075965942814946175: 0.0\n",
      "EI: 0.021909622475504875: 0.0\n",
      "Outer epoch 95, Inner epoch 1, Loss: \tIE: -0.008024320006370544: 0.0\n",
      "EI: 0.021884270012378693: 0.0\n",
      "Outer epoch 96, Inner epoch 1, Loss: \tIE: -0.0075882477685809135: 0.0\n",
      "EI: 0.021889908239245415: 0.0\n",
      "Outer epoch 97, Inner epoch 1, Loss: \tIE: -0.007565528154373169: 0.0\n",
      "EI: 0.02169349230825901: 0.0\n",
      "Outer epoch 98, Inner epoch 1, Loss: \tIE: -0.007818336598575115: 0.0\n",
      "EI: 0.021296687424182892: 0.0\n",
      "Outer epoch 99, Inner epoch 1, Loss: \tIE: -0.007688721176236868: 0.0\n",
      "EI: 0.02169649489223957: 0.0\n",
      "Outer epoch 100, Inner epoch 1, Loss: \tIE: -0.007831511087715626: 0.0\n",
      "EI: 0.02142949216067791: 0.0\n",
      "Outer epoch 101, Inner epoch 1, Loss: \tIE: -0.007777554914355278: 0.0\n",
      "EI: 0.021059764549136162: 0.0\n",
      "Outer epoch 102, Inner epoch 1, Loss: \tIE: -0.008077875711023808: 0.0\n",
      "EI: 0.021013498306274414: 0.0\n",
      "Outer epoch 103, Inner epoch 1, Loss: \tIE: -0.0073700519278645515: 0.0\n",
      "EI: 0.021338386461138725: 0.0\n",
      "Outer epoch 104, Inner epoch 1, Loss: \tIE: -0.0080140745267272: 0.0\n",
      "EI: 0.020807990804314613: 0.0\n",
      "Outer epoch 105, Inner epoch 1, Loss: \tIE: -0.007550620008260012: 0.0\n",
      "EI: 0.02087203599512577: 0.0\n",
      "Outer epoch 106, Inner epoch 1, Loss: \tIE: -0.008097388781607151: 0.0\n",
      "EI: 0.020866118371486664: 0.0\n",
      "Outer epoch 107, Inner epoch 1, Loss: \tIE: -0.007130514830350876: 0.0\n",
      "EI: 0.020667172968387604: 0.0\n",
      "Outer epoch 108, Inner epoch 1, Loss: \tIE: -0.007578651420772076: 0.0\n",
      "EI: 0.02084410935640335: 0.0\n",
      "Outer epoch 109, Inner epoch 1, Loss: \tIE: -0.007280269172042608: 0.0\n",
      "EI: 0.02085340954363346: 0.0\n",
      "Outer epoch 110, Inner epoch 1, Loss: \tIE: -0.007806212641298771: 0.0\n",
      "EI: 0.02057690918445587: 0.0\n",
      "Outer epoch 111, Inner epoch 1, Loss: \tIE: -0.0073862224817276: 0.0\n",
      "EI: 0.02078065648674965: 0.0\n",
      "Outer epoch 112, Inner epoch 1, Loss: \tIE: -0.007774225901812315: 0.0\n",
      "EI: 0.020375913009047508: 0.0\n",
      "Outer epoch 113, Inner epoch 1, Loss: \tIE: -0.007735786959528923: 0.0\n",
      "EI: 0.02032141573727131: 0.0\n",
      "Outer epoch 114, Inner epoch 1, Loss: \tIE: -0.0074216267094016075: 0.0\n",
      "EI: 0.020428407937288284: 0.0\n",
      "Outer epoch 115, Inner epoch 1, Loss: \tIE: -0.007811401505023241: 0.0\n",
      "EI: 0.02006104588508606: 0.0\n",
      "Outer epoch 116, Inner epoch 1, Loss: \tIE: -0.007664353586733341: 0.0\n",
      "EI: 0.020268943160772324: 0.0\n",
      "Outer epoch 117, Inner epoch 1, Loss: \tIE: -0.007485599722713232: 0.0\n",
      "EI: 0.02017093077301979: 0.0\n",
      "Outer epoch 118, Inner epoch 1, Loss: \tIE: -0.007436440326273441: 0.0\n",
      "EI: 0.01986904814839363: 0.0\n",
      "Outer epoch 119, Inner epoch 1, Loss: \tIE: -0.00784611888229847: 0.0\n",
      "EI: 0.0196128711104393: 0.0\n",
      "Outer epoch 120, Inner epoch 1, Loss: \tIE: -0.0074000973254442215: 0.0\n",
      "EI: 0.019611455500125885: 0.0\n",
      "Outer epoch 121, Inner epoch 1, Loss: \tIE: -0.0074511077255010605: 0.0\n",
      "EI: 0.01961721107363701: 0.0\n",
      "Outer epoch 122, Inner epoch 1, Loss: \tIE: -0.007840213365852833: 0.0\n",
      "EI: 0.01943507231771946: 0.0\n",
      "Outer epoch 123, Inner epoch 1, Loss: \tIE: -0.007627089042216539: 0.0\n",
      "EI: 0.019342638552188873: 0.0\n",
      "Outer epoch 124, Inner epoch 1, Loss: \tIE: -0.007484128698706627: 0.0\n",
      "EI: 0.019435064867138863: 0.0\n",
      "Outer epoch 125, Inner epoch 1, Loss: \tIE: -0.007670064456760883: 0.0\n",
      "EI: 0.01924961246550083: 0.0\n",
      "Outer epoch 126, Inner epoch 1, Loss: \tIE: -0.007554910611361265: 0.0\n",
      "EI: 0.019361773505806923: 0.0\n",
      "Outer epoch 127, Inner epoch 1, Loss: \tIE: -0.007708332967013121: 0.0\n",
      "EI: 0.019191130995750427: 0.0\n",
      "Outer epoch 128, Inner epoch 1, Loss: \tIE: -0.007724076043814421: 0.0\n",
      "EI: 0.01908254809677601: 0.0\n",
      "Outer epoch 129, Inner epoch 1, Loss: \tIE: -0.00783458724617958: 0.0\n",
      "EI: 0.01864497922360897: 0.0\n",
      "Outer epoch 130, Inner epoch 1, Loss: \tIE: -0.007955535314977169: 0.0\n",
      "EI: 0.018826130777597427: 0.0\n",
      "Outer epoch 131, Inner epoch 1, Loss: \tIE: -0.007612043526023626: 0.0\n",
      "EI: 0.018806030973792076: 0.0\n",
      "Outer epoch 132, Inner epoch 1, Loss: \tIE: -0.007532475981861353: 0.0\n",
      "EI: 0.018705997616052628: 0.0\n",
      "Outer epoch 133, Inner epoch 1, Loss: \tIE: -0.008186248131096363: 0.0\n",
      "EI: 0.018421337008476257: 0.0\n",
      "Outer epoch 134, Inner epoch 1, Loss: \tIE: -0.007451755926012993: 0.0\n",
      "EI: 0.018494924530386925: 0.0\n",
      "Outer epoch 135, Inner epoch 1, Loss: \tIE: -0.007407404948025942: 0.0\n",
      "EI: 0.01822049356997013: 0.0\n",
      "Outer epoch 136, Inner epoch 1, Loss: \tIE: -0.007602029945701361: 0.0\n",
      "EI: 0.01817643828690052: 0.0\n",
      "Outer epoch 137, Inner epoch 1, Loss: \tIE: -0.007754933089017868: 0.0\n",
      "EI: 0.01817891374230385: 0.0\n",
      "Outer epoch 138, Inner epoch 1, Loss: \tIE: -0.007652123458683491: 0.0\n",
      "EI: 0.018213238567113876: 0.0\n",
      "Outer epoch 139, Inner epoch 1, Loss: \tIE: -0.007584740873426199: 0.0\n",
      "EI: 0.017904357984662056: 0.0\n",
      "Outer epoch 140, Inner epoch 1, Loss: \tIE: -0.007565157022327185: 0.0\n",
      "EI: 0.01798318512737751: 0.0\n",
      "Outer epoch 141, Inner epoch 1, Loss: \tIE: -0.007609244901686907: 0.0\n",
      "EI: 0.017848482355475426: 0.0\n",
      "Outer epoch 142, Inner epoch 1, Loss: \tIE: -0.007803439162671566: 0.0\n",
      "EI: 0.017684968188405037: 0.0\n",
      "Outer epoch 143, Inner epoch 1, Loss: \tIE: -0.007702454924583435: 0.0\n",
      "EI: 0.017738966271281242: 0.0\n",
      "Outer epoch 144, Inner epoch 1, Loss: \tIE: -0.00783656444400549: 0.0\n",
      "EI: 0.017656739801168442: 0.0\n",
      "Outer epoch 145, Inner epoch 1, Loss: \tIE: -0.007598889991641045: 0.0\n",
      "EI: 0.01749700866639614: 0.0\n",
      "Outer epoch 146, Inner epoch 1, Loss: \tIE: -0.0075742932967841625: 0.0\n",
      "EI: 0.017314434051513672: 0.0\n",
      "Outer epoch 147, Inner epoch 1, Loss: \tIE: -0.007626046426594257: 0.0\n",
      "EI: 0.017241980880498886: 0.0\n",
      "Outer epoch 148, Inner epoch 1, Loss: \tIE: -0.007979210466146469: 0.0\n",
      "EI: 0.016975706443190575: 0.0\n",
      "Outer epoch 149, Inner epoch 1, Loss: \tIE: -0.00769448047503829: 0.0\n",
      "EI: 0.01731853000819683: 0.0\n",
      "Outer epoch 150, Inner epoch 1, Loss: \tIE: -0.007562627550214529: 0.0\n",
      "EI: 0.017045384272933006: 0.0\n",
      "Outer epoch 151, Inner epoch 1, Loss: \tIE: -0.007584649603813887: 0.0\n",
      "EI: 0.01697097346186638: 0.0\n",
      "Outer epoch 152, Inner epoch 1, Loss: \tIE: -0.007479378487914801: 0.0\n",
      "EI: 0.01692051813006401: 0.0\n",
      "Outer epoch 153, Inner epoch 1, Loss: \tIE: -0.007651542779058218: 0.0\n",
      "EI: 0.01686364784836769: 0.0\n",
      "Outer epoch 154, Inner epoch 1, Loss: \tIE: -0.007692096289247274: 0.0\n",
      "EI: 0.01669606752693653: 0.0\n",
      "Outer epoch 155, Inner epoch 1, Loss: \tIE: -0.0071830349043011665: 0.0\n",
      "EI: 0.01676437072455883: 0.0\n",
      "Outer epoch 156, Inner epoch 1, Loss: \tIE: -0.007445392198860645: 0.0\n",
      "EI: 0.016399547457695007: 0.0\n",
      "Outer epoch 157, Inner epoch 1, Loss: \tIE: -0.007800301071256399: 0.0\n",
      "EI: 0.016336573287844658: 0.0\n",
      "Outer epoch 158, Inner epoch 1, Loss: \tIE: -0.007221799343824387: 0.0\n",
      "EI: 0.016535835340619087: 0.0\n",
      "Outer epoch 159, Inner epoch 1, Loss: \tIE: -0.007412616163492203: 0.0\n",
      "EI: 0.016574935987591743: 0.0\n",
      "Outer epoch 160, Inner epoch 1, Loss: \tIE: -0.007531729992479086: 0.0\n",
      "EI: 0.016320502385497093: 0.0\n",
      "Outer epoch 161, Inner epoch 1, Loss: \tIE: -0.007594177965074778: 0.0\n",
      "EI: 0.015811404213309288: 0.0\n",
      "Outer epoch 162, Inner epoch 1, Loss: \tIE: -0.007615814451128244: 0.0\n",
      "EI: 0.016049738973379135: 0.0\n",
      "Outer epoch 163, Inner epoch 1, Loss: \tIE: -0.007588142994791269: 0.0\n",
      "EI: 0.016028517857193947: 0.0\n",
      "Outer epoch 164, Inner epoch 1, Loss: \tIE: -0.00785983819514513: 0.0\n",
      "EI: 0.01580883376300335: 0.0\n",
      "Outer epoch 165, Inner epoch 1, Loss: \tIE: -0.007755917496979237: 0.0\n",
      "EI: 0.01571369357407093: 0.0\n",
      "Outer epoch 166, Inner epoch 1, Loss: \tIE: -0.007797652389854193: 0.0\n",
      "EI: 0.01575831137597561: 0.0\n",
      "Outer epoch 167, Inner epoch 1, Loss: \tIE: -0.007579701952636242: 0.0\n",
      "EI: 0.015640858560800552: 0.0\n",
      "Outer epoch 168, Inner epoch 1, Loss: \tIE: -0.00729441549628973: 0.0\n",
      "EI: 0.015536597929894924: 0.0\n",
      "Outer epoch 169, Inner epoch 1, Loss: \tIE: -0.007410877384245396: 0.0\n",
      "EI: 0.015787383541464806: 0.0\n",
      "Outer epoch 170, Inner epoch 1, Loss: \tIE: -0.007583781611174345: 0.0\n",
      "EI: 0.015425342135131359: 0.0\n",
      "Outer epoch 171, Inner epoch 1, Loss: \tIE: -0.007412058766931295: 0.0\n",
      "EI: 0.01546057965606451: 0.0\n",
      "Outer epoch 172, Inner epoch 1, Loss: \tIE: -0.007088745012879372: 0.0\n",
      "EI: 0.015380393713712692: 0.0\n",
      "Outer epoch 173, Inner epoch 1, Loss: \tIE: -0.0072642164304852486: 0.0\n",
      "EI: 0.015237049199640751: 0.0\n",
      "Outer epoch 174, Inner epoch 1, Loss: \tIE: -0.007110714912414551: 0.0\n",
      "EI: 0.015127107501029968: 0.0\n",
      "Outer epoch 175, Inner epoch 1, Loss: \tIE: -0.007453675381839275: 0.0\n",
      "EI: 0.015177871100604534: 0.0\n",
      "Outer epoch 176, Inner epoch 1, Loss: \tIE: -0.007508682087063789: 0.0\n",
      "EI: 0.014843343757092953: 0.0\n",
      "Outer epoch 177, Inner epoch 1, Loss: \tIE: -0.00720193749293685: 0.0\n",
      "EI: 0.015062152408063412: 0.0\n",
      "Outer epoch 178, Inner epoch 1, Loss: \tIE: -0.007079727482050657: 0.0\n",
      "EI: 0.01502305082976818: 0.0\n",
      "Outer epoch 179, Inner epoch 1, Loss: \tIE: -0.007657067384570837: 0.0\n",
      "EI: 0.01491648517549038: 0.0\n",
      "Outer epoch 180, Inner epoch 1, Loss: \tIE: -0.007626072969287634: 0.0\n",
      "EI: 0.014918073080480099: 0.0\n",
      "Outer epoch 181, Inner epoch 1, Loss: \tIE: -0.007388096768409014: 0.0\n",
      "EI: 0.014143818989396095: 0.0\n",
      "Outer epoch 182, Inner epoch 1, Loss: \tIE: -0.007693159859627485: 0.0\n",
      "EI: 0.014395508915185928: 0.0\n",
      "Outer epoch 183, Inner epoch 1, Loss: \tIE: -0.007664916105568409: 0.0\n",
      "EI: 0.014379371888935566: 0.0\n",
      "Outer epoch 184, Inner epoch 1, Loss: \tIE: -0.007216849364340305: 0.0\n",
      "EI: 0.014452364295721054: 0.0\n",
      "Outer epoch 185, Inner epoch 1, Loss: \tIE: -0.00796345341950655: 0.0\n",
      "EI: 0.013715297915041447: 0.0\n",
      "Outer epoch 186, Inner epoch 1, Loss: \tIE: -0.007600543089210987: 0.0\n",
      "EI: 0.014035084284842014: 0.0\n",
      "Outer epoch 187, Inner epoch 1, Loss: \tIE: -0.007894235663115978: 0.0\n",
      "EI: 0.013770931400358677: 0.0\n",
      "Outer epoch 188, Inner epoch 1, Loss: \tIE: -0.007836505770683289: 0.0\n",
      "EI: 0.013905101455748081: 0.0\n",
      "Outer epoch 189, Inner epoch 1, Loss: \tIE: -0.0076641663908958435: 0.0\n",
      "EI: 0.013566910289227962: 0.0\n",
      "Outer epoch 190, Inner epoch 1, Loss: \tIE: -0.00736455712467432: 0.0\n",
      "EI: 0.013977793045341969: 0.0\n",
      "Outer epoch 191, Inner epoch 1, Loss: \tIE: -0.007413926068693399: 0.0\n",
      "EI: 0.013825423084199429: 0.0\n",
      "Outer epoch 192, Inner epoch 1, Loss: \tIE: -0.007362574338912964: 0.0\n",
      "EI: 0.013595514930784702: 0.0\n",
      "Outer epoch 193, Inner epoch 1, Loss: \tIE: -0.007768349722027779: 0.0\n",
      "EI: 0.013339810073375702: 0.0\n",
      "Outer epoch 194, Inner epoch 1, Loss: \tIE: -0.0075361160561442375: 0.0\n",
      "EI: 0.013220974244177341: 0.0\n",
      "Outer epoch 195, Inner epoch 1, Loss: \tIE: -0.00741439126431942: 0.0\n",
      "EI: 0.013244714587926865: 0.0\n",
      "Outer epoch 196, Inner epoch 1, Loss: \tIE: -0.007367692422121763: 0.0\n",
      "EI: 0.013373007997870445: 0.0\n",
      "Outer epoch 197, Inner epoch 1, Loss: \tIE: -0.007550818845629692: 0.0\n",
      "EI: 0.013078438118100166: 0.0\n",
      "Outer epoch 198, Inner epoch 1, Loss: \tIE: -0.007167338859289885: 0.0\n",
      "EI: 0.01340874657034874: 0.0\n",
      "Outer epoch 199, Inner epoch 1, Loss: \tIE: -0.007141142152249813: 0.0\n",
      "EI: 0.01299543958157301: 0.0\n",
      "Outer epoch 200, Inner epoch 1, Loss: \tIE: -0.007329167798161507: 0.0\n",
      "EI: 0.013071523047983646: 0.0\n",
      "Outer epoch 201, Inner epoch 1, Loss: \tIE: -0.007817529141902924: 0.0\n",
      "EI: 0.012765901163220406: 0.0\n",
      "Outer epoch 202, Inner epoch 1, Loss: \tIE: -0.007334255147725344: 0.0\n",
      "EI: 0.012861104682087898: 0.0\n",
      "Outer epoch 203, Inner epoch 1, Loss: \tIE: -0.007455603219568729: 0.0\n",
      "EI: 0.01283293031156063: 0.0\n",
      "Outer epoch 204, Inner epoch 1, Loss: \tIE: -0.007713484577834606: 0.0\n",
      "EI: 0.012410302646458149: 0.0\n",
      "Outer epoch 205, Inner epoch 1, Loss: \tIE: -0.007729904260486364: 0.0\n",
      "EI: 0.012403573840856552: 0.0\n",
      "Outer epoch 206, Inner epoch 1, Loss: \tIE: -0.007292797788977623: 0.0\n",
      "EI: 0.012436571530997753: 0.0\n",
      "Outer epoch 207, Inner epoch 1, Loss: \tIE: -0.007472157943993807: 0.0\n",
      "EI: 0.012299474328756332: 0.0\n",
      "Outer epoch 208, Inner epoch 1, Loss: \tIE: -0.007390959188342094: 0.0\n",
      "EI: 0.0123907420784235: 0.0\n",
      "Outer epoch 209, Inner epoch 1, Loss: \tIE: -0.007751210127025843: 0.0\n",
      "EI: 0.011828417889773846: 0.0\n",
      "Outer epoch 210, Inner epoch 1, Loss: \tIE: -0.007441147696226835: 0.0\n",
      "EI: 0.012045202776789665: 0.0\n",
      "Outer epoch 211, Inner epoch 1, Loss: \tIE: -0.007194967474788427: 0.0\n",
      "EI: 0.012250690720975399: 0.0\n",
      "Outer epoch 212, Inner epoch 1, Loss: \tIE: -0.007677809800952673: 0.0\n",
      "EI: 0.012016295455396175: 0.0\n",
      "Outer epoch 213, Inner epoch 1, Loss: \tIE: -0.007546672597527504: 0.0\n",
      "EI: 0.012002573348581791: 0.0\n",
      "Outer epoch 214, Inner epoch 1, Loss: \tIE: -0.0071665458381175995: 0.0\n",
      "EI: 0.01204939465969801: 0.0\n",
      "Outer epoch 215, Inner epoch 1, Loss: \tIE: -0.007364683318883181: 0.0\n",
      "EI: 0.011697967536747456: 0.0\n",
      "Outer epoch 216, Inner epoch 1, Loss: \tIE: -0.007318406831473112: 0.0\n",
      "EI: 0.011931661516427994: 0.0\n",
      "Outer epoch 217, Inner epoch 1, Loss: \tIE: -0.00798800215125084: 0.0\n",
      "EI: 0.011436949484050274: 0.0\n",
      "Outer epoch 218, Inner epoch 1, Loss: \tIE: -0.007454282604157925: 0.0\n",
      "EI: 0.011579935438930988: 0.0\n",
      "Outer epoch 219, Inner epoch 1, Loss: \tIE: -0.007476676721125841: 0.0\n",
      "EI: 0.011307652108371258: 0.0\n",
      "Outer epoch 220, Inner epoch 1, Loss: \tIE: -0.007720292545855045: 0.0\n",
      "EI: 0.01123450044542551: 0.0\n",
      "Outer epoch 221, Inner epoch 1, Loss: \tIE: -0.007281262427568436: 0.0\n",
      "EI: 0.011410619132220745: 0.0\n",
      "Outer epoch 222, Inner epoch 1, Loss: \tIE: -0.007597645744681358: 0.0\n",
      "EI: 0.011264333501458168: 0.0\n",
      "Outer epoch 223, Inner epoch 1, Loss: \tIE: -0.007491056341677904: 0.0\n",
      "EI: 0.011000406928360462: 0.0\n",
      "Outer epoch 224, Inner epoch 1, Loss: \tIE: -0.007578733842819929: 0.0\n",
      "EI: 0.011066583916544914: 0.0\n",
      "Outer epoch 225, Inner epoch 1, Loss: \tIE: -0.007691782899200916: 0.0\n",
      "EI: 0.010897502303123474: 0.0\n",
      "Outer epoch 226, Inner epoch 1, Loss: \tIE: -0.007347726263105869: 0.0\n",
      "EI: 0.010862412862479687: 0.0\n",
      "Outer epoch 227, Inner epoch 1, Loss: \tIE: -0.007527102250605822: 0.0\n",
      "EI: 0.011056289076805115: 0.0\n",
      "Outer epoch 228, Inner epoch 1, Loss: \tIE: -0.007762537337839603: 0.0\n",
      "EI: 0.010658658109605312: 0.0\n",
      "Outer epoch 229, Inner epoch 1, Loss: \tIE: -0.007351499050855637: 0.0\n",
      "EI: 0.0105682872235775: 0.0\n",
      "Outer epoch 230, Inner epoch 1, Loss: \tIE: -0.007068396080285311: 0.0\n",
      "EI: 0.010684913955628872: 0.0\n",
      "Outer epoch 231, Inner epoch 1, Loss: \tIE: -0.007447706535458565: 0.0\n",
      "EI: 0.010382656008005142: 0.0\n",
      "Outer epoch 232, Inner epoch 1, Loss: \tIE: -0.007146930322051048: 0.0\n",
      "EI: 0.010547342710196972: 0.0\n",
      "Outer epoch 233, Inner epoch 1, Loss: \tIE: -0.006892379838973284: 0.0\n",
      "EI: 0.01046478096395731: 0.0\n",
      "Outer epoch 234, Inner epoch 1, Loss: \tIE: -0.007378351408988237: 0.0\n",
      "EI: 0.010410301387310028: 0.0\n",
      "Outer epoch 235, Inner epoch 1, Loss: \tIE: -0.00775325670838356: 0.0\n",
      "EI: 0.01007632352411747: 0.0\n",
      "Outer epoch 236, Inner epoch 1, Loss: \tIE: -0.007588132284581661: 0.0\n",
      "EI: 0.010054362006485462: 0.0\n",
      "Outer epoch 237, Inner epoch 1, Loss: \tIE: -0.0076715475879609585: 0.0\n",
      "EI: 0.010014683939516544: 0.0\n",
      "Outer epoch 238, Inner epoch 1, Loss: \tIE: -0.00736232940107584: 0.0\n",
      "EI: 0.009962317533791065: 0.0\n",
      "Outer epoch 239, Inner epoch 1, Loss: \tIE: -0.007382828276604414: 0.0\n",
      "EI: 0.009875443764030933: 0.0\n",
      "Outer epoch 240, Inner epoch 1, Loss: \tIE: -0.007309938780963421: 0.0\n",
      "EI: 0.00998639315366745: 0.0\n",
      "Outer epoch 241, Inner epoch 1, Loss: \tIE: -0.007027521263808012: 0.0\n",
      "EI: 0.009808102622628212: 0.0\n",
      "Outer epoch 242, Inner epoch 1, Loss: \tIE: -0.007205515168607235: 0.0\n",
      "EI: 0.00973583571612835: 0.0\n",
      "Outer epoch 243, Inner epoch 1, Loss: \tIE: -0.007386051118373871: 0.0\n",
      "EI: 0.009715124033391476: 0.0\n",
      "Outer epoch 244, Inner epoch 1, Loss: \tIE: -0.007155510131269693: 0.0\n",
      "EI: 0.009654125198721886: 0.0\n",
      "Outer epoch 245, Inner epoch 1, Loss: \tIE: -0.0078001124784350395: 0.0\n",
      "EI: 0.009077263064682484: 0.0\n",
      "Outer epoch 246, Inner epoch 1, Loss: \tIE: -0.007260929327458143: 0.0\n",
      "EI: 0.00935071986168623: 0.0\n",
      "Outer epoch 247, Inner epoch 1, Loss: \tIE: -0.007332005072385073: 0.0\n",
      "EI: 0.009388838894665241: 0.0\n",
      "Outer epoch 248, Inner epoch 1, Loss: \tIE: -0.007117107044905424: 0.0\n",
      "EI: 0.009423519484698772: 0.0\n",
      "Outer epoch 249, Inner epoch 1, Loss: \tIE: -0.0071526868268847466: 0.0\n",
      "EI: 0.009202191606163979: 0.0\n",
      "Outer epoch 250, Inner epoch 1, Loss: \tIE: -0.007245159242302179: 0.0\n",
      "EI: 0.008989985100924969: 0.0\n",
      "Outer epoch 251, Inner epoch 1, Loss: \tIE: -0.007424958981573582: 0.0\n",
      "EI: 0.008825833909213543: 0.0\n",
      "Outer epoch 252, Inner epoch 1, Loss: \tIE: -0.007421801332384348: 0.0\n",
      "EI: 0.008779214695096016: 0.0\n",
      "Outer epoch 253, Inner epoch 1, Loss: \tIE: -0.006861461326479912: 0.0\n",
      "EI: 0.00906030647456646: 0.0\n",
      "Outer epoch 254, Inner epoch 1, Loss: \tIE: -0.007051181048154831: 0.0\n",
      "EI: 0.008924967609345913: 0.0\n",
      "Outer epoch 255, Inner epoch 1, Loss: \tIE: -0.007376818917691708: 0.0\n",
      "EI: 0.008651555515825748: 0.0\n",
      "Outer epoch 256, Inner epoch 1, Loss: \tIE: -0.007557637989521027: 0.0\n",
      "EI: 0.008490643464028835: 0.0\n",
      "Outer epoch 257, Inner epoch 1, Loss: \tIE: -0.007038069423288107: 0.0\n",
      "EI: 0.008575527928769588: 0.0\n",
      "Outer epoch 258, Inner epoch 1, Loss: \tIE: -0.007205327041447163: 0.0\n",
      "EI: 0.00840786099433899: 0.0\n",
      "Outer epoch 259, Inner epoch 1, Loss: \tIE: -0.007093926891684532: 0.0\n",
      "EI: 0.008381811901926994: 0.0\n",
      "Outer epoch 260, Inner epoch 1, Loss: \tIE: -0.0072981202974915504: 0.0\n",
      "EI: 0.008272909559309483: 0.0\n",
      "Outer epoch 261, Inner epoch 1, Loss: \tIE: -0.007683380972594023: 0.0\n",
      "EI: 0.007651146035641432: 0.0\n",
      "Outer epoch 262, Inner epoch 1, Loss: \tIE: -0.0076150549575686455: 0.0\n",
      "EI: 0.0076379310339689255: 0.0\n",
      "Outer epoch 263, Inner epoch 1, Loss: \tIE: -0.0076126703061163425: 0.0\n",
      "EI: 0.007927210070192814: 0.0\n",
      "Outer epoch 264, Inner epoch 1, Loss: \tIE: -0.00700004119426012: 0.0\n",
      "EI: 0.008041029796004295: 0.0\n",
      "Outer epoch 265, Inner epoch 1, Loss: \tIE: -0.0078051211312413216: 0.0\n",
      "EI: 0.007570207584649324: 0.0\n",
      "Outer epoch 266, Inner epoch 1, Loss: \tIE: -0.007078353315591812: 0.0\n",
      "EI: 0.0075651854276657104: 0.0\n",
      "Outer epoch 267, Inner epoch 1, Loss: \tIE: -0.007314679678529501: 0.0\n",
      "EI: 0.007780289277434349: 0.0\n",
      "Outer epoch 268, Inner epoch 1, Loss: \tIE: -0.007378709968179464: 0.0\n",
      "EI: 0.007515733130276203: 0.0\n",
      "Outer epoch 269, Inner epoch 1, Loss: \tIE: -0.00722910650074482: 0.0\n",
      "EI: 0.007432741113007069: 0.0\n",
      "Outer epoch 270, Inner epoch 1, Loss: \tIE: -0.006924355868250132: 0.0\n",
      "EI: 0.007617818657308817: 0.0\n",
      "Outer epoch 271, Inner epoch 1, Loss: \tIE: -0.00740106962621212: 0.0\n",
      "EI: 0.007507236674427986: 0.0\n",
      "Outer epoch 272, Inner epoch 1, Loss: \tIE: -0.007102937437593937: 0.0\n",
      "EI: 0.00748472148552537: 0.0\n",
      "Outer epoch 273, Inner epoch 1, Loss: \tIE: -0.007682813331484795: 0.0\n",
      "EI: 0.007005719002336264: 0.0\n",
      "Outer epoch 274, Inner epoch 1, Loss: \tIE: -0.007589093875139952: 0.0\n",
      "EI: 0.006842090282589197: 0.0\n",
      "Outer epoch 275, Inner epoch 1, Loss: \tIE: -0.006846869830042124: 0.0\n",
      "EI: 0.007172311190515757: 0.0\n",
      "Outer epoch 276, Inner epoch 1, Loss: \tIE: -0.006973673589527607: 0.0\n",
      "EI: 0.007431334350258112: 0.0\n",
      "Outer epoch 277, Inner epoch 1, Loss: \tIE: -0.007700745947659016: 0.0\n",
      "EI: 0.0067313797771930695: 0.0\n",
      "Outer epoch 278, Inner epoch 1, Loss: \tIE: -0.007404673378914595: 0.0\n",
      "EI: 0.006726217456161976: 0.0\n",
      "Outer epoch 279, Inner epoch 1, Loss: \tIE: -0.007608386222273111: 0.0\n",
      "EI: 0.00640877103433013: 0.0\n",
      "Outer epoch 280, Inner epoch 1, Loss: \tIE: -0.00722451601177454: 0.0\n",
      "EI: 0.0067963567562401295: 0.0\n",
      "Outer epoch 281, Inner epoch 1, Loss: \tIE: -0.007369404658675194: 0.0\n",
      "EI: 0.006463349796831608: 0.0\n",
      "Outer epoch 282, Inner epoch 1, Loss: \tIE: -0.00725289061665535: 0.0\n",
      "EI: 0.00660561490803957: 0.0\n",
      "Outer epoch 283, Inner epoch 1, Loss: \tIE: -0.007149199489504099: 0.0\n",
      "EI: 0.00652597239241004: 0.0\n",
      "Outer epoch 284, Inner epoch 1, Loss: \tIE: -0.00757085345685482: 0.0\n",
      "EI: 0.0062781162559986115: 0.0\n",
      "Outer epoch 285, Inner epoch 1, Loss: \tIE: -0.00756461825221777: 0.0\n",
      "EI: 0.006223886273801327: 0.0\n",
      "Outer epoch 286, Inner epoch 1, Loss: \tIE: -0.007093689404428005: 0.0\n",
      "EI: 0.006465369835495949: 0.0\n",
      "Outer epoch 287, Inner epoch 1, Loss: \tIE: -0.007360378280282021: 0.0\n",
      "EI: 0.006391850300133228: 0.0\n",
      "Outer epoch 288, Inner epoch 1, Loss: \tIE: -0.007868166081607342: 0.0\n",
      "EI: 0.005534369498491287: 0.0\n",
      "Outer epoch 289, Inner epoch 1, Loss: \tIE: -0.007104731630533934: 0.0\n",
      "EI: 0.005978885572403669: 0.0\n",
      "Outer epoch 290, Inner epoch 1, Loss: \tIE: -0.007071890402585268: 0.0\n",
      "EI: 0.00594023521989584: 0.0\n",
      "Outer epoch 291, Inner epoch 1, Loss: \tIE: -0.007319923024624586: 0.0\n",
      "EI: 0.005815478507429361: 0.0\n",
      "Outer epoch 292, Inner epoch 1, Loss: \tIE: -0.006960132159292698: 0.0\n",
      "EI: 0.006122893653810024: 0.0\n",
      "Outer epoch 293, Inner epoch 1, Loss: \tIE: -0.0071307215839624405: 0.0\n",
      "EI: 0.006072645075619221: 0.0\n",
      "Outer epoch 294, Inner epoch 1, Loss: \tIE: -0.007172165438532829: 0.0\n",
      "EI: 0.005801385268568993: 0.0\n",
      "Outer epoch 295, Inner epoch 1, Loss: \tIE: -0.00702210608869791: 0.0\n",
      "EI: 0.006078833714127541: 0.0\n",
      "Outer epoch 296, Inner epoch 1, Loss: \tIE: -0.007210331968963146: 0.0\n",
      "EI: 0.005450658965855837: 0.0\n",
      "Outer epoch 297, Inner epoch 1, Loss: \tIE: -0.007124856114387512: 0.0\n",
      "EI: 0.005423446651548147: 0.0\n",
      "Outer epoch 298, Inner epoch 1, Loss: \tIE: -0.007188092917203903: 0.0\n",
      "EI: 0.0055763074196875095: 0.0\n",
      "Outer epoch 299, Inner epoch 1, Loss: \tIE: -0.00753047177568078: 0.0\n",
      "EI: 0.005017694551497698: 0.0\n",
      "Outer epoch 300, Inner epoch 1, Loss: \tIE: -0.0072382730431854725: 0.0\n",
      "EI: 0.005334974266588688: 0.0\n",
      "Outer epoch 301, Inner epoch 1, Loss: \tIE: -0.006939259823411703: 0.0\n",
      "EI: 0.005578968208283186: 0.0\n",
      "Outer epoch 302, Inner epoch 1, Loss: \tIE: -0.007268127985298634: 0.0\n",
      "EI: 0.005313487723469734: 0.0\n",
      "Outer epoch 303, Inner epoch 1, Loss: \tIE: -0.0066832732409238815: 0.0\n",
      "EI: 0.005500486586242914: 0.0\n",
      "Outer epoch 304, Inner epoch 1, Loss: \tIE: -0.0076163397170603275: 0.0\n",
      "EI: 0.004859462380409241: 0.0\n",
      "Outer epoch 305, Inner epoch 1, Loss: \tIE: -0.007500178180634975: 0.0\n",
      "EI: 0.004703732207417488: 0.0\n",
      "Outer epoch 306, Inner epoch 1, Loss: \tIE: -0.007184823974967003: 0.0\n",
      "EI: 0.00495810154825449: 0.0\n",
      "Outer epoch 307, Inner epoch 1, Loss: \tIE: -0.007490085903555155: 0.0\n",
      "EI: 0.004657117649912834: 0.0\n",
      "Outer epoch 308, Inner epoch 1, Loss: \tIE: -0.00729718990623951: 0.0\n",
      "EI: 0.004428529180586338: 0.0\n",
      "Outer epoch 309, Inner epoch 1, Loss: \tIE: -0.006782958284020424: 0.0\n",
      "EI: 0.004966923035681248: 0.0\n",
      "Outer epoch 310, Inner epoch 1, Loss: \tIE: -0.007175165228545666: 0.0\n",
      "EI: 0.004341544583439827: 0.0\n",
      "Outer epoch 311, Inner epoch 1, Loss: \tIE: -0.007702038157731295: 0.0\n",
      "EI: 0.0037528732791543007: 0.0\n",
      "Outer epoch 312, Inner epoch 1, Loss: \tIE: -0.007235080935060978: 0.0\n",
      "EI: 0.004285247065126896: 0.0\n",
      "Outer epoch 313, Inner epoch 1, Loss: \tIE: -0.007981786504387856: 0.0\n",
      "EI: 0.0033217675518244505: 0.0\n",
      "Outer epoch 314, Inner epoch 1, Loss: \tIE: -0.007155994419008493: 0.0\n",
      "EI: 0.003994753118604422: 0.0\n",
      "Outer epoch 315, Inner epoch 1, Loss: \tIE: -0.007271616719663143: 0.0\n",
      "EI: 0.004032103810459375: 0.0\n",
      "Outer epoch 316, Inner epoch 1, Loss: \tIE: -0.0068886312656104565: 0.0\n",
      "EI: 0.004180207382887602: 0.0\n",
      "Outer epoch 317, Inner epoch 1, Loss: \tIE: -0.007185457274317741: 0.0\n",
      "EI: 0.003872533096000552: 0.0\n",
      "Outer epoch 318, Inner epoch 1, Loss: \tIE: -0.007081690710037947: 0.0\n",
      "EI: 0.0040445877239108086: 0.0\n",
      "Outer epoch 319, Inner epoch 1, Loss: \tIE: -0.007184471003711224: 0.0\n",
      "EI: 0.00371122476644814: 0.0\n",
      "Outer epoch 320, Inner epoch 1, Loss: \tIE: -0.007085005287081003: 0.0\n",
      "EI: 0.0033638696186244488: 0.0\n",
      "Outer epoch 321, Inner epoch 1, Loss: \tIE: -0.0069648586213588715: 0.0\n",
      "EI: 0.003888442413881421: 0.0\n",
      "Outer epoch 322, Inner epoch 1, Loss: \tIE: -0.007383931893855333: 0.0\n",
      "EI: 0.0031289795879274607: 0.0\n",
      "Outer epoch 323, Inner epoch 1, Loss: \tIE: -0.007445459254086018: 0.0\n",
      "EI: 0.003307427978143096: 0.0\n",
      "Outer epoch 324, Inner epoch 1, Loss: \tIE: -0.007092153187841177: 0.0\n",
      "EI: 0.003537370590493083: 0.0\n",
      "Outer epoch 325, Inner epoch 1, Loss: \tIE: -0.007008457090705633: 0.0\n",
      "EI: 0.0035094248596578836: 0.0\n",
      "Outer epoch 326, Inner epoch 1, Loss: \tIE: -0.007216228637844324: 0.0\n",
      "EI: 0.0030235592275857925: 0.0\n",
      "Outer epoch 327, Inner epoch 1, Loss: \tIE: -0.007413691375404596: 0.0\n",
      "EI: 0.0026764797512441874: 0.0\n",
      "Outer epoch 328, Inner epoch 1, Loss: \tIE: -0.006613168399780989: 0.0\n",
      "EI: 0.0034605904947966337: 0.0\n",
      "Outer epoch 329, Inner epoch 1, Loss: \tIE: -0.006973563227802515: 0.0\n",
      "EI: 0.002827598247677088: 0.0\n",
      "Outer epoch 330, Inner epoch 1, Loss: \tIE: -0.0070678358897566795: 0.0\n",
      "EI: 0.0029258837457746267: 0.0\n",
      "Outer epoch 331, Inner epoch 1, Loss: \tIE: -0.007231093477457762: 0.0\n",
      "EI: 0.003079841611906886: 0.0\n",
      "Outer epoch 332, Inner epoch 1, Loss: \tIE: -0.007157410494983196: 0.0\n",
      "EI: 0.00279094185680151: 0.0\n",
      "Outer epoch 333, Inner epoch 1, Loss: \tIE: -0.0071066152304410934: 0.0\n",
      "EI: 0.002781354356557131: 0.0\n",
      "Outer epoch 334, Inner epoch 1, Loss: \tIE: -0.006935897283256054: 0.0\n",
      "EI: 0.0028684474527835846: 0.0\n",
      "Outer epoch 335, Inner epoch 1, Loss: \tIE: -0.0075248973444104195: 0.0\n",
      "EI: 0.0020692578982561827: 0.0\n",
      "Outer epoch 336, Inner epoch 1, Loss: \tIE: -0.007513438817113638: 0.0\n",
      "EI: 0.0022629357408732176: 0.0\n",
      "Outer epoch 337, Inner epoch 1, Loss: \tIE: -0.007431546691805124: 0.0\n",
      "EI: 0.002010071650147438: 0.0\n",
      "Outer epoch 338, Inner epoch 1, Loss: \tIE: -0.0073845344595611095: 0.0\n",
      "EI: 0.002078843303024769: 0.0\n",
      "Outer epoch 339, Inner epoch 1, Loss: \tIE: -0.007584953680634499: 0.0\n",
      "EI: 0.00197618268430233: 0.0\n",
      "Outer epoch 340, Inner epoch 1, Loss: \tIE: -0.007215810474008322: 0.0\n",
      "EI: 0.002338120946660638: 0.0\n",
      "Outer epoch 341, Inner epoch 1, Loss: \tIE: -0.007876034826040268: 0.0\n",
      "EI: 0.0016845841892063618: 0.0\n",
      "Outer epoch 342, Inner epoch 1, Loss: \tIE: -0.007090788334608078: 0.0\n",
      "EI: 0.0023448094725608826: 0.0\n",
      "Outer epoch 343, Inner epoch 1, Loss: \tIE: -0.007374262902885675: 0.0\n",
      "EI: 0.0017809950513765216: 0.0\n",
      "Outer epoch 344, Inner epoch 1, Loss: \tIE: -0.006909166928380728: 0.0\n",
      "EI: 0.0021891603246331215: 0.0\n",
      "Outer epoch 345, Inner epoch 1, Loss: \tIE: -0.006973161827772856: 0.0\n",
      "EI: 0.002115346724167466: 0.0\n",
      "Outer epoch 346, Inner epoch 1, Loss: \tIE: -0.006857517175376415: 0.0\n",
      "EI: 0.0020847178529947996: 0.0\n",
      "Outer epoch 347, Inner epoch 1, Loss: \tIE: -0.007255120202898979: 0.0\n",
      "EI: 0.0013958873460069299: 0.0\n",
      "Outer epoch 348, Inner epoch 1, Loss: \tIE: -0.007643017452210188: 0.0\n",
      "EI: 0.0011105915764346719: 0.0\n",
      "Outer epoch 349, Inner epoch 1, Loss: \tIE: -0.007492683827877045: 0.0\n",
      "EI: 0.001415280275978148: 0.0\n",
      "Outer epoch 350, Inner epoch 1, Loss: \tIE: -0.006837609224021435: 0.0\n",
      "EI: 0.001446380978450179: 0.0\n",
      "Outer epoch 351, Inner epoch 1, Loss: \tIE: -0.007307749707251787: 0.0\n",
      "EI: 0.0014250536914914846: 0.0\n",
      "Outer epoch 352, Inner epoch 1, Loss: \tIE: -0.0074180010706186295: 0.0\n",
      "EI: 0.0012257860507816076: 0.0\n",
      "Outer epoch 353, Inner epoch 1, Loss: \tIE: -0.007347133010625839: 0.0\n",
      "EI: 0.000872634700499475: 0.0\n",
      "Outer epoch 354, Inner epoch 1, Loss: \tIE: -0.007256753742694855: 0.0\n",
      "EI: 0.0013393608387559652: 0.0\n",
      "Outer epoch 355, Inner epoch 1, Loss: \tIE: -0.0076646809466183186: 0.0\n",
      "EI: 0.0006357434904202819: 0.0\n",
      "Outer epoch 356, Inner epoch 1, Loss: \tIE: -0.006820929702371359: 0.0\n",
      "EI: 0.0014945719158276916: 0.0\n",
      "Outer epoch 357, Inner epoch 1, Loss: \tIE: -0.007170156575739384: 0.0\n",
      "EI: 0.0006411001086235046: 0.0\n",
      "Outer epoch 358, Inner epoch 1, Loss: \tIE: -0.007095861714333296: 0.0\n",
      "EI: 0.001195030752569437: 0.0\n",
      "Outer epoch 359, Inner epoch 1, Loss: \tIE: -0.007532800547778606: 0.0\n",
      "EI: 0.00025443537742830813: 0.0\n",
      "Outer epoch 360, Inner epoch 1, Loss: \tIE: -0.006994107738137245: 0.0\n",
      "EI: 0.0010181781835854053: 0.0\n",
      "Outer epoch 361, Inner epoch 1, Loss: \tIE: -0.007354431785643101: 0.0\n",
      "EI: 0.0005659784656018019: 0.0\n",
      "Outer epoch 362, Inner epoch 1, Loss: \tIE: -0.006940844468772411: 0.0\n",
      "EI: 0.0005317407194525003: 0.0\n",
      "Outer epoch 363, Inner epoch 1, Loss: \tIE: -0.007516858167946339: 0.0\n",
      "EI: 0.00017689034575596452: 0.0\n",
      "Outer epoch 364, Inner epoch 1, Loss: \tIE: -0.0073609733954072: 0.0\n",
      "EI: -4.445705314992665e-07: 0.0\n",
      "Outer epoch 365, Inner epoch 1, Loss: \tIE: -0.007059032097458839: 0.0\n",
      "EI: 0.00038517441134899855: 0.0\n",
      "Outer epoch 366, Inner epoch 1, Loss: \tIE: -0.007222104351967573: 0.0\n",
      "EI: 0.00010154515621252358: 0.0\n",
      "Outer epoch 367, Inner epoch 1, Loss: \tIE: -0.0073623694479465485: 0.0\n",
      "EI: -8.641638123663142e-05: 0.0\n",
      "Outer epoch 368, Inner epoch 1, Loss: \tIE: -0.007183386012911797: 0.0\n",
      "EI: -0.00012517525465227664: 0.0\n",
      "Outer epoch 369, Inner epoch 1, Loss: \tIE: -0.007271993905305862: 0.0\n",
      "EI: 0.000204532450879924: 0.0\n",
      "Outer epoch 370, Inner epoch 1, Loss: \tIE: -0.0067002372816205025: 0.0\n",
      "EI: 0.00023693741241004318: 0.0\n",
      "Outer epoch 371, Inner epoch 1, Loss: \tIE: -0.00687952758744359: 0.0\n",
      "EI: 0.0003218352503608912: 0.0\n",
      "Outer epoch 372, Inner epoch 1, Loss: \tIE: -0.006905709858983755: 0.0\n",
      "EI: 0.00022680574329569936: 0.0\n",
      "Outer epoch 373, Inner epoch 1, Loss: \tIE: -0.007012803107500076: 0.0\n",
      "EI: -0.00027642681379802525: 0.0\n",
      "Outer epoch 374, Inner epoch 1, Loss: \tIE: -0.006943749263882637: 0.0\n",
      "EI: -8.037748193601146e-05: 0.0\n",
      "Outer epoch 375, Inner epoch 1, Loss: \tIE: -0.0071745081804692745: 0.0\n",
      "EI: -0.0005759161431342363: 0.0\n",
      "Outer epoch 376, Inner epoch 1, Loss: \tIE: -0.007344373501837254: 0.0\n",
      "EI: -0.00046173587907105684: 0.0\n",
      "Outer epoch 377, Inner epoch 1, Loss: \tIE: -0.006661327555775642: 0.0\n",
      "EI: -0.00018855718371924013: 0.0\n",
      "Outer epoch 378, Inner epoch 1, Loss: \tIE: -0.0070672351866960526: 0.0\n",
      "EI: -0.00040176277980208397: 0.0\n",
      "Outer epoch 379, Inner epoch 1, Loss: \tIE: -0.0070693339221179485: 0.0\n",
      "EI: -0.0005186097696423531: 0.0\n",
      "Outer epoch 380, Inner epoch 1, Loss: \tIE: -0.007354178000241518: 0.0\n",
      "EI: -0.0009819924598559737: 0.0\n",
      "Outer epoch 381, Inner epoch 1, Loss: \tIE: -0.007454290520399809: 0.0\n",
      "EI: -0.0011757271131500602: 0.0\n",
      "Outer epoch 382, Inner epoch 1, Loss: \tIE: -0.007563111372292042: 0.0\n",
      "EI: -0.001239785342477262: 0.0\n",
      "Outer epoch 383, Inner epoch 1, Loss: \tIE: -0.0067541226744651794: 0.0\n",
      "EI: -0.0006398106343112886: 0.0\n",
      "Outer epoch 384, Inner epoch 1, Loss: \tIE: -0.006759942043572664: 0.0\n",
      "EI: -0.0006880860310047865: 0.0\n",
      "Outer epoch 385, Inner epoch 1, Loss: \tIE: -0.006795953027904034: 0.0\n",
      "EI: -0.0007677667308598757: 0.0\n",
      "Outer epoch 386, Inner epoch 1, Loss: \tIE: -0.007082972675561905: 0.0\n",
      "EI: -0.0009635952301323414: 0.0\n",
      "Outer epoch 387, Inner epoch 1, Loss: \tIE: -0.006772208958864212: 0.0\n",
      "EI: -0.0011952603235840797: 0.0\n",
      "Outer epoch 388, Inner epoch 1, Loss: \tIE: -0.006735608913004398: 0.0\n",
      "EI: -0.0009980190079659224: 0.0\n",
      "Outer epoch 389, Inner epoch 1, Loss: \tIE: -0.007348291110247374: 0.0\n",
      "EI: -0.001622436917386949: 0.0\n",
      "Outer epoch 390, Inner epoch 1, Loss: \tIE: -0.0070323278196156025: 0.0\n",
      "EI: -0.0015572125557810068: 0.0\n",
      "Outer epoch 391, Inner epoch 1, Loss: \tIE: -0.007313832640647888: 0.0\n",
      "EI: -0.001804105006158352: 0.0\n",
      "Outer epoch 392, Inner epoch 1, Loss: \tIE: -0.007367568556219339: 0.0\n",
      "EI: -0.0019294160883873701: 0.0\n",
      "Outer epoch 393, Inner epoch 1, Loss: \tIE: -0.007163180969655514: 0.0\n",
      "EI: -0.0020182353910058737: 0.0\n",
      "Outer epoch 394, Inner epoch 1, Loss: \tIE: -0.007035791873931885: 0.0\n",
      "EI: -0.001683682668954134: 0.0\n",
      "Outer epoch 395, Inner epoch 1, Loss: \tIE: -0.006595991086214781: 0.0\n",
      "EI: -0.0012176393065601587: 0.0\n",
      "Outer epoch 396, Inner epoch 1, Loss: \tIE: -0.006834696512669325: 0.0\n",
      "EI: -0.0018691394943743944: 0.0\n",
      "Outer epoch 397, Inner epoch 1, Loss: \tIE: -0.006930137984454632: 0.0\n",
      "EI: -0.0020479699596762657: 0.0\n",
      "Outer epoch 398, Inner epoch 1, Loss: \tIE: -0.006722244434058666: 0.0\n",
      "EI: -0.0016739192651584744: 0.0\n",
      "Outer epoch 399, Inner epoch 1, Loss: \tIE: -0.006845215335488319: 0.0\n",
      "EI: -0.0020094572100788355: 0.0\n",
      "Outer epoch 400, Inner epoch 1, Loss: \tIE: -0.006972532719373703: 0.0\n",
      "EI: -0.0018167324597015977: 0.0\n",
      "Outer epoch 401, Inner epoch 1, Loss: \tIE: -0.006915731821209192: 0.0\n",
      "EI: -0.0020191725343465805: 0.0\n",
      "Outer epoch 402, Inner epoch 1, Loss: \tIE: -0.007174088153988123: 0.0\n",
      "EI: -0.0026413670275360346: 0.0\n",
      "Outer epoch 403, Inner epoch 1, Loss: \tIE: -0.00706735672429204: 0.0\n",
      "EI: -0.0027552503161132336: 0.0\n",
      "Outer epoch 404, Inner epoch 1, Loss: \tIE: -0.006824251264333725: 0.0\n",
      "EI: -0.0021488219499588013: 0.0\n",
      "Outer epoch 405, Inner epoch 1, Loss: \tIE: -0.0070817735977470875: 0.0\n",
      "EI: -0.002515364671126008: 0.0\n",
      "Outer epoch 406, Inner epoch 1, Loss: \tIE: -0.006705574691295624: 0.0\n",
      "EI: -0.002566578332334757: 0.0\n",
      "Outer epoch 407, Inner epoch 1, Loss: \tIE: -0.007468398194760084: 0.0\n",
      "EI: -0.0032246001064777374: 0.0\n",
      "Outer epoch 408, Inner epoch 1, Loss: \tIE: -0.007305898237973452: 0.0\n",
      "EI: -0.002883758395910263: 0.0\n",
      "Outer epoch 409, Inner epoch 1, Loss: \tIE: -0.007004572078585625: 0.0\n",
      "EI: -0.0028093685396015644: 0.0\n",
      "Outer epoch 410, Inner epoch 1, Loss: \tIE: -0.007375206332653761: 0.0\n",
      "EI: -0.0039481790736317635: 0.0\n",
      "Outer epoch 411, Inner epoch 1, Loss: \tIE: -0.007056503556668758: 0.0\n",
      "EI: -0.0030658061150461435: 0.0\n",
      "Outer epoch 412, Inner epoch 1, Loss: \tIE: -0.007572801783680916: 0.0\n",
      "EI: -0.004203185439109802: 0.0\n",
      "Outer epoch 413, Inner epoch 1, Loss: \tIE: -0.007218716200441122: 0.0\n",
      "EI: -0.00348718767054379: 0.0\n",
      "Outer epoch 414, Inner epoch 1, Loss: \tIE: -0.006862214766442776: 0.0\n",
      "EI: -0.0031417175196111202: 0.0\n",
      "Outer epoch 415, Inner epoch 1, Loss: \tIE: -0.006766850128769875: 0.0\n",
      "EI: -0.0028985056560486555: 0.0\n",
      "Outer epoch 416, Inner epoch 1, Loss: \tIE: -0.007144632749259472: 0.0\n",
      "EI: -0.0033231566194444895: 0.0\n",
      "Outer epoch 417, Inner epoch 1, Loss: \tIE: -0.00689952215179801: 0.0\n",
      "EI: -0.0030840812250971794: 0.0\n",
      "Outer epoch 418, Inner epoch 1, Loss: \tIE: -0.007167434319853783: 0.0\n",
      "EI: -0.0035284115001559258: 0.0\n",
      "Outer epoch 419, Inner epoch 1, Loss: \tIE: -0.006798698566854: 0.0\n",
      "EI: -0.0034265529830008745: 0.0\n",
      "Outer epoch 420, Inner epoch 1, Loss: \tIE: -0.006914123892784119: 0.0\n",
      "EI: -0.0035628764890134335: 0.0\n",
      "Outer epoch 421, Inner epoch 1, Loss: \tIE: -0.006611009128391743: 0.0\n",
      "EI: -0.003146452596411109: 0.0\n",
      "Outer epoch 422, Inner epoch 1, Loss: \tIE: -0.007736305706202984: 0.0\n",
      "EI: -0.004646212328225374: 0.0\n",
      "Outer epoch 423, Inner epoch 1, Loss: \tIE: -0.00669849431142211: 0.0\n",
      "EI: -0.00328326178714633: 0.0\n",
      "Outer epoch 424, Inner epoch 1, Loss: \tIE: -0.007374963257461786: 0.0\n",
      "EI: -0.004384559579193592: 0.0\n",
      "Outer epoch 425, Inner epoch 1, Loss: \tIE: -0.0069496408104896545: 0.0\n",
      "EI: -0.003566869767382741: 0.0\n",
      "Outer epoch 426, Inner epoch 1, Loss: \tIE: -0.00728283217176795: 0.0\n",
      "EI: -0.004102608188986778: 0.0\n",
      "Outer epoch 427, Inner epoch 1, Loss: \tIE: -0.007178634870797396: 0.0\n",
      "EI: -0.004427407402545214: 0.0\n",
      "Outer epoch 428, Inner epoch 1, Loss: \tIE: -0.006635375786572695: 0.0\n",
      "EI: -0.0037444387562572956: 0.0\n",
      "Outer epoch 429, Inner epoch 1, Loss: \tIE: -0.006747978273779154: 0.0\n",
      "EI: -0.004043141379952431: 0.0\n",
      "Outer epoch 430, Inner epoch 1, Loss: \tIE: -0.006956422235816717: 0.0\n",
      "EI: -0.004371301271021366: 0.0\n",
      "Outer epoch 431, Inner epoch 1, Loss: \tIE: -0.006963331717997789: 0.0\n",
      "EI: -0.00419279420748353: 0.0\n",
      "Outer epoch 432, Inner epoch 1, Loss: \tIE: -0.006867874879390001: 0.0\n",
      "EI: -0.004413879942148924: 0.0\n",
      "Outer epoch 433, Inner epoch 1, Loss: \tIE: -0.006817028857767582: 0.0\n",
      "EI: -0.004325558431446552: 0.0\n",
      "Outer epoch 434, Inner epoch 1, Loss: \tIE: -0.007031728513538837: 0.0\n",
      "EI: -0.004693455994129181: 0.0\n",
      "Outer epoch 435, Inner epoch 1, Loss: \tIE: -0.007422047667205334: 0.0\n",
      "EI: -0.005556078162044287: 0.0\n",
      "Outer epoch 436, Inner epoch 1, Loss: \tIE: -0.00698257889598608: 0.0\n",
      "EI: -0.005006001330912113: 0.0\n",
      "Outer epoch 437, Inner epoch 1, Loss: \tIE: -0.006941142026335001: 0.0\n",
      "EI: -0.0049027265049517155: 0.0\n",
      "Outer epoch 438, Inner epoch 1, Loss: \tIE: -0.00715105514973402: 0.0\n",
      "EI: -0.005020132753998041: 0.0\n",
      "Outer epoch 439, Inner epoch 1, Loss: \tIE: -0.006992079317569733: 0.0\n",
      "EI: -0.00475650979205966: 0.0\n",
      "Outer epoch 440, Inner epoch 1, Loss: \tIE: -0.00658825458958745: 0.0\n",
      "EI: -0.004851358477026224: 0.0\n",
      "Outer epoch 441, Inner epoch 1, Loss: \tIE: -0.00678594782948494: 0.0\n",
      "EI: -0.0049090818502008915: 0.0\n",
      "Outer epoch 442, Inner epoch 1, Loss: \tIE: -0.006778563838452101: 0.0\n",
      "EI: -0.0048878854140639305: 0.0\n",
      "Outer epoch 443, Inner epoch 1, Loss: \tIE: -0.007161250337958336: 0.0\n",
      "EI: -0.005711601115763187: 0.0\n",
      "Outer epoch 444, Inner epoch 1, Loss: \tIE: -0.007104045245796442: 0.0\n",
      "EI: -0.005365681368857622: 0.0\n",
      "Outer epoch 445, Inner epoch 1, Loss: \tIE: -0.0072475909255445: 0.0\n",
      "EI: -0.0056495703756809235: 0.0\n",
      "Outer epoch 446, Inner epoch 1, Loss: \tIE: -0.0069236657582223415: 0.0\n",
      "EI: -0.005418794229626656: 0.0\n",
      "Outer epoch 447, Inner epoch 1, Loss: \tIE: -0.007648773491382599: 0.0\n",
      "EI: -0.0064739347435534: 0.0\n",
      "Outer epoch 448, Inner epoch 1, Loss: \tIE: -0.007304960861802101: 0.0\n",
      "EI: -0.005923726130276918: 0.0\n",
      "Outer epoch 449, Inner epoch 1, Loss: \tIE: -0.0073892297223210335: 0.0\n",
      "EI: -0.00601810310035944: 0.0\n",
      "Outer epoch 450, Inner epoch 1, Loss: \tIE: -0.00707594258710742: 0.0\n",
      "EI: -0.005970584228634834: 0.0\n",
      "Outer epoch 451, Inner epoch 1, Loss: \tIE: -0.006733074318617582: 0.0\n",
      "EI: -0.0054048094898462296: 0.0\n",
      "Outer epoch 452, Inner epoch 1, Loss: \tIE: -0.007109294179826975: 0.0\n",
      "EI: -0.006477628834545612: 0.0\n",
      "Outer epoch 453, Inner epoch 1, Loss: \tIE: -0.006922979839146137: 0.0\n",
      "EI: -0.006090260576456785: 0.0\n",
      "Outer epoch 454, Inner epoch 1, Loss: \tIE: -0.007037964649498463: 0.0\n",
      "EI: -0.0061036101542413235: 0.0\n",
      "Outer epoch 455, Inner epoch 1, Loss: \tIE: -0.007134591694921255: 0.0\n",
      "EI: -0.006050219759345055: 0.0\n",
      "Outer epoch 456, Inner epoch 1, Loss: \tIE: -0.006631017662584782: 0.0\n",
      "EI: -0.0058831823989748955: 0.0\n",
      "Outer epoch 457, Inner epoch 1, Loss: \tIE: -0.0071829394437372684: 0.0\n",
      "EI: -0.006412970367819071: 0.0\n",
      "Outer epoch 458, Inner epoch 1, Loss: \tIE: -0.00697053549811244: 0.0\n",
      "EI: -0.006142837461084127: 0.0\n",
      "Outer epoch 459, Inner epoch 1, Loss: \tIE: -0.007316442672163248: 0.0\n",
      "EI: -0.006692269816994667: 0.0\n",
      "Outer epoch 460, Inner epoch 1, Loss: \tIE: -0.006812022998929024: 0.0\n",
      "EI: -0.006278357468545437: 0.0\n",
      "Outer epoch 461, Inner epoch 1, Loss: \tIE: -0.006932404357939959: 0.0\n",
      "EI: -0.006653258576989174: 0.0\n",
      "Outer epoch 462, Inner epoch 1, Loss: \tIE: -0.007145003415644169: 0.0\n",
      "EI: -0.006732659880071878: 0.0\n",
      "Outer epoch 463, Inner epoch 1, Loss: \tIE: -0.007230391260236502: 0.0\n",
      "EI: -0.007278532721102238: 0.0\n",
      "Outer epoch 464, Inner epoch 1, Loss: \tIE: -0.006804730277508497: 0.0\n",
      "EI: -0.006594147067517042: 0.0\n",
      "Outer epoch 465, Inner epoch 1, Loss: \tIE: -0.007025325670838356: 0.0\n",
      "EI: -0.006847192533314228: 0.0\n",
      "Outer epoch 466, Inner epoch 1, Loss: \tIE: -0.007085716351866722: 0.0\n",
      "EI: -0.0069793579168617725: 0.0\n",
      "Outer epoch 467, Inner epoch 1, Loss: \tIE: -0.007133745588362217: 0.0\n",
      "EI: -0.0069277407601475716: 0.0\n",
      "Outer epoch 468, Inner epoch 1, Loss: \tIE: -0.006781787145882845: 0.0\n",
      "EI: -0.00691286101937294: 0.0\n",
      "Outer epoch 469, Inner epoch 1, Loss: \tIE: -0.007259397767484188: 0.0\n",
      "EI: -0.007728966884315014: 0.0\n",
      "Outer epoch 470, Inner epoch 1, Loss: \tIE: -0.00638827309012413: 0.0\n",
      "EI: -0.0065659647807478905: 0.0\n",
      "Outer epoch 471, Inner epoch 1, Loss: \tIE: -0.007481425069272518: 0.0\n",
      "EI: -0.007862634025514126: 0.0\n",
      "Outer epoch 472, Inner epoch 1, Loss: \tIE: -0.006829136051237583: 0.0\n",
      "EI: -0.007000273559242487: 0.0\n",
      "Outer epoch 473, Inner epoch 1, Loss: \tIE: -0.00671157194301486: 0.0\n",
      "EI: -0.0071950554847717285: 0.0\n",
      "Outer epoch 474, Inner epoch 1, Loss: \tIE: -0.006373308133333921: 0.0\n",
      "EI: -0.006758864037692547: 0.0\n",
      "Outer epoch 475, Inner epoch 1, Loss: \tIE: -0.007073582150042057: 0.0\n",
      "EI: -0.007444987539201975: 0.0\n",
      "Outer epoch 476, Inner epoch 1, Loss: \tIE: -0.007348568178713322: 0.0\n",
      "EI: -0.00829403381794691: 0.0\n",
      "Outer epoch 477, Inner epoch 1, Loss: \tIE: -0.0069074309431016445: 0.0\n",
      "EI: -0.007693727966398001: 0.0\n",
      "Outer epoch 478, Inner epoch 1, Loss: \tIE: -0.006661717779934406: 0.0\n",
      "EI: -0.0071657742373645306: 0.0\n",
      "Outer epoch 479, Inner epoch 1, Loss: \tIE: -0.006852888036519289: 0.0\n",
      "EI: -0.00767528684809804: 0.0\n",
      "Outer epoch 480, Inner epoch 1, Loss: \tIE: -0.006811678875237703: 0.0\n",
      "EI: -0.0076595027931034565: 0.0\n",
      "Outer epoch 481, Inner epoch 1, Loss: \tIE: -0.007151350378990173: 0.0\n",
      "EI: -0.008319762535393238: 0.0\n",
      "Outer epoch 482, Inner epoch 1, Loss: \tIE: -0.007051498629152775: 0.0\n",
      "EI: -0.00835789367556572: 0.0\n",
      "Outer epoch 483, Inner epoch 1, Loss: \tIE: -0.007087095640599728: 0.0\n",
      "EI: -0.008341874927282333: 0.0\n",
      "Outer epoch 484, Inner epoch 1, Loss: \tIE: -0.00727424118667841: 0.0\n",
      "EI: -0.00852974969893694: 0.0\n",
      "Outer epoch 485, Inner epoch 1, Loss: \tIE: -0.007021408528089523: 0.0\n",
      "EI: -0.008753187954425812: 0.0\n",
      "Outer epoch 486, Inner epoch 1, Loss: \tIE: -0.007257006596773863: 0.0\n",
      "EI: -0.008733982220292091: 0.0\n",
      "Outer epoch 487, Inner epoch 1, Loss: \tIE: -0.00742440577596426: 0.0\n",
      "EI: -0.009130516089498997: 0.0\n",
      "Outer epoch 488, Inner epoch 1, Loss: \tIE: -0.006626293063163757: 0.0\n",
      "EI: -0.007927359081804752: 0.0\n",
      "Outer epoch 489, Inner epoch 1, Loss: \tIE: -0.006921452935785055: 0.0\n",
      "EI: -0.008517658337950706: 0.0\n",
      "Outer epoch 490, Inner epoch 1, Loss: \tIE: -0.007101836148649454: 0.0\n",
      "EI: -0.009298617020249367: 0.0\n",
      "Outer epoch 491, Inner epoch 1, Loss: \tIE: -0.006687854882329702: 0.0\n",
      "EI: -0.008677328936755657: 0.0\n",
      "Outer epoch 492, Inner epoch 1, Loss: \tIE: -0.0064439247362315655: 0.0\n",
      "EI: -0.008484024554491043: 0.0\n",
      "Outer epoch 493, Inner epoch 1, Loss: \tIE: -0.006761936936527491: 0.0\n",
      "EI: -0.008491541258990765: 0.0\n",
      "Outer epoch 494, Inner epoch 1, Loss: \tIE: -0.006754888221621513: 0.0\n",
      "EI: -0.009101404808461666: 0.0\n",
      "Outer epoch 495, Inner epoch 1, Loss: \tIE: -0.006815034430474043: 0.0\n",
      "EI: -0.008916572667658329: 0.0\n",
      "Outer epoch 496, Inner epoch 1, Loss: \tIE: -0.007011062931269407: 0.0\n",
      "EI: -0.009000839665532112: 0.0\n",
      "Outer epoch 497, Inner epoch 1, Loss: \tIE: -0.007086032070219517: 0.0\n",
      "EI: -0.009031428024172783: 0.0\n",
      "Outer epoch 498, Inner epoch 1, Loss: \tIE: -0.00675459997728467: 0.0\n",
      "EI: -0.008729243651032448: 0.0\n",
      "Outer epoch 499, Inner epoch 1, Loss: \tIE: -0.006934829987585545: 0.0\n",
      "EI: -0.00935454573482275: 0.0\n",
      "Outer epoch 500, Inner epoch 1, Loss: \tIE: -0.007314503192901611: 0.0\n",
      "EI: -0.00962857250124216: 0.0\n",
      "Outer epoch 501, Inner epoch 1, Loss: \tIE: -0.007290344685316086: 0.0\n",
      "EI: -0.010402725078165531: 0.0\n",
      "Outer epoch 502, Inner epoch 1, Loss: \tIE: -0.006520952098071575: 0.0\n",
      "EI: -0.009415076114237309: 0.0\n",
      "Outer epoch 503, Inner epoch 1, Loss: \tIE: -0.007530729752033949: 0.0\n",
      "EI: -0.010882771573960781: 0.0\n",
      "Outer epoch 504, Inner epoch 1, Loss: \tIE: -0.007108650635927916: 0.0\n",
      "EI: -0.01008536759763956: 0.0\n",
      "Outer epoch 505, Inner epoch 1, Loss: \tIE: -0.006686014123260975: 0.0\n",
      "EI: -0.009704725816845894: 0.0\n",
      "Outer epoch 506, Inner epoch 1, Loss: \tIE: -0.007181091699749231: 0.0\n",
      "EI: -0.010083500295877457: 0.0\n",
      "Outer epoch 507, Inner epoch 1, Loss: \tIE: -0.006968345958739519: 0.0\n",
      "EI: -0.009701122529804707: 0.0\n",
      "Outer epoch 508, Inner epoch 1, Loss: \tIE: -0.006969802547246218: 0.0\n",
      "EI: -0.010119182989001274: 0.0\n",
      "Outer epoch 509, Inner epoch 1, Loss: \tIE: -0.006622896064072847: 0.0\n",
      "EI: -0.009551082737743855: 0.0\n",
      "Outer epoch 510, Inner epoch 1, Loss: \tIE: -0.006887283641844988: 0.0\n",
      "EI: -0.01024630293250084: 0.0\n",
      "Outer epoch 511, Inner epoch 1, Loss: \tIE: -0.00687194149941206: 0.0\n",
      "EI: -0.009989004582166672: 0.0\n",
      "Outer epoch 512, Inner epoch 1, Loss: \tIE: -0.0069831074215471745: 0.0\n",
      "EI: -0.010400449857115746: 0.0\n",
      "Outer epoch 513, Inner epoch 1, Loss: \tIE: -0.007034244015812874: 0.0\n",
      "EI: -0.010794238187372684: 0.0\n",
      "Outer epoch 514, Inner epoch 1, Loss: \tIE: -0.006875569466501474: 0.0\n",
      "EI: -0.010470298118889332: 0.0\n",
      "Outer epoch 515, Inner epoch 1, Loss: \tIE: -0.00659274123609066: 0.0\n",
      "EI: -0.010082395747303963: 0.0\n",
      "Outer epoch 516, Inner epoch 1, Loss: \tIE: -0.006958877667784691: 0.0\n",
      "EI: -0.010425770655274391: 0.0\n",
      "Outer epoch 517, Inner epoch 1, Loss: \tIE: -0.006963761523365974: 0.0\n",
      "EI: -0.011035890318453312: 0.0\n",
      "Outer epoch 518, Inner epoch 1, Loss: \tIE: -0.006790137849748135: 0.0\n",
      "EI: -0.01038502249866724: 0.0\n",
      "Outer epoch 519, Inner epoch 1, Loss: \tIE: -0.006836605723947287: 0.0\n",
      "EI: -0.010752911679446697: 0.0\n",
      "Outer epoch 520, Inner epoch 1, Loss: \tIE: -0.006727881263941526: 0.0\n",
      "EI: -0.010777556337416172: 0.0\n",
      "Outer epoch 521, Inner epoch 1, Loss: \tIE: -0.006756264250725508: 0.0\n",
      "EI: -0.010710802860558033: 0.0\n",
      "Outer epoch 522, Inner epoch 1, Loss: \tIE: -0.006877036765217781: 0.0\n",
      "EI: -0.010785304941236973: 0.0\n",
      "Outer epoch 523, Inner epoch 1, Loss: \tIE: -0.006668907590210438: 0.0\n",
      "EI: -0.011073991656303406: 0.0\n",
      "Outer epoch 524, Inner epoch 1, Loss: \tIE: -0.006837941240519285: 0.0\n",
      "EI: -0.010845815762877464: 0.0\n",
      "Outer epoch 525, Inner epoch 1, Loss: \tIE: -0.006862518843263388: 0.0\n",
      "EI: -0.011323349550366402: 0.0\n",
      "Outer epoch 526, Inner epoch 1, Loss: \tIE: -0.0067573064006865025: 0.0\n",
      "EI: -0.01111910492181778: 0.0\n",
      "Outer epoch 527, Inner epoch 1, Loss: \tIE: -0.006795146036893129: 0.0\n",
      "EI: -0.011044261045753956: 0.0\n",
      "Outer epoch 528, Inner epoch 1, Loss: \tIE: -0.006855472456663847: 0.0\n",
      "EI: -0.011390183120965958: 0.0\n",
      "Outer epoch 529, Inner epoch 1, Loss: \tIE: -0.0068975454196333885: 0.0\n",
      "EI: -0.01151877362281084: 0.0\n",
      "Outer epoch 530, Inner epoch 1, Loss: \tIE: -0.006941233761608601: 0.0\n",
      "EI: -0.011724244803190231: 0.0\n",
      "Outer epoch 531, Inner epoch 1, Loss: \tIE: -0.00684808986261487: 0.0\n",
      "EI: -0.011590550653636456: 0.0\n",
      "Outer epoch 532, Inner epoch 1, Loss: \tIE: -0.007071650121361017: 0.0\n",
      "EI: -0.012249981053173542: 0.0\n",
      "Outer epoch 533, Inner epoch 1, Loss: \tIE: -0.00711527606472373: 0.0\n",
      "EI: -0.01216045767068863: 0.0\n",
      "Outer epoch 534, Inner epoch 1, Loss: \tIE: -0.0069359983317554: 0.0\n",
      "EI: -0.012078746221959591: 0.0\n",
      "Outer epoch 535, Inner epoch 1, Loss: \tIE: -0.006498206872493029: 0.0\n",
      "EI: -0.011432389728724957: 0.0\n",
      "Outer epoch 536, Inner epoch 1, Loss: \tIE: -0.006739705801010132: 0.0\n",
      "EI: -0.012023205868899822: 0.0\n",
      "Outer epoch 537, Inner epoch 1, Loss: \tIE: -0.006667334120720625: 0.0\n",
      "EI: -0.011899163946509361: 0.0\n",
      "Outer epoch 538, Inner epoch 1, Loss: \tIE: -0.006743161473423243: 0.0\n",
      "EI: -0.011996697634458542: 0.0\n",
      "Outer epoch 539, Inner epoch 1, Loss: \tIE: -0.006843550596386194: 0.0\n",
      "EI: -0.012388880364596844: 0.0\n",
      "Outer epoch 540, Inner epoch 1, Loss: \tIE: -0.006945811677724123: 0.0\n",
      "EI: -0.012409806251525879: 0.0\n",
      "Outer epoch 541, Inner epoch 1, Loss: \tIE: -0.006833024322986603: 0.0\n",
      "EI: -0.012503215111792088: 0.0\n",
      "Outer epoch 542, Inner epoch 1, Loss: \tIE: -0.006885557435452938: 0.0\n",
      "EI: -0.012410887517035007: 0.0\n",
      "Outer epoch 543, Inner epoch 1, Loss: \tIE: -0.006784432101994753: 0.0\n",
      "EI: -0.012347295880317688: 0.0\n",
      "Outer epoch 544, Inner epoch 1, Loss: \tIE: -0.007163038477301598: 0.0\n",
      "EI: -0.013156559318304062: 0.0\n",
      "Outer epoch 545, Inner epoch 1, Loss: \tIE: -0.00660618906840682: 0.0\n",
      "EI: -0.012478522956371307: 0.0\n",
      "Outer epoch 546, Inner epoch 1, Loss: \tIE: -0.007107640616595745: 0.0\n",
      "EI: -0.013271870091557503: 0.0\n",
      "Outer epoch 547, Inner epoch 1, Loss: \tIE: -0.006410497706383467: 0.0\n",
      "EI: -0.012336158193647861: 0.0\n",
      "Outer epoch 548, Inner epoch 1, Loss: \tIE: -0.006633144337683916: 0.0\n",
      "EI: -0.01259942352771759: 0.0\n",
      "Outer epoch 549, Inner epoch 1, Loss: \tIE: -0.006696418859064579: 0.0\n",
      "EI: -0.012967981398105621: 0.0\n",
      "Outer epoch 550, Inner epoch 1, Loss: \tIE: -0.007142344955354929: 0.0\n",
      "EI: -0.01368379220366478: 0.0\n",
      "Outer epoch 551, Inner epoch 1, Loss: \tIE: -0.006984720937907696: 0.0\n",
      "EI: -0.013209046795964241: 0.0\n",
      "Outer epoch 552, Inner epoch 1, Loss: \tIE: -0.006944701075553894: 0.0\n",
      "EI: -0.013148292899131775: 0.0\n",
      "Outer epoch 553, Inner epoch 1, Loss: \tIE: -0.0067862458527088165: 0.0\n",
      "EI: -0.013176031410694122: 0.0\n",
      "Outer epoch 554, Inner epoch 1, Loss: \tIE: -0.007345707155764103: 0.0\n",
      "EI: -0.01404010970145464: 0.0\n",
      "Outer epoch 555, Inner epoch 1, Loss: \tIE: -0.006852221675217152: 0.0\n",
      "EI: -0.013404957950115204: 0.0\n",
      "Outer epoch 556, Inner epoch 1, Loss: \tIE: -0.006831870414316654: 0.0\n",
      "EI: -0.01344167161732912: 0.0\n",
      "Outer epoch 557, Inner epoch 1, Loss: \tIE: -0.006717012729495764: 0.0\n",
      "EI: -0.013067523948848248: 0.0\n",
      "Outer epoch 558, Inner epoch 1, Loss: \tIE: -0.006379805970937014: 0.0\n",
      "EI: -0.012818558141589165: 0.0\n",
      "Outer epoch 559, Inner epoch 1, Loss: \tIE: -0.007477170787751675: 0.0\n",
      "EI: -0.014687108807265759: 0.0\n",
      "Outer epoch 560, Inner epoch 1, Loss: \tIE: -0.006747218314558268: 0.0\n",
      "EI: -0.013478647917509079: 0.0\n",
      "Outer epoch 561, Inner epoch 1, Loss: \tIE: -0.006886619143188: 0.0\n",
      "EI: -0.013878350146114826: 0.0\n",
      "Outer epoch 562, Inner epoch 1, Loss: \tIE: -0.006755094509571791: 0.0\n",
      "EI: -0.013832570984959602: 0.0\n",
      "Outer epoch 563, Inner epoch 1, Loss: \tIE: -0.007106108590960503: 0.0\n",
      "EI: -0.014579186215996742: 0.0\n",
      "Outer epoch 564, Inner epoch 1, Loss: \tIE: -0.00666458485648036: 0.0\n",
      "EI: -0.013858387246727943: 0.0\n",
      "Outer epoch 565, Inner epoch 1, Loss: \tIE: -0.006750582251697779: 0.0\n",
      "EI: -0.013832212425768375: 0.0\n",
      "Outer epoch 566, Inner epoch 1, Loss: \tIE: -0.007065786048769951: 0.0\n",
      "EI: -0.015001420862972736: 0.0\n",
      "Outer epoch 567, Inner epoch 1, Loss: \tIE: -0.007211820688098669: 0.0\n",
      "EI: -0.014873497188091278: 0.0\n",
      "Outer epoch 568, Inner epoch 1, Loss: \tIE: -0.006819416303187609: 0.0\n",
      "EI: -0.014703141525387764: 0.0\n",
      "Outer epoch 569, Inner epoch 1, Loss: \tIE: -0.006663944106549025: 0.0\n",
      "EI: -0.014021423645317554: 0.0\n",
      "Outer epoch 570, Inner epoch 1, Loss: \tIE: -0.007205535192042589: 0.0\n",
      "EI: -0.015082762576639652: 0.0\n",
      "Outer epoch 571, Inner epoch 1, Loss: \tIE: -0.0066457209177315235: 0.0\n",
      "EI: -0.014276360161602497: 0.0\n",
      "Outer epoch 572, Inner epoch 1, Loss: \tIE: -0.007034050300717354: 0.0\n",
      "EI: -0.015477672219276428: 0.0\n",
      "Outer epoch 573, Inner epoch 1, Loss: \tIE: -0.0066567896865308285: 0.0\n",
      "EI: -0.014312236569821835: 0.0\n",
      "Outer epoch 574, Inner epoch 1, Loss: \tIE: -0.0066827647387981415: 0.0\n",
      "EI: -0.014659165404736996: 0.0\n",
      "Outer epoch 575, Inner epoch 1, Loss: \tIE: -0.006968194618821144: 0.0\n",
      "EI: -0.015522334724664688: 0.0\n",
      "Outer epoch 576, Inner epoch 1, Loss: \tIE: -0.006920528132468462: 0.0\n",
      "EI: -0.015575372613966465: 0.0\n",
      "Outer epoch 577, Inner epoch 1, Loss: \tIE: -0.007122134789824486: 0.0\n",
      "EI: -0.015428375452756882: 0.0\n",
      "Outer epoch 578, Inner epoch 1, Loss: \tIE: -0.007444389164447784: 0.0\n",
      "EI: -0.016329413279891014: 0.0\n",
      "Outer epoch 579, Inner epoch 1, Loss: \tIE: -0.006665607914328575: 0.0\n",
      "EI: -0.014838521368801594: 0.0\n",
      "Outer epoch 580, Inner epoch 1, Loss: \tIE: -0.006279609631747007: 0.0\n",
      "EI: -0.014979659579694271: 0.0\n",
      "Outer epoch 581, Inner epoch 1, Loss: \tIE: -0.006475753616541624: 0.0\n",
      "EI: -0.014907477423548698: 0.0\n",
      "Outer epoch 582, Inner epoch 1, Loss: \tIE: -0.0068249935284256935: 0.0\n",
      "EI: -0.015728585422039032: 0.0\n",
      "Outer epoch 583, Inner epoch 1, Loss: \tIE: -0.007180456072092056: 0.0\n",
      "EI: -0.01603654772043228: 0.0\n",
      "Outer epoch 584, Inner epoch 1, Loss: \tIE: -0.006523899734020233: 0.0\n",
      "EI: -0.015321234241127968: 0.0\n",
      "Outer epoch 585, Inner epoch 1, Loss: \tIE: -0.006821257062256336: 0.0\n",
      "EI: -0.01570296287536621: 0.0\n",
      "Outer epoch 586, Inner epoch 1, Loss: \tIE: -0.007037131581455469: 0.0\n",
      "EI: -0.016036784276366234: 0.0\n",
      "Outer epoch 587, Inner epoch 1, Loss: \tIE: -0.006045883987098932: 0.0\n",
      "EI: -0.014731349423527718: 0.0\n",
      "Outer epoch 588, Inner epoch 1, Loss: \tIE: -0.006814582739025354: 0.0\n",
      "EI: -0.015841491520404816: 0.0\n",
      "Outer epoch 589, Inner epoch 1, Loss: \tIE: -0.006772193126380444: 0.0\n",
      "EI: -0.016042862087488174: 0.0\n",
      "Outer epoch 590, Inner epoch 1, Loss: \tIE: -0.006431514862924814: 0.0\n",
      "EI: -0.015728453174233437: 0.0\n",
      "Outer epoch 591, Inner epoch 1, Loss: \tIE: -0.007423579227179289: 0.0\n",
      "EI: -0.017138173803687096: 0.0\n",
      "Outer epoch 592, Inner epoch 1, Loss: \tIE: -0.006785810459405184: 0.0\n",
      "EI: -0.016161976382136345: 0.0\n",
      "Outer epoch 593, Inner epoch 1, Loss: \tIE: -0.006834728643298149: 0.0\n",
      "EI: -0.016211334615945816: 0.0\n",
      "Outer epoch 594, Inner epoch 1, Loss: \tIE: -0.00626576179638505: 0.0\n",
      "EI: -0.01563371904194355: 0.0\n",
      "Outer epoch 595, Inner epoch 1, Loss: \tIE: -0.006514015607535839: 0.0\n",
      "EI: -0.01618899777531624: 0.0\n",
      "Outer epoch 596, Inner epoch 1, Loss: \tIE: -0.007169966585934162: 0.0\n",
      "EI: -0.0171922966837883: 0.0\n",
      "Outer epoch 597, Inner epoch 1, Loss: \tIE: -0.006992006674408913: 0.0\n",
      "EI: -0.016756467521190643: 0.0\n",
      "Outer epoch 598, Inner epoch 1, Loss: \tIE: -0.006627880968153477: 0.0\n",
      "EI: -0.016261707991361618: 0.0\n",
      "Outer epoch 599, Inner epoch 1, Loss: \tIE: -0.0068396166898310184: 0.0\n",
      "EI: -0.016654621809720993: 0.0\n",
      "Outer epoch 600, Inner epoch 1, Loss: \tIE: -0.006883310154080391: 0.0\n",
      "EI: -0.016886213794350624: 0.0\n",
      "Outer epoch 601, Inner epoch 1, Loss: \tIE: -0.006680829916149378: 0.0\n",
      "EI: -0.016743440181016922: 0.0\n",
      "Outer epoch 602, Inner epoch 1, Loss: \tIE: -0.006690974812954664: 0.0\n",
      "EI: -0.016901783645153046: 0.0\n",
      "Outer epoch 603, Inner epoch 1, Loss: \tIE: -0.006732204928994179: 0.0\n",
      "EI: -0.01692025549709797: 0.0\n",
      "Outer epoch 604, Inner epoch 1, Loss: \tIE: -0.006522444076836109: 0.0\n",
      "EI: -0.01706613600254059: 0.0\n",
      "Outer epoch 605, Inner epoch 1, Loss: \tIE: -0.006761336699128151: 0.0\n",
      "EI: -0.017356378957629204: 0.0\n",
      "Outer epoch 606, Inner epoch 1, Loss: \tIE: -0.0069583384320139885: 0.0\n",
      "EI: -0.017465127632021904: 0.0\n",
      "Outer epoch 607, Inner epoch 1, Loss: \tIE: -0.007291292771697044: 0.0\n",
      "EI: -0.017844777554273605: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[39m=\u001b[39m train_model()\n",
      "Cell \u001b[0;32mIn[24], line 69\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# batch every 20 (ex. tried 50 and was slightly more noisy (but w/ similar mean))\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# batch_every = 20\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m# batch_loss = 0\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m outer_e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_outer):\n\u001b[0;32m---> 69\u001b[0m     i \u001b[39m=\u001b[39m correlated_mitral_activity()\n\u001b[1;32m     70\u001b[0m     hbar_ff \u001b[39m=\u001b[39m compute_feedforward_activity(i)\n\u001b[1;32m     72\u001b[0m     W_initial \u001b[39m=\u001b[39m compute_initial_recurrent_weights()\n",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mcorrelated_mitral_activity\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m projection \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnormal(torch\u001b[39m.\u001b[39mzeros((P_prime, P)), torch\u001b[39m.\u001b[39mones(P_prime, P) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msqrt(var))\n\u001b[1;32m      9\u001b[0m activity \u001b[39m=\u001b[39m p_prime_activity \u001b[39m@\u001b[39m projection\n\u001b[0;32m---> 10\u001b[0m \u001b[39mreturn\u001b[39;00m activity\u001b[39m.\u001b[39;49mto(gpu)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8aabaa70054798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.960642Z",
     "start_time": "2024-08-01T17:46:56.958505Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(torch.arange(losses.shape[0]), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss ratio\")\n",
    "    plt.title(\"Loss ratio per realization over time\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3843b2195b086d5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.959377Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = correlated_mitral_activity()\n",
    "hbar_ff = compute_feedforward_activity(i)\n",
    "W_random = compute_initial_recurrent_weights()\n",
    "R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da719c5a4a5ebbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.976483Z",
     "start_time": "2024-08-01T17:46:56.961173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stats at initialization\n",
    "mu_familiar_0 = torch.mean(R_random[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_0 = torch.var(R_random[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_0 = torch.mean(R_random[:num_e, novel_inds], dim=1)\n",
    "sig_novel_0 = torch.var(R_random[:num_e, novel_inds], dim=1)\n",
    "#print(f\"Initialization: \\nFamiliar -  mean: {mu_familiar_0}, var: {sig_familiar_0}\\nNovel -  mean: {mu_novel_0}, var: {sig_novel_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67971e29f9bb692",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.962883Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random = R_random.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9b6d1ff27717",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.964663Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IE: 0.27762988209724426: 0.0\n",
      "EI: -0.07021322101354599: 0.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_inner):\n\u001b[0;32m---> 23\u001b[0m         _, W_plastic, R_plastic \u001b[39m=\u001b[39m loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, with_loss\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m         W_tracked[i, :, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m W_plastic[ie_update_inds][ie_track_inds]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     25\u001b[0m         W_tracked[i, :, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m W_plastic[ei_update_inds][ei_track_inds]\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "W_plastic = W_random.detach().clone()\n",
    "R_plastic = R_random.detach().clone()\n",
    "with torch.no_grad():\n",
    "    ie_update_inds = get_update_inds(ie_post, ie_pre, W_plastic)\n",
    "    ei_update_inds = get_update_inds(ei_post, ei_pre, W_plastic)\n",
    "    clamp_min = torch.zeros_like(W_plastic)\n",
    "    clamp_min[ei_update_inds] = ei_min_weight\n",
    "    clamp_min[ie_update_inds] = ie_min_weight\n",
    "    clamp_max = torch.zeros_like(W_plastic)\n",
    "    clamp_max[ie_update_inds] = ie_max_weight\n",
    "    clamp_max[ei_update_inds] = ei_max_weight\n",
    "    weight_range = (clamp_min, clamp_max)\n",
    "\n",
    "\n",
    "# Number of weights from each group to track\n",
    "num_samples = 100\n",
    "W_tracked = torch.empty((n_inner, num_samples, 2))\n",
    "R_tracked = torch.empty((n_inner, num_i))\n",
    "ie_track_inds = torch.randint(0, len(ie_update_inds), size=(num_samples,))\n",
    "ei_track_inds = torch.randint(0, len(ei_update_inds), size=(num_samples,))\n",
    "with torch.no_grad():\n",
    "    for i in range(n_inner):\n",
    "        _, W_plastic, R_plastic = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        W_tracked[i, :, 0] = W_plastic[ie_update_inds][ie_track_inds].detach()\n",
    "        W_tracked[i, :, 1] = W_plastic[ei_update_inds][ei_track_inds].detach()\n",
    "        R_tracked[i, :] = torch.mean(R_plastic[num_e:, :], dim=1).detach()\n",
    "\n",
    "mu_familiar_f = torch.mean(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_f = torch.var(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_f = torch.mean(R_plastic[:num_e, novel_inds], dim=1)\n",
    "sig_novel_f = torch.var(R_plastic[:num_e, novel_inds], dim=1)\n",
    "mu_novel_diff = mu_novel_f - mu_novel_0\n",
    "mu_familiar_diff = mu_familiar_f - mu_familiar_0\n",
    "sig_novel_diff = sig_novel_f - sig_novel_0\n",
    "sig_familiar_diff = sig_familiar_f - sig_familiar_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6bbd72d52ab77",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.966417Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random = R_random.cpu()\n",
    "R_plastic = R_plastic.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630eb42d255674cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.968115Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(R_tracked)\n",
    "plt.title(\"I responses across inner epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea879c3b9927e2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.970104Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at how weights evolve over the course of inner epoch, for a random realization\n",
    "# Store a random set of 10 weights and see how the 10 weights change over the course of the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd48d9b5cdbe58",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.971685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.hist(torch.flatten(R_random[:num_e]), density=True, label=\"E responses\")\n",
    "    ax1.hist(torch.flatten(R_random[num_e:]), density=True, label=\"I responses\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.hist(torch.flatten(R_plastic[:num_e]), density=True, label=\"E responses\")\n",
    "    ax2.hist(torch.flatten(R_plastic[num_e:]), density=True, label=\"I responses\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Responses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7a48ffa1db260",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.973328Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random[num_e:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a783a635f96f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.975049Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO figure out why I responses keep getting blown up to 0\n",
    "R_plastic[num_e:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2901f2e35c4e550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.986239Z",
     "start_time": "2024-08-01T17:46:56.976720Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ei_update_inds].cpu().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71d4a512bc4881",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.978392Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ie_update_inds].cpu().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547affea0c66ded8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.979980Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO model is learning to set all E->I weights to 0, and that's causing I responses to go to 0, then I->E weights can't do anything to fix that\n",
    "torch.unique((W_plastic[ie_update_inds].cpu().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970014f84261189",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.981629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    sparsities_0 = sparsity_per_odor(R_random)\n",
    "    spars_novel = sparsities_0[novel_inds]\n",
    "    spars_familiar = sparsities_0[familiar_inds]\n",
    "    ax1.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax1.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    sparsities_f = sparsity_per_odor(R_plastic)\n",
    "    spars_novel = sparsities_f[novel_inds]\n",
    "    spars_familiar = sparsities_f[familiar_inds]\n",
    "    ax2.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax2.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Sparsity per odor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68603cd329249b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.983228Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verification_set(ie_model, ei_model, runs=10):\n",
    "    corrs = torch.empty((runs,))\n",
    "    ratios = torch.empty((runs,))\n",
    "    spars_change = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        I_ff = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(I_ff)\n",
    "        W_random = compute_initial_recurrent_weights()\n",
    "        R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)\n",
    "        _, initial_corr = odor_corrs(R_random)\n",
    "        spars_initial = sparsity_per_odor(R_random)\n",
    "        initial_spars_diff = torch.abs(torch.mean(spars_initial[novel_inds]) - torch.mean(spars_initial[familiar_inds]))\n",
    "        \n",
    "        ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "        ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "        \n",
    "        for _ in range(n_inner):\n",
    "            _, W_random, R_random = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_random, R_random, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        \n",
    "        _, final_corr = odor_corrs(R_random)\n",
    "        ratio = final_corr / initial_corr\n",
    "        spars_final = sparsity_per_odor(R_random)\n",
    "        # Plot actual difference in the sparsity number, not the percent change ratio\n",
    "        # - Also see whether \n",
    "        final_spars_diff = torch.abs(torch.mean(spars_final[novel_inds]) - torch.mean(spars_final[familiar_inds]))\n",
    "        \n",
    "        print(f\"Corr: {initial_corr} -> {final_corr}, Sparsity: {initial_spars_diff} -> {final_spars_diff}, Loss Ratio: {ratio}\")\n",
    "        corrs[i] = final_corr.item()\n",
    "        ratios[i] = ratio.item()\n",
    "        spars_change[i] = (((final_spars_diff - initial_spars_diff) / initial_spars_diff) * 100).item()\n",
    "        \n",
    "    return corrs, ratios, spars_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bafcc0031d6f22",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.984704Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrs, ratios, spars_change = verification_set(ie_model, ei_model, runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae243066bddc4c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.986194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(ratios, bins=20)\n",
    "plt.title(\"Loss ratios\")\n",
    "plt.show()\n",
    "plt.hist(spars_change, bins=20)\n",
    "plt.title(\"Percent change in sparsity per odor for novel vs. familiar families\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cf70906cd0175",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.987518Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spars_change[spars_change < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279dce2095c309d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.988670Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# version = \"6\"\n",
    "# ie_path = f\"./joint_models/ie_models/ie_model_{version}\"\n",
    "# torch.save(ie_model.state_dict(), ie_path)\n",
    "# ei_path = f\"./joint_models/ei_models/ei_model_{version}\"\n",
    "# torch.save(ei_model.state_dict(), ei_path)\n",
    "\n",
    "# Model 2 - loss ratios between 0.75 and 1, most sparsity changes within 100%\n",
    "# Model 3 - loss ratios between 0.94 and 1.04, reduces sparsity close to 100% (but has NaN values) \n",
    "# Model 4 - reduces sparsity close to 100%, loss ratios between 0.85 and 1 (basically tries to set weights to 0)\n",
    "# Model 5 - loss ratios between 0.9 and 1.1, most sparsity changes go to 0 (negative 100% sparsity change)\n",
    "\n",
    "# Interesting result from model 1 - tries to set all weights to 0\n",
    "#ie_model = create_model()\n",
    "#ie_model.load_state_dict(torch.load(ie_path))\n",
    "#ei_model = create_model()\n",
    "#ei_model.load_state_dict(torch.load(ei_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8ceeefdbfa5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.990202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_colormap(model, type=\"ie\"):\n",
    "    num_ticks = 100\n",
    "    E_min, E_max = torch.min(R_plastic[:num_e, :]), torch.max(R_plastic[:num_e, :])\n",
    "    I_min, I_max = torch.min(R_plastic[num_e:, :]), torch.max(R_plastic[num_e:, :])\n",
    "    E_vals = torch.linspace(E_min, E_max, num_ticks)\n",
    "    I_vals = torch.linspace(I_min, I_max, num_ticks)\n",
    "    E_coords, I_coords = torch.meshgrid(E_vals, I_vals, indexing=\"ij\")\n",
    "    R_plot = 0\n",
    "    if type == \"ie\":\n",
    "        R_plot = torch.stack((E_coords.flatten(), I_coords.flatten()), dim=1)\n",
    "    elif type == \"ei\":\n",
    "        R_plot = torch.stack((I_coords.flatten(), E_coords.flatten()), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        plasticity_vals = model(R_plot.to(gpu)).squeeze(0).cpu()\n",
    "        \n",
    "    plot = plt.scatter(R_plot[:, 0], R_plot[:, 1], c=plasticity_vals, cmap='rainbow')\n",
    "    clrbar = plt.colorbar(plot)\n",
    "    clrbar.set_label('Model plasticity')\n",
    "    plt.xlabel(\"E responses\")\n",
    "    plt.ylabel(\"I responses\")\n",
    "        \n",
    "    return E_min, E_max, I_min, I_max, R_plot, plasticity_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28c037048e3b5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.991543Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def fit_model(model, type=\"ie\", degree=3):\n",
    "    E_min, E_max, I_min, I_max, R_plot, plasticity_vals = make_colormap(model, type)\n",
    "    \n",
    "\n",
    "    std = 2 # Take data within 2 std of responses mean\n",
    "    mu_e = torch.mean(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    sig_e = torch.std(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    mu_i = torch.mean(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    sig_i = torch.std(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    e_train_bounds = (torch.max(E_min, mu_e - std*sig_e), torch.min(E_max, mu_e + std*sig_e))\n",
    "    i_train_bounds = (torch.max(I_min, mu_i - std*sig_i), torch.min(I_max, mu_i + std*sig_i))\n",
    "    \n",
    "    reg = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "    \n",
    "    e_inds = torch.logical_and(torch.ge(R_plot[:, 0], e_train_bounds[0]), torch.le(R_plot[:, 0], e_train_bounds[1]))\n",
    "    i_inds = torch.logical_and(torch.ge(R_plot[:, 1], i_train_bounds[0]), torch.le(R_plot[:, 1], i_train_bounds[1]))\n",
    "    b = torch.logical_and(e_inds, i_inds)\n",
    "    X = R_plot[b]\n",
    "    y = plasticity_vals[b].squeeze(0)\n",
    "    \n",
    "    reg = reg.fit(X, y)\n",
    "    # Exclude bias term\n",
    "    coefs = reg.named_steps['linear'].coef_[0][1:]\n",
    "    # How many terms to consider\n",
    "    take = 3\n",
    "    # Take largest magnitude terms\n",
    "    inds = np.argsort(np.abs(coefs))[-take:]\n",
    "\n",
    "    # Exclude bias term\n",
    "    term_list = reg.named_steps['poly'].powers_[1:, :]\n",
    "    terms = f\"[Pre, Post] Powers:\\n\"\n",
    "    for i in range(take - 1, -1, -1):\n",
    "        term = f\"{term_list[inds][i]}: {coefs[inds][i]}\\n\"\n",
    "        terms += term\n",
    "    \n",
    "    print(terms)\n",
    "    \n",
    "    preds = reg.predict(X).ravel()\n",
    "    return X, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac6392aab0837e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.992481Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ie_X, ie_preds = fit_model(ie_model, type=\"ie\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ie_X[:, 0], ie_X[:, 1], c=ie_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"E-I plasticity based on neural responses\")\n",
    "plt.xlabel(\"E responses\")\n",
    "plt.ylabel(\"I responses\")\n",
    "plt.show()\n",
    "\n",
    "ei_X, ei_preds = fit_model(ei_model, type=\"ei\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ei_X[:, 0], ei_X[:, 1], c=ei_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"I-E plasticity based on neural responses\")\n",
    "plt.xlabel(\"I responses\")\n",
    "plt.ylabel(\"E responses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa938bb7961846a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:57.060814Z",
     "start_time": "2024-08-01T17:46:56.993604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a plot of rho_0 vs rho_final (for ex. 100 random initializations) to see whether the model can decorrelate odors\n",
    "# Even though potentiation is always positive for the set of responses, since we are doing E to I, increasing the connection strength effectively increases inhibition too, so it balances out\n",
    "# Not directly hebbian because that would mean there is some constant c*r_i*r_j, but when one of the responses is decreasing, the resulting potentiation doesn't decrease\n",
    "# Instead, it will be an a * r_i + b * r_j term\n",
    "# These terms come from the polynomial expansion of the function defined by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fe6484d9fbc92",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.994660Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at statistics of E neurons (mean firing rate across odors, variance, etc) at initialization compared to after plasticity\n",
    "# Also look at sparsity - one across odors and another across neurons (sparsity is essentially 1 minus the square of the coefficient of variation)\n",
    "# We care mostly about the sparsity across neurons (ex between neurons) and what it would be between odors (should be similar between the familiar odors b/c that's where we applied the plasticity) and we also know what it's like between novel odors\n",
    "# We don't want a change in firing rate, because it should be same for novel and familiar odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed3911273faa8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.995682Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pct_change = ((W_tracked[:, :, 0] - w_ie) / w_ie) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"E-I weight change\")\n",
    "    plt.show()\n",
    "    \n",
    "    pct_change = ((W_tracked[:, :, 1] - w_ei) / w_ei) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"I-E weight change\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d8df91ba43350",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.996672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_change = ((W_plastic[ie_update_inds].cpu() - w_ie) / w_ie) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"E-I Percent weight change from initialization\")\n",
    "plt.show()\n",
    "\n",
    "W_change = ((W_plastic[ei_update_inds].cpu() - w_ei) / w_ei) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"I-E Percent weight change from initialization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d07fb09871512a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.997803Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_familiar_0.detach().cpu() - mu_novel_0.detach().cpu(), bins=50, label=\"Before\")\n",
    "plt.hist(mu_familiar_f.detach().cpu() - mu_novel_f.detach().cpu(), bins=50, label=\"After\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90208f664a85ae",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.998838Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_novel_diff, bins=50, label=\"Novel\")\n",
    "plt.hist(mu_familiar_diff, bins=50, label=\"Familiar\")\n",
    "plt.title(\"Difference in mean responses before and after plasticity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc851bc8f43f35",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.999877Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_novel_0 = sparsity_per_neuron(R_random, novel_inds)\n",
    "sp_novel_f = sparsity_per_neuron(R_plastic, novel_inds)\n",
    "sp_familiar_0 = sparsity_per_neuron(R_random, familiar_inds)\n",
    "sp_familiar_f = sparsity_per_neuron(R_plastic, familiar_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee33d1ae62b6cd3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.000850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.hist(sp_novel_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_0\")\n",
    "    plt.hist(sp_familiar_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_0\")\n",
    "    plt.hist(sp_novel_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_f\")\n",
    "    plt.hist(sp_familiar_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_f\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce75bd2a06f39a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.001884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create scatter plot of odor correlation vs sparsity difference - odors that are highly positively correlated should have similar sparsities across neurons, and those that are highly negatively correlated should have different sparsities across neurons\n",
    "# Check - increase P' all the way to 16, and with lower correlations, there should be less variability between the sparsity for each odor\n",
    "# - would tell us how much the natural spread in sparsity is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd6174a08701aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.003211Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Re-run model w/ lower threshold (0 stdev above mean)\n",
    "# 2. Figure out percentage-wise how much the weights actually change (do a hist)\n",
    "# - if it's 1-2% it's too small, we want the weights to change 10-100%, the weights could even change 10-fold\n",
    "# 3. Add metrics to training function - automatically compute sparsity etc (also look at the sparsity between odors, not just the sparsity between neurons, see if it changes for novel vs familiar)\n",
    "# Create bar plot for each number of odors, how many neurons respond to that number of odors (ideally, if we have a threshold of 0 stdev, most neurons should respond to ~4 odors, since on average a neuron will respond to half of the total odors, so 8 odors, and out of those, it should be equal between 4 novel and 4 familiar)\n",
    "# Hypothesis right now - weights aren't changing that much, so we can add more epoch_inner steps (since the gradient isn't blowing up)\n",
    "# Then, try removing plasticity ramp - keep 1e-3 (don't do epoch_inner increase and remove plasticity ramp at same time)\n",
    "# Goal: understand what mechanism the meta-learning discovered that makes correlations smaller (see whether it acts on sparsity etc)\n",
    "# Think of other metrics to quantify network behavior to understand change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e8808ab6eb092",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.004349Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f64f3bee69275d7dadabcd164c00bee7a237ebc40dc30e8b43706029d0d9fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
