{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.243331Z",
     "start_time": "2024-08-01T17:46:22.473598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install torch\n",
    "# !pip install matplotlib\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4fbc6b56b8cbae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274764Z",
     "start_time": "2024-08-01T17:46:22.475263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80a1147c4c0a6f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274897Z",
     "start_time": "2024-08-01T17:46:24.148740Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.device(\"cuda:0\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# Use smaller network for testing - ex 2000 neurons\n",
    "# Even for the project, doing it for 10^6 neurons would take too long\n",
    "# Problem this creates: test network is denser than actual network b/c we have 10^3 neurons but 10^2 connections per neuron\n",
    "num_neurons = 2000\n",
    "num_i = int(0.1 * num_neurons)\n",
    "num_e = int(0.9 * num_neurons)\n",
    "\n",
    "# Epsilon value close to 0 to prevent nan in division by 0\n",
    "eps = 1e-6\n",
    "\n",
    "# Num excitatory inputs and inhibitory inputs to each neuron (in reality it should be 500 but we reduce it here to make things faster)\n",
    "k = 100\n",
    "\n",
    "# Number of olfactory bulb channels (glomeruli) to each neuron\n",
    "D = 10 ** 3\n",
    "# For each neuron, how many glomeruli inputs it receives (should be 10^2)\n",
    "num_channel_inputs = 100\n",
    "\n",
    "# Number of odors\n",
    "P = 16\n",
    "# Novel activity is up to P // 2, and familiar activity is after\n",
    "novel_inds = torch.arange(0, P // 2)\n",
    "familiar_inds = torch.arange(P // 2, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4e13c61f25818ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274933Z",
     "start_time": "2024-08-01T17:46:24.155175Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates sparse adjacency matrix with the given probability of edge connection and size mxn\n",
    "def create_adj_matrix(p, m, n):\n",
    "    # num_connections = int(p * m * n)\n",
    "    # m_coords = torch.randint(0, m, (num_connections,))\n",
    "    # n_coords = torch.randint(0, n, (num_connections,))\n",
    "    # indices = torch.vstack((m_coords, n_coords))\n",
    "    # values = torch.ones(num_connections)\n",
    "    # A_mn = torch.sparse_coo_tensor(indices, values, (m, n))\n",
    "    probs = torch.ones(m, n) * p\n",
    "    A_mn = torch.bernoulli(probs)\n",
    "    return A_mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6b94961ca601610",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274966Z",
     "start_time": "2024-08-01T17:46:24.159685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New way of generating correlations between odors: we want different sets of odors to be correlated differently, so that when we subtract each neuron's mean activity over odors, it doesn't cancel out the variation between odors (if all the odors are correlated the same, they will tend to produce similar values for a single neuron and therefore subtracting by the mean will remove these values and only leave small fluctuations)\n",
    "# So we sample a small set of odors P' and make them linearly independent, and then by multiplying by a P'x P gaussian matrix we project into mitral cell activity space for all P odors, basically making the P odors a linear combination of the set of P' odors (the smaller P' is, the more correlated the resulting set of P odors will be)\n",
    "# We also scale the variance depending on how small P' is, so we will maintain differently correlated odors, just with higher total correlation if P' is small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28887b0933bdd67a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.274997Z",
     "start_time": "2024-08-01T17:46:24.162068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_prime = 4\n",
    "def correlated_mitral_activity():\n",
    "    # Each of the P' odors is independent (correlation of 0)\n",
    "    sigma_p_prime = torch.zeros((P_prime, P_prime)).fill_diagonal_(1)\n",
    "    dist = torch.distributions.MultivariateNormal(torch.zeros(P_prime), sigma_p_prime)\n",
    "    p_prime_activity = dist.sample(torch.Size([D]))\n",
    "    var = 1 / P_prime\n",
    "    projection = torch.normal(torch.zeros((P_prime, P)), torch.ones(P_prime, P) * np.sqrt(var))\n",
    "    activity = p_prime_activity @ projection\n",
    "    return activity.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67da2d3fe4b6a081",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275027Z",
     "start_time": "2024-08-01T17:46:24.167675Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes in mitral activity I and computes feedforward activity h_bar_ff\n",
    "def compute_feedforward_activity(I):\n",
    "    # Probability that a channel weight will be nonzero\n",
    "    p = num_channel_inputs / D\n",
    "    # Only the first 0.9 * n rows should have this bernoulli number, the rest should be 0 b/c they don't receive a channel input\n",
    "    # Check whether each neuron still receives ~10^2 nonzero inputs or what the distribution actually looks like\n",
    "    # Because when we calculate the adjacency matrix we don't go by row (e.g ensuring each neuron has these ~10^2 connections)\n",
    "    # Alternative: sample from Binomial distribution w/ mean 100\n",
    "    # The output n for each row is the number of nonzero inputs, and you choose a random subset n of the indices for that row and make them 1\n",
    "    with torch.device(gpu):\n",
    "        a = create_adj_matrix(p, num_e, D)\n",
    "        # Inhibitory neurons don't receive channel input\n",
    "        # This is the first simplification, where we neglect the first inhibitory layer I_ff\n",
    "        b = torch.zeros(size=(num_i, D))\n",
    "        W_ff = torch.cat(tensors=(a, b), dim=0)\n",
    "        \n",
    "        h_ff = (W_ff @ I) * (1 / np.sqrt(num_channel_inputs))\n",
    "        h_bar_ff = torch.zeros_like(h_ff)\n",
    "        h_bar_ff[:num_e] = h_ff[:num_e] - torch.mean(h_ff[:num_e], dim=0, keepdim=True)\n",
    "    return h_bar_ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23d7ccb6f4b0787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275058Z",
     "start_time": "2024-08-01T17:46:24.170963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_initial_recurrent_weights():\n",
    "    k_ee = k_ei = k_ie = k_ii = k\n",
    "    #p_ee = k_ee / num_e\n",
    "    # k inhibitory inputs to that e neuron, out of num_i total inhibitory neurons gives the connection probability per neuron\n",
    "    p_ei = k_ei / num_i\n",
    "    p_ie = k_ie / num_e\n",
    "    #p_ii = k_ii / num_i\n",
    "    \n",
    "    # Constants\n",
    "    #w_ee = 0.1\n",
    "    w_ei = 0.2\n",
    "    w_ie = 0.5\n",
    "    #w_ii = 0.3\n",
    "    # Ignore ee and ii weights for now:\n",
    "    p_ee = p_ii = w_ee = w_ii = 0\n",
    "    with torch.device(gpu):\n",
    "        W_ee = create_adj_matrix(p_ee, num_e, num_e) * w_ee\n",
    "        W_ei = create_adj_matrix(p_ei, num_e, num_i) * -w_ei\n",
    "        W_ie = create_adj_matrix(p_ie, num_i, num_e) * w_ie\n",
    "        W_ii = create_adj_matrix(p_ii, num_i, num_i) * -w_ii\n",
    "        \n",
    "        # Concat\n",
    "        W_1 = torch.cat(tensors=(W_ee, W_ei), dim=1)\n",
    "        W_2 = torch.cat(tensors=(W_ie, W_ii), dim=1)\n",
    "        W_rec = torch.cat(tensors=(W_1, W_2), dim=0)\n",
    "    \n",
    "    return W_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ae3fd6edb6a55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275088Z",
     "start_time": "2024-08-01T17:46:24.176889Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes activation threshold for neurons, based on the standard deviation of their firing rates across odors\n",
    "# This average standard deviation, multiplied by theta=2, ensures that each neuron will fire for only 5% of odors\n",
    "def compute_threshold(total_input, theta):\n",
    "    #center = torch.mean(torch.mean(total_input, dim=1, keepdim=True), dim=0, keepdim=True)\n",
    "    #shift = torch.std(total_input, dim=1, keepdim=True)\n",
    "    threshold = torch.ones_like(total_input) * theta\n",
    "    # Since inhibitory neurons are linear\n",
    "    threshold[num_e:, :] = 0\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce5f8670e8cb95e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275118Z",
     "start_time": "2024-08-01T17:46:24.180482Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ReLU for excitatory, linear for inhibitory\n",
    "def neuron_activations(X):\n",
    "    # Mask to keep excitatory\n",
    "    mask1 = torch.ones((num_neurons, 1), device=gpu)\n",
    "    mask1[num_e:, :] = 0\n",
    "    # Mask to keep inhibitory\n",
    "    mask2 = torch.zeros((num_neurons, 1), device=gpu)\n",
    "    mask2[num_e:, :] = 1\n",
    "    activation = (torch.relu(X) * mask1) + (X * mask2)\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14aa733bbbadf1db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275148Z",
     "start_time": "2024-08-01T17:46:24.186449Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Computes R for each odor, with the activation threshold theta\n",
    "def compute_piriform_response(h_bar_ff, W_rec, threshold_mult):\n",
    "    # The coefficient of x_bar\n",
    "    tau = 1\n",
    "    # time step\n",
    "    dt = 0.1\n",
    "    # Number of time steps\n",
    "    T = 200\n",
    "    \n",
    "    # Initial condition where states are gaussian\n",
    "    mu_0 = 0.\n",
    "    sigma_0 = 0.2\n",
    "    X_0 = torch.normal(mu_0, sigma_0, size=(num_neurons, P))\n",
    "    X = X_0\n",
    "    X = X.to(gpu)\n",
    "    \n",
    "    #pts = []\n",
    "    for i in range(T-2):\n",
    "        with torch.no_grad():\n",
    "            part1 = -1 * X\n",
    "            part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "            part3 = h_bar_ff\n",
    "            dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "            X = X + (dXdt * dt)\n",
    "        # Look at convergence pattern for first odor, assuming that it'll\n",
    "        # be similar across odors (since they are all independent)\n",
    "        #pts.append(torch.mean(dXdt, dim=0)[0].item())\n",
    "   \n",
    "    # On the last 2 iterations only, track the gradient\n",
    "    # TODO increased to 4 just for more coverage\n",
    "    X.requires_grad_(True)\n",
    "    # Turn this into matrix exponential since during convergent dynamics the ReLU activates the same neurons (no longer becomes nonlinearity)\n",
    "    for j in range(2):\n",
    "        part1 = -1 * X\n",
    "        part2 = (W_rec @ neuron_activations(X)) * (1 / np.sqrt(k))\n",
    "        part3 = h_bar_ff\n",
    "        dXdt = (1 / tau) * (part1 + part2 + part3)\n",
    "        X = X + (dXdt * dt)\n",
    "    \n",
    "    # The total input to the neuron at this last time step (should be equivalent to the resulting value of X after this time step, since dxdt = 0 after the recurrent network converges)\n",
    "    total_input = part2 + part3\n",
    "\n",
    "    # TODO fixed threshold across neurons - theta is now the threshold value\n",
    "    threshold = compute_threshold(total_input, 0)\n",
    "    \n",
    "    # Plot derivatives to see if state converged\n",
    "    # plt.plot(torch.arange(T-2), pts)\n",
    "    # plt.show()\n",
    "    R = neuron_activations(X - threshold)\n",
    "    \n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "defc0132008cbd72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275177Z",
     "start_time": "2024-08-01T17:46:24.189623Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute dimensionality of activity matrix R for either novel or familiar\n",
    "# def compute_dim(R, odor_inds):\n",
    "#     # Only compute for the excitatory neurons (b/c those are the ones that send signals to rest of brain\n",
    "#     C = torch.cov(R[:num_e, odor_inds[0]:odor_inds[-1]])\n",
    "#     dim = torch.trace(C) ** 2 / torch.trace(C @ C)\n",
    "#     return dim\n",
    "\n",
    "# trace() is invariant for cyclic permutations of a matrix\n",
    "# Since C is symmetric, it can be orthogonally diagonalized into UDU^T where U is composed of orthonormal eigenvectors, U^T = U^-1, and D is a diagonal matrix of eigenvalues\n",
    "# therefore, trace(C) = trace(UDU^T) = trace(DU^TU) = trace(D) = sum(eigvals of C)\n",
    "# Similarly for the denominator, we need to compute the sum of the squared eigenvalues, which is trace(D^2). trace(D^2) = trace(D^2U^TU) = trace(UD^2U^T) = trace((UDU^T)^2)) [by the property of matrix exponentiation for a diagonalizable matrix] = trace(C^2) = trace(C @ C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24911045583425aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275393Z",
     "start_time": "2024-08-01T17:46:24.224277Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def create_model() -> torch.nn.Sequential:\n",
    "    # Number of neurons in hidden layer\n",
    "    hidden_size = 100\n",
    "    # Constrain output to certain values: nn.Hardtanh(-1, 1)\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(2, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, 1),\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1cd6e5a0aa03ae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275425Z",
     "start_time": "2024-08-01T17:46:24.228522Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start and stop indices for the section of W_rec we want to update, respectively \n",
    "# Takes in R matrix (neuron responses for each odor and tuple of update inds representing ie, then ei (each element in that tuple is itself a tuple of (post, pre))\n",
    "def compute_updates(R: torch.Tensor, models: tuple, update_inds: tuple) -> torch.Tensor:\n",
    "    # Compute the same pairs of R_i and R_j for every odor, per model\n",
    "    all_updates = []\n",
    "    for i in range(len(models)):\n",
    "        postsyn_responses = R[update_inds[i][0], :]\n",
    "        presyn_responses = R[update_inds[i][1], :]\n",
    "        model_input = torch.stack(tensors=(presyn_responses, postsyn_responses), dim=2).transpose(1, 0)\n",
    "        updates_per_odor = models[i](model_input)\n",
    "        model_updates = torch.mean(updates_per_odor, dim=0).squeeze(dim=1)\n",
    "        all_updates.append(model_updates)\n",
    "    \n",
    "    return all_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f51d515fadf0381",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275457Z",
     "start_time": "2024-08-01T17:46:24.232958Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ie_post = (num_e, num_neurons)\n",
    "# ie_pre = (0, num_e)\n",
    "# ei_post = (0, num_e)\n",
    "# ei_pre = (num_e, num_neurons)\n",
    "# ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "# ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "# # (16, 19953, 2) -> (16, 19953)\n",
    "# a1, a2 = compute_updates(R_random.to(gpu), (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "# #print(a1.shape, a2.shape)\n",
    "# print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8887e40e8a4d89c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275488Z",
     "start_time": "2024-08-01T17:46:24.235295Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def odor_corrs(R):\n",
    "    # We don't care about the actual responses per odor, just about a neuron's fluctuations around its mean response across odors\n",
    "    R_adjusted = R[:num_e] - torch.mean(R[:num_e], dim=1, keepdim=True)\n",
    "    # Each odor becomes a variable, because we want to calculate correlations between them across neurons\n",
    "    R_adjusted.t_()\n",
    "    # Like cov but divides by standard deviations, effectively normalizing the values (the diagonals of the resulting matrix become 1)\n",
    "    corrcoefs = torch.corrcoef(R_adjusted)\n",
    "    # If the responses are 0, variances across neurons will be 0, so denominator of corrcoef is 0, so term becomes nan\n",
    "    # In this case, the responses are \"perfectly correlated\" (bc always same value of 0) so its maximum correlation\n",
    "    # TODO do we need to change this NaN formulation so we propagate grad correctly?\n",
    "    corrcoefs = torch.nan_to_num(corrcoefs, nan=1.0)\n",
    "    # We only care about the correlations between the familiar odors\n",
    "    familiar_corrs = corrcoefs[P//2:P, P//2:P] - torch.eye(P // 2, device=gpu)\n",
    "    corr_sum = torch.sum(familiar_corrs ** 2)\n",
    "    avg_corr = torch.mean(torch.abs(familiar_corrs))\n",
    "    \n",
    "    return corr_sum, avg_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4d37265e0e371e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275519Z",
     "start_time": "2024-08-01T17:46:24.241695Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sparsity per odor, across all (E) neurons\n",
    "def sparsity_per_odor(R):\n",
    "    # Epsilon for if we have zero responses\n",
    "    eps = 1e-6\n",
    "    sp_per_odor = 1 - ((torch.sum(R[:num_e], dim=0) ** 2 + eps) / (num_e * (torch.sum(R[:num_e] ** 2, dim=0)) + eps))\n",
    "    # Sparsity nan means that the responses were all 0 for an odor, meaning that its max sparsity of 1\n",
    "    return sp_per_odor\n",
    "\n",
    "# Sparsity per (E) neuron, across a given odor family\n",
    "def sparsity_per_neuron(R, odor_inds):\n",
    "    sp_per_neuron = 1 - (\n",
    "                (torch.sum(R[:num_e, odor_inds], dim=1) ** 2) / ((P // 2) * torch.sum(R[:num_e, odor_inds] ** 2, dim=1)))\n",
    "    return sp_per_neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1003ae68e7009d39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275588Z",
     "start_time": "2024-08-01T17:46:24.248644Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try to minimize the correlations between values\n",
    "def loss_fn(R, W, ie_update_inds, ei_update_inds, lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp, do_print=True):\n",
    "    corr_sum, avg_corr = odor_corrs(R)\n",
    "    corr_loss = (1 / ((P // 2) ** 2)) * corr_sum\n",
    "    corr_term = lambda_corr * corr_loss\n",
    "    \n",
    "    means = torch.mean(R[:num_e], dim=0)\n",
    "    means_novel = torch.mean(means[novel_inds])\n",
    "    means_familiar = torch.mean(means[familiar_inds])\n",
    "    if torch.abs(means_novel + means_familiar) < eps:\n",
    "        # All means are the same so there's technically no loss\n",
    "        mu_term = 0\n",
    "    else:\n",
    "        mu_term = lambda_mu * (((means_familiar - means_novel + eps) / (means_novel + means_familiar)) ** 2)\n",
    "    \n",
    "    vars = torch.var(R[:num_e], dim=0)\n",
    "    var_novel = torch.mean(vars[novel_inds])\n",
    "    var_familiar = torch.mean(vars[familiar_inds])\n",
    "    if torch.abs(var_novel + var_familiar) < eps:\n",
    "        # All variances are the same so there's technically no loss\n",
    "        var_term = 0\n",
    "    else:\n",
    "        var_term = lambda_var * (((var_familiar - var_novel) / (var_novel + var_familiar)) ** 2)\n",
    "    \n",
    "    sparsities = sparsity_per_odor(R)\n",
    "    spars_novel = torch.mean(sparsities[novel_inds])\n",
    "    spars_familiar = torch.mean(sparsities[familiar_inds])\n",
    "    if torch.abs(spars_novel + spars_familiar) < eps:\n",
    "        # Sparsities are technically the same so the term shouldn't contribute to loss\n",
    "        spars_term = 0\n",
    "    else:\n",
    "        spars_term = lambda_sp * (((spars_familiar - spars_novel) / (spars_novel + spars_familiar)) ** 2)\n",
    "\n",
    "    # Multiply by 1/P^2 for the decorrelation term and by 1 / num_e*K for the (EI) weight regularization term and 1/num_i*K for the (IE) weight regularization term\n",
    "    # Track each term independently to make sure they're on the same scale\n",
    "    # Do it for backprop too \n",
    "\n",
    "    ie_weight_reg = torch.sum((W[ie_update_inds] - w_ie) ** 2) / (num_i * k)\n",
    "    ei_weight_reg = torch.sum((W[ei_update_inds] - w_ei) ** 2) / (num_e * k)\n",
    "\n",
    "    # IE and EI have same weight regularization term for now\n",
    "    ie_weight_term = lambda_w * ie_weight_reg\n",
    "    ei_weight_term = lambda_w * ei_weight_reg\n",
    "    \n",
    "    if do_print:\n",
    "        #print(\"Avg Corr: %.4f, Corr: %.4f, Mu: %.4f, Var: %.4f, Sparsity: %.4f\" % (avg_corr, corr_term, mu_term, var_term, spars_term))\n",
    "        print(\"Avg Corr: %.4f, Corr: %.4f, Sparsity: %.4f, IE: %.4f, EI: %.4f\" % (avg_corr, corr_term, spars_term, ie_weight_term, ei_weight_term))\n",
    " \n",
    "    loss = corr_term + mu_term + var_term + spars_term + ie_weight_term + ei_weight_term\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e8cc5982ed854c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275650Z",
     "start_time": "2024-08-01T17:46:24.251548Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Detaching vs zero grad - should detach because we have a term dependent on the previous model iteration which isn't zero but some constant gradient, accumulated from that model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1252ae4977d8dbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275684Z",
     "start_time": "2024-08-01T17:46:24.260339Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_after_odors(ie_model: torch.nn.Sequential, ei_model: torch.nn.Sequential, ie_update_inds, ei_update_inds, W_rec: torch.Tensor, R_current: torch.Tensor, h_bar_ff: torch.Tensor, threshold_mult, plasticity_ie, plasticity_ei, weight_decay_rate, weight_range: tuple, lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False):\n",
    "   # First, compute the respective weight updates through the novel and familiar odors from the current neural responses (which are from the current weight matrix)\n",
    "    \n",
    "    W_rec.requires_grad_(True)\n",
    "    \n",
    "    ie_model_updates, ei_model_updates = compute_updates(R_current, (ie_model, ei_model), (ie_update_inds, ei_update_inds))\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     val_tensor = ((1 - plasticity_rate * weight_decay_rate) * W_rec[update_inds]) + updates + (plasticity_rate * odor_update)\n",
    "    #     condition = torch.logical_and(torch.gt(val_tensor, min_weight), torch.le(val_tensor, max_weight))\n",
    "    \n",
    "    ie_updates = plasticity_ie * (ie_model_updates - weight_decay_rate * W_rec[ie_update_inds])\n",
    "    ei_updates = plasticity_ei * (ei_model_updates - weight_decay_rate * W_rec[ei_update_inds])\n",
    "    \n",
    "    \n",
    "    ie_cond = torch.logical_and(torch.ge(W_rec[ie_update_inds] + ie_updates, weight_range[0][ie_update_inds]), torch.le(W_rec[ie_update_inds] + ie_updates, weight_range[1][ie_update_inds]))\n",
    "    ei_cond = torch.logical_and(torch.ge(W_rec[ei_update_inds] + ei_updates, weight_range[0][ei_update_inds]), torch.le(W_rec[ei_update_inds] + ei_updates, weight_range[1][ei_update_inds]))\n",
    "    \n",
    "    # TODO clamping updates instead of the weights post-update - change in gradient?\n",
    "    # TODO if we clamp to bounds then we'll see changes in correlation, b/c there is a change in weight instead of none at all\n",
    "    ie_bounded_updates = torch.where(ie_cond, ie_updates, 0)\n",
    "    ei_bounded_updates = torch.where(ei_cond, ei_updates, 0)\n",
    "   \n",
    "    # # TODO penalize model for causing weight to go over threshold\n",
    "    # # Amount below min\n",
    "    # ie_a = torch.relu(weight_range[0][ie_update_inds] - (W_rec[ie_update_inds] + ie_updates))\n",
    "    # # Amount above max\n",
    "    # ie_b = torch.relu((W_rec[ie_update_inds] + ie_updates) - weight_range[1][ie_update_inds])\n",
    "    # ie_over_weight = torch.mean(ie_a + ie_b)\n",
    "    # # Amount below min\n",
    "    # ei_a = torch.relu(weight_range[0][ei_update_inds] - (W_rec[ei_update_inds] + ei_updates))\n",
    "    # # Amount above max\n",
    "    # ei_b = torch.relu((W_rec[ei_update_inds] + ei_updates) - weight_range[1][ei_update_inds])\n",
    "    # ei_over_weight = torch.mean(ei_a + ei_b)\n",
    "\n",
    "    # print(f\"IE: {torch.mean(ie_updates)}: {ie_over_weight}\")\n",
    "    # print(f\"EI: {torch.mean(ei_updates)}: {ei_over_weight}\")\n",
    "    \n",
    "    # #W_rec = W_rec.clamp(min=weight_range[0], max=weight_range[1])\n",
    "    \n",
    "    W_rec = torch.index_put(W_rec, ie_update_inds, ie_bounded_updates, accumulate=True)\n",
    "    W_rec = torch.index_put(W_rec, ei_update_inds, ei_bounded_updates, accumulate=True)\n",
    "     \n",
    "    \n",
    "    R = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "    R_new = R\n",
    "    \n",
    "    if detach_grad:\n",
    "        # We want a new R response tensor which only has the previous weight update but not anything before that\n",
    "        W_rec = W_rec.detach()\n",
    "        R_new = compute_piriform_response(h_bar_ff, W_rec, threshold_mult)\n",
    "\n",
    "    if with_loss:\n",
    "        loss = loss_fn(R, W_rec, ie_update_inds, ei_update_inds, lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp, do_print=True)\n",
    "    else:\n",
    "        loss = 0\n",
    "    \n",
    "    return loss, W_rec, R_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22297cdeba801b81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275753Z",
     "start_time": "2024-08-01T17:46:24.267072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_update_inds(post, pre, W):\n",
    "    weights_slice = W[post[0]:post[1], pre[0]:pre[1]]\n",
    "    inds = torch.nonzero(weights_slice, as_tuple=True)\n",
    "    update_inds = (inds[0] + post[0], inds[1] + pre[0])\n",
    "    \n",
    "    return update_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c09c7202ebd3f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:25.275720Z",
     "start_time": "2024-08-01T17:46:24.263582Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def test_regime(ie_val, ei_val):\n",
    "    runs = 100\n",
    "    loss_ratios = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        with torch.no_grad():\n",
    "            I = correlated_mitral_activity()\n",
    "            hbar_ff = compute_feedforward_activity(I)\n",
    "            W = compute_initial_recurrent_weights()\n",
    "            R = compute_piriform_response(hbar_ff, W, 0)\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            initial_loss = loss_fn(R, W, ie_update_inds, ei_update_inds, 1, 0, 0, 0, 0, do_print=False)\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            W[ie_update_inds] = ie_val\n",
    "            W[ei_update_inds] = ei_val\n",
    "            R_0 = compute_piriform_response(hbar_ff, W, 0)\n",
    "            final_loss = loss_fn(R_0, W, ie_update_inds, ei_update_inds, 1, 0, 0, 0, 0, do_print=False)\n",
    "        \n",
    "        total_loss = final_loss / initial_loss\n",
    "        #print(f\"Loss Ratio: {total_loss.item()}\")\n",
    "        loss_ratios[i] = total_loss\n",
    "        \n",
    "    plt.hist(loss_ratios, bins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9cdbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both zero\n",
    "# test_regime(0., 0.)\n",
    "# plt.title(\"Loss Ratios - both 0\")\n",
    "# plt.show()\n",
    "# # IE cranked\n",
    "# test_regime(ie_max_weight, w_ei)\n",
    "# plt.title(\"Loss Ratios - E->I cranked\")\n",
    "# plt.show()\n",
    "# # EI cranked\n",
    "# test_regime(w_ie, ei_min_weight)\n",
    "# plt.title(\"Loss Ratios - I->E cranked\")\n",
    "# plt.show()\n",
    "# # Both cranked\n",
    "# test_regime(ie_max_weight, ei_min_weight)\n",
    "# plt.title(\"Loss Ratios - both cranked\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70723f2b6444f1b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:20:56.821190Z",
     "start_time": "2024-08-01T18:20:56.742882Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of independent \"sniffs\" of the 16 odors\n",
    "n_inner = 1\n",
    "# Number of different realizations of the odor sniffing process\n",
    "n_outer = 1000\n",
    "\n",
    "# TODO change back to -3 if too noisy\n",
    "plasticity_rate = 1e-2\n",
    "# Same plasticity rate for EI and IE\n",
    "plasticity_ie = plasticity_ei = plasticity_rate\n",
    "\n",
    "# TODO experiment w/ diff gradient tracking numbers\n",
    "# Number of inner epochs between model updates, n_update <= n_inner\n",
    "n_update = 1\n",
    "# Number of inner epochs across which the gradient is tracked (right now we detach the gradient after each inner epoch), n_track <= n_inner\n",
    "n_track = n_update\n",
    "\n",
    "# n_track = n_update to test formulation where we track the gradient across a subset of the inner epochs and update the model\n",
    "# If n_update > n_track, the model will have multiple \"gradient\" paths of loss accumulated: ex. inner epochs 1-5 and a separate branch of 6-10\n",
    "# If n_update < n_track, it doesn't matter b/c the model will truncate the history past its previous update since the gradient is zeroed\n",
    "\n",
    "# Number of standard deviations from mean, we are trying 0 b/c 1 and 2 is too sparse\n",
    "threshold_multiplier = 0\n",
    "\n",
    "# TODO for now go to simple formulation, no weight decay\n",
    "weight_decay = 0\n",
    "# How much to weight each of the regularization terms\n",
    "# Sparsity = 1000 doesn't improve loss ratios\n",
    "# Sparsity = 100, 500 epochs, loss ratios 1.0-1.8, blows up sparsity\n",
    "# Go back to 500 epochs no sparsity reg, w/ new nan clipping formulation\n",
    "lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp = 10, 1, 0, 0, 0\n",
    "\n",
    "ie_model = create_model()\n",
    "ie_model.to(gpu)\n",
    "ei_model = create_model()\n",
    "ei_model.to(gpu)\n",
    "\n",
    "\n",
    "mult = 100\n",
    "w_ie = 0.5\n",
    "ie_max_weight = mult * w_ie\n",
    "ie_min_weight = 0\n",
    "\n",
    "w_ei = -0.2\n",
    "ei_max_weight = 0\n",
    "ei_min_weight = mult * w_ei\n",
    "\n",
    "ie_post = (num_e, num_neurons)\n",
    "ie_pre = (0, num_e)\n",
    "\n",
    "ei_post = (0, num_e)\n",
    "ei_pre = (num_e, num_neurons)\n",
    "\n",
    "def train_model():\n",
    "    #torch.autograd.set_detect_anomaly(True)\n",
    "    \n",
    "    # TODO try larger learning rate\n",
    "    # TODO problem was adaptive threshold - as we tried to boost the weights, the threshold for the response would change too\n",
    "    # So it would negate the effect of reducing the response\n",
    "    ie_optim = optim.SGD(ie_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    ei_optim = optim.SGD(ei_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "    \n",
    "    updates_per_outer = n_inner // n_update\n",
    "    num_losses = int(updates_per_outer * n_outer)\n",
    "    losses = torch.empty(size=(num_losses,))\n",
    "    # batch every 20 (ex. tried 50 and was slightly more noisy (but w/ similar mean))\n",
    "    # batch_every = 20\n",
    "    # batch_loss = 0\n",
    "    \n",
    "    for outer_e in range(n_outer):\n",
    "        i = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(i)\n",
    "        \n",
    "        W_initial = compute_initial_recurrent_weights()\n",
    "        W = W_initial.clone().to(gpu)\n",
    "        W.requires_grad_(True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ie_update_inds = get_update_inds(ie_post, ie_pre, W)\n",
    "            ei_update_inds = get_update_inds(ei_post, ei_pre, W)\n",
    "            \n",
    "            clamp_min = torch.zeros_like(W)\n",
    "            clamp_min[ei_update_inds] = ei_min_weight\n",
    "            clamp_min[ie_update_inds] = ie_min_weight\n",
    "            clamp_max = torch.zeros_like(W)\n",
    "            clamp_max[ie_update_inds] = ie_max_weight\n",
    "            clamp_max[ei_update_inds] = ei_max_weight\n",
    "            weight_range = (clamp_min, clamp_max)\n",
    "        \n",
    "        # Initial neuron responses\n",
    "        R = compute_piriform_response(hbar_ff, W, threshold_multiplier)\n",
    "        \n",
    "        for i in range(1, n_inner + 1):\n",
    "            with_loss = False\n",
    "            detach_grad = False\n",
    "            if i % n_update == 0:\n",
    "                with_loss = True\n",
    "                print(f\"Outer epoch {outer_e}, Inner epoch {i}, Loss: \\t\", end=\"\")\n",
    "            if i % n_track == 0:\n",
    "                detach_grad = True\n",
    "            \n",
    "            loss, W, R = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W, R, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp, detach_grad=detach_grad, with_loss=with_loss)\n",
    "            if with_loss:\n",
    "                final_loss = loss\n",
    "\n",
    "                R_initial = compute_piriform_response(hbar_ff, W_initial, threshold_multiplier)\n",
    "                initial_loss = loss_fn(R_initial, W_initial, ie_update_inds, ei_update_inds, lambda_corr, lambda_w, lambda_mu, lambda_var, lambda_sp, do_print=False)\n",
    "                total_loss = final_loss / initial_loss\n",
    "                #total_loss += overload\n",
    "                #print(f\"Loss ratio: {total_loss}\")\n",
    "                losses[(outer_e * updates_per_outer)  + (i // n_update) - 1] = total_loss.item()\n",
    "                total_loss.backward()\n",
    "                #batch_loss += total_loss\n",
    "                # ie_grad = torch.nn.utils.clip_grad_norm_(ie_model.parameters(), max_norm = 1e5)\n",
    "                # ei_grad = torch.nn.utils.clip_grad_norm_(ei_model.parameters(), max_norm = 1e5)\n",
    "                # print(f\"ie model grad: {ie_grad}\")\n",
    "                # print(f\"ei model grad: {ei_grad}\")\n",
    "                ie_optim.step()  \n",
    "                ei_optim.step()\n",
    "                ie_optim.zero_grad()\n",
    "                ei_optim.zero_grad()\n",
    "            \n",
    "        # if outer_e % batch_every == 0:\n",
    "        #     batch_loss.backward()   \n",
    "        #     ie_optim.step()\n",
    "        #     ei_optim.step()\n",
    "        #     ie_optim.zero_grad()\n",
    "        #     ei_optim.zero_grad()\n",
    "        #     batch_loss = 0\n",
    "                \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e34a7ed21ff8c903",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T18:21:43.717186Z",
     "start_time": "2024-08-01T18:20:57.000500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer epoch 0, Inner epoch 1, Loss: \tAvg Corr: 0.2030, Corr: 0.6558, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 1, Inner epoch 1, Loss: \tAvg Corr: 0.1708, Corr: 0.5268, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 2, Inner epoch 1, Loss: \tAvg Corr: 0.1763, Corr: 0.6076, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 3, Inner epoch 1, Loss: \tAvg Corr: 0.2902, Corr: 1.2478, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 4, Inner epoch 1, Loss: \tAvg Corr: 0.2521, Corr: 0.9899, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 5, Inner epoch 1, Loss: \tAvg Corr: 0.2315, Corr: 0.9497, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 6, Inner epoch 1, Loss: \tAvg Corr: 0.2322, Corr: 0.8718, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 7, Inner epoch 1, Loss: \tAvg Corr: 0.1862, Corr: 0.5552, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 8, Inner epoch 1, Loss: \tAvg Corr: 0.2010, Corr: 0.7669, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 9, Inner epoch 1, Loss: \tAvg Corr: 0.2186, Corr: 0.7367, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 10, Inner epoch 1, Loss: \tAvg Corr: 0.2065, Corr: 0.6623, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 11, Inner epoch 1, Loss: \tAvg Corr: 0.2430, Corr: 0.9718, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 12, Inner epoch 1, Loss: \tAvg Corr: 0.2773, Corr: 1.1335, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 13, Inner epoch 1, Loss: \tAvg Corr: 0.1928, Corr: 0.6934, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 14, Inner epoch 1, Loss: \tAvg Corr: 0.2198, Corr: 0.9459, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 15, Inner epoch 1, Loss: \tAvg Corr: 0.2123, Corr: 0.7468, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 16, Inner epoch 1, Loss: \tAvg Corr: 0.1786, Corr: 0.5251, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 17, Inner epoch 1, Loss: \tAvg Corr: 0.2269, Corr: 0.7483, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 18, Inner epoch 1, Loss: \tAvg Corr: 0.1976, Corr: 0.6894, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 19, Inner epoch 1, Loss: \tAvg Corr: 0.2185, Corr: 0.6926, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 20, Inner epoch 1, Loss: \tAvg Corr: 0.2230, Corr: 0.7451, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 21, Inner epoch 1, Loss: \tAvg Corr: 0.2923, Corr: 1.3494, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 22, Inner epoch 1, Loss: \tAvg Corr: 0.2376, Corr: 0.8804, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 23, Inner epoch 1, Loss: \tAvg Corr: 0.2592, Corr: 1.0513, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 24, Inner epoch 1, Loss: \tAvg Corr: 0.2251, Corr: 0.7698, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 25, Inner epoch 1, Loss: \tAvg Corr: 0.2681, Corr: 1.1536, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 26, Inner epoch 1, Loss: \tAvg Corr: 0.2151, Corr: 0.7489, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 27, Inner epoch 1, Loss: \tAvg Corr: 0.2250, Corr: 0.8736, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 28, Inner epoch 1, Loss: \tAvg Corr: 0.1662, Corr: 0.4117, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 29, Inner epoch 1, Loss: \tAvg Corr: 0.2934, Corr: 1.2589, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 30, Inner epoch 1, Loss: \tAvg Corr: 0.2448, Corr: 1.0024, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 31, Inner epoch 1, Loss: \tAvg Corr: 0.1947, Corr: 0.6178, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 32, Inner epoch 1, Loss: \tAvg Corr: 0.2579, Corr: 0.9119, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 33, Inner epoch 1, Loss: \tAvg Corr: 0.1494, Corr: 0.3531, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 34, Inner epoch 1, Loss: \tAvg Corr: 0.2235, Corr: 0.8691, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 35, Inner epoch 1, Loss: \tAvg Corr: 0.1659, Corr: 0.4507, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 36, Inner epoch 1, Loss: \tAvg Corr: 0.2077, Corr: 0.7731, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 37, Inner epoch 1, Loss: \tAvg Corr: 0.2102, Corr: 0.6231, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 38, Inner epoch 1, Loss: \tAvg Corr: 0.2644, Corr: 1.1407, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 39, Inner epoch 1, Loss: \tAvg Corr: 0.2541, Corr: 1.1072, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 40, Inner epoch 1, Loss: \tAvg Corr: 0.1995, Corr: 0.6309, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 41, Inner epoch 1, Loss: \tAvg Corr: 0.2477, Corr: 0.8505, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 42, Inner epoch 1, Loss: \tAvg Corr: 0.2291, Corr: 0.8056, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 43, Inner epoch 1, Loss: \tAvg Corr: 0.2020, Corr: 0.6159, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 44, Inner epoch 1, Loss: \tAvg Corr: 0.1940, Corr: 0.6068, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 45, Inner epoch 1, Loss: \tAvg Corr: 0.1694, Corr: 0.5128, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 46, Inner epoch 1, Loss: \tAvg Corr: 0.2759, Corr: 1.1295, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 47, Inner epoch 1, Loss: \tAvg Corr: 0.2107, Corr: 0.7066, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 48, Inner epoch 1, Loss: \tAvg Corr: 0.2196, Corr: 0.7943, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 49, Inner epoch 1, Loss: \tAvg Corr: 0.2425, Corr: 1.0116, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 50, Inner epoch 1, Loss: \tAvg Corr: 0.2376, Corr: 0.9034, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 51, Inner epoch 1, Loss: \tAvg Corr: 0.2296, Corr: 0.8279, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 52, Inner epoch 1, Loss: \tAvg Corr: 0.2184, Corr: 0.7798, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 53, Inner epoch 1, Loss: \tAvg Corr: 0.2102, Corr: 0.6528, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 54, Inner epoch 1, Loss: \tAvg Corr: 0.2784, Corr: 1.2715, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 55, Inner epoch 1, Loss: \tAvg Corr: 0.2091, Corr: 0.7912, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 56, Inner epoch 1, Loss: \tAvg Corr: 0.2199, Corr: 0.7422, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 57, Inner epoch 1, Loss: \tAvg Corr: 0.2600, Corr: 1.1285, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 58, Inner epoch 1, Loss: \tAvg Corr: 0.2545, Corr: 1.0039, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 59, Inner epoch 1, Loss: \tAvg Corr: 0.2168, Corr: 0.6862, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 60, Inner epoch 1, Loss: \tAvg Corr: 0.2353, Corr: 0.8393, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 61, Inner epoch 1, Loss: \tAvg Corr: 0.2098, Corr: 0.6365, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 62, Inner epoch 1, Loss: \tAvg Corr: 0.2170, Corr: 0.7924, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 63, Inner epoch 1, Loss: \tAvg Corr: 0.2078, Corr: 0.8334, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 64, Inner epoch 1, Loss: \tAvg Corr: 0.4091, Corr: 2.3239, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 65, Inner epoch 1, Loss: \tAvg Corr: 0.2391, Corr: 0.8871, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 66, Inner epoch 1, Loss: \tAvg Corr: 0.2665, Corr: 1.1260, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 67, Inner epoch 1, Loss: \tAvg Corr: 0.2129, Corr: 0.6542, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 68, Inner epoch 1, Loss: \tAvg Corr: 0.2246, Corr: 0.7455, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 69, Inner epoch 1, Loss: \tAvg Corr: 0.2386, Corr: 0.9641, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 70, Inner epoch 1, Loss: \tAvg Corr: 0.2069, Corr: 0.6524, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 71, Inner epoch 1, Loss: \tAvg Corr: 0.1818, Corr: 0.5140, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 72, Inner epoch 1, Loss: \tAvg Corr: 0.2091, Corr: 0.6390, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 73, Inner epoch 1, Loss: \tAvg Corr: 0.2214, Corr: 0.8441, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 74, Inner epoch 1, Loss: \tAvg Corr: 0.1944, Corr: 0.5882, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 75, Inner epoch 1, Loss: \tAvg Corr: 0.2180, Corr: 0.7932, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 76, Inner epoch 1, Loss: \tAvg Corr: 0.2451, Corr: 0.9167, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 77, Inner epoch 1, Loss: \tAvg Corr: 0.1505, Corr: 0.3321, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 78, Inner epoch 1, Loss: \tAvg Corr: 0.1999, Corr: 0.6750, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 79, Inner epoch 1, Loss: \tAvg Corr: 0.2324, Corr: 0.7363, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 80, Inner epoch 1, Loss: \tAvg Corr: 0.2408, Corr: 1.0231, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 81, Inner epoch 1, Loss: \tAvg Corr: 0.2228, Corr: 0.7209, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 82, Inner epoch 1, Loss: \tAvg Corr: 0.2405, Corr: 0.9146, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 83, Inner epoch 1, Loss: \tAvg Corr: 0.2073, Corr: 0.8186, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 84, Inner epoch 1, Loss: \tAvg Corr: 0.2335, Corr: 0.8866, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 85, Inner epoch 1, Loss: \tAvg Corr: 0.1835, Corr: 0.5515, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 86, Inner epoch 1, Loss: \tAvg Corr: 0.1907, Corr: 0.5227, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 87, Inner epoch 1, Loss: \tAvg Corr: 0.1937, Corr: 0.5917, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 88, Inner epoch 1, Loss: \tAvg Corr: 0.1860, Corr: 0.6030, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 89, Inner epoch 1, Loss: \tAvg Corr: 0.1977, Corr: 0.7366, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 90, Inner epoch 1, Loss: \tAvg Corr: 0.2298, Corr: 0.8645, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 91, Inner epoch 1, Loss: \tAvg Corr: 0.2131, Corr: 0.6637, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 92, Inner epoch 1, Loss: \tAvg Corr: 0.1896, Corr: 0.6388, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 93, Inner epoch 1, Loss: \tAvg Corr: 0.2112, Corr: 0.6719, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 94, Inner epoch 1, Loss: \tAvg Corr: 0.2318, Corr: 0.8256, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 95, Inner epoch 1, Loss: \tAvg Corr: 0.1976, Corr: 0.7264, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 96, Inner epoch 1, Loss: \tAvg Corr: 0.2764, Corr: 1.2848, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 97, Inner epoch 1, Loss: \tAvg Corr: 0.2737, Corr: 1.2231, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 98, Inner epoch 1, Loss: \tAvg Corr: 0.2328, Corr: 0.8552, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 99, Inner epoch 1, Loss: \tAvg Corr: 0.2172, Corr: 0.7053, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 100, Inner epoch 1, Loss: \tAvg Corr: 0.2293, Corr: 0.8149, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 101, Inner epoch 1, Loss: \tAvg Corr: 0.1999, Corr: 0.6925, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 102, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.8382, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 103, Inner epoch 1, Loss: \tAvg Corr: 0.1450, Corr: 0.3921, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 104, Inner epoch 1, Loss: \tAvg Corr: 0.2964, Corr: 1.1615, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 105, Inner epoch 1, Loss: \tAvg Corr: 0.2224, Corr: 0.7154, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 106, Inner epoch 1, Loss: \tAvg Corr: 0.2223, Corr: 0.8247, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 107, Inner epoch 1, Loss: \tAvg Corr: 0.2212, Corr: 0.8595, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 108, Inner epoch 1, Loss: \tAvg Corr: 0.2852, Corr: 1.2539, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 109, Inner epoch 1, Loss: \tAvg Corr: 0.1975, Corr: 0.6610, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 110, Inner epoch 1, Loss: \tAvg Corr: 0.1670, Corr: 0.5418, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 111, Inner epoch 1, Loss: \tAvg Corr: 0.2574, Corr: 1.0543, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 112, Inner epoch 1, Loss: \tAvg Corr: 0.3119, Corr: 1.4103, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 113, Inner epoch 1, Loss: \tAvg Corr: 0.1888, Corr: 0.7118, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 114, Inner epoch 1, Loss: \tAvg Corr: 0.2416, Corr: 0.9878, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 115, Inner epoch 1, Loss: \tAvg Corr: 0.2723, Corr: 1.2933, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 116, Inner epoch 1, Loss: \tAvg Corr: 0.2151, Corr: 0.6896, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 117, Inner epoch 1, Loss: \tAvg Corr: 0.2005, Corr: 0.5924, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 118, Inner epoch 1, Loss: \tAvg Corr: 0.2438, Corr: 0.9023, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 119, Inner epoch 1, Loss: \tAvg Corr: 0.1798, Corr: 0.4662, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 120, Inner epoch 1, Loss: \tAvg Corr: 0.2219, Corr: 0.7057, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 121, Inner epoch 1, Loss: \tAvg Corr: 0.2112, Corr: 0.7921, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 122, Inner epoch 1, Loss: \tAvg Corr: 0.2829, Corr: 1.2755, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 123, Inner epoch 1, Loss: \tAvg Corr: 0.2431, Corr: 0.8442, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 124, Inner epoch 1, Loss: \tAvg Corr: 0.1896, Corr: 0.6109, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 125, Inner epoch 1, Loss: \tAvg Corr: 0.2495, Corr: 1.0475, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 126, Inner epoch 1, Loss: \tAvg Corr: 0.2786, Corr: 1.1470, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 127, Inner epoch 1, Loss: \tAvg Corr: 0.2547, Corr: 1.0400, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 128, Inner epoch 1, Loss: \tAvg Corr: 0.2500, Corr: 0.9446, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 129, Inner epoch 1, Loss: \tAvg Corr: 0.2346, Corr: 0.9369, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 130, Inner epoch 1, Loss: \tAvg Corr: 0.2319, Corr: 0.8018, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 131, Inner epoch 1, Loss: \tAvg Corr: 0.1906, Corr: 0.5522, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 132, Inner epoch 1, Loss: \tAvg Corr: 0.2128, Corr: 0.6955, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 133, Inner epoch 1, Loss: \tAvg Corr: 0.2500, Corr: 0.9989, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 134, Inner epoch 1, Loss: \tAvg Corr: 0.1720, Corr: 0.4780, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 135, Inner epoch 1, Loss: \tAvg Corr: 0.1725, Corr: 0.5085, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 136, Inner epoch 1, Loss: \tAvg Corr: 0.2169, Corr: 0.7082, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 137, Inner epoch 1, Loss: \tAvg Corr: 0.2529, Corr: 1.0487, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 138, Inner epoch 1, Loss: \tAvg Corr: 0.2372, Corr: 0.8608, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 139, Inner epoch 1, Loss: \tAvg Corr: 0.2369, Corr: 0.8472, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 140, Inner epoch 1, Loss: \tAvg Corr: 0.2644, Corr: 1.1155, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 141, Inner epoch 1, Loss: \tAvg Corr: 0.2467, Corr: 1.1166, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 142, Inner epoch 1, Loss: \tAvg Corr: 0.2428, Corr: 0.8897, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 143, Inner epoch 1, Loss: \tAvg Corr: 0.1936, Corr: 0.6144, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 144, Inner epoch 1, Loss: \tAvg Corr: 0.2367, Corr: 0.9811, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 145, Inner epoch 1, Loss: \tAvg Corr: 0.2615, Corr: 1.0501, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 146, Inner epoch 1, Loss: \tAvg Corr: 0.2172, Corr: 0.6677, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 147, Inner epoch 1, Loss: \tAvg Corr: 0.1687, Corr: 0.5506, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 148, Inner epoch 1, Loss: \tAvg Corr: 0.1930, Corr: 0.6514, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 149, Inner epoch 1, Loss: \tAvg Corr: 0.2467, Corr: 1.0741, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 150, Inner epoch 1, Loss: \tAvg Corr: 0.1974, Corr: 0.6415, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 151, Inner epoch 1, Loss: \tAvg Corr: 0.2459, Corr: 1.0113, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 152, Inner epoch 1, Loss: \tAvg Corr: 0.2623, Corr: 1.0208, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 153, Inner epoch 1, Loss: \tAvg Corr: 0.1529, Corr: 0.3905, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 154, Inner epoch 1, Loss: \tAvg Corr: 0.2306, Corr: 0.8479, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 155, Inner epoch 1, Loss: \tAvg Corr: 0.2244, Corr: 0.8766, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 156, Inner epoch 1, Loss: \tAvg Corr: 0.2331, Corr: 0.9469, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 157, Inner epoch 1, Loss: \tAvg Corr: 0.2160, Corr: 0.7856, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 158, Inner epoch 1, Loss: \tAvg Corr: 0.1936, Corr: 0.5346, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 159, Inner epoch 1, Loss: \tAvg Corr: 0.2524, Corr: 0.9881, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 160, Inner epoch 1, Loss: \tAvg Corr: 0.2707, Corr: 1.1230, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 161, Inner epoch 1, Loss: \tAvg Corr: 0.2261, Corr: 0.7674, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 162, Inner epoch 1, Loss: \tAvg Corr: 0.1852, Corr: 0.5918, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 163, Inner epoch 1, Loss: \tAvg Corr: 0.2768, Corr: 1.1030, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 164, Inner epoch 1, Loss: \tAvg Corr: 0.2313, Corr: 0.8479, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 165, Inner epoch 1, Loss: \tAvg Corr: 0.1608, Corr: 0.4251, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 166, Inner epoch 1, Loss: \tAvg Corr: 0.2188, Corr: 0.8005, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 167, Inner epoch 1, Loss: \tAvg Corr: 0.1811, Corr: 0.4976, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 168, Inner epoch 1, Loss: \tAvg Corr: 0.2083, Corr: 0.7110, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 169, Inner epoch 1, Loss: \tAvg Corr: 0.2368, Corr: 0.9532, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 170, Inner epoch 1, Loss: \tAvg Corr: 0.2146, Corr: 0.8448, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 171, Inner epoch 1, Loss: \tAvg Corr: 0.1810, Corr: 0.5410, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 172, Inner epoch 1, Loss: \tAvg Corr: 0.1953, Corr: 0.6555, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 173, Inner epoch 1, Loss: \tAvg Corr: 0.2093, Corr: 0.6536, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 174, Inner epoch 1, Loss: \tAvg Corr: 0.1560, Corr: 0.3988, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 175, Inner epoch 1, Loss: \tAvg Corr: 0.1974, Corr: 0.5756, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 176, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.9047, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 177, Inner epoch 1, Loss: \tAvg Corr: 0.2426, Corr: 0.9504, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 178, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.7204, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 179, Inner epoch 1, Loss: \tAvg Corr: 0.2369, Corr: 0.9622, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 180, Inner epoch 1, Loss: \tAvg Corr: 0.2049, Corr: 0.6022, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 181, Inner epoch 1, Loss: \tAvg Corr: 0.1992, Corr: 0.7744, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 182, Inner epoch 1, Loss: \tAvg Corr: 0.1963, Corr: 0.6065, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 183, Inner epoch 1, Loss: \tAvg Corr: 0.1804, Corr: 0.5432, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 184, Inner epoch 1, Loss: \tAvg Corr: 0.1742, Corr: 0.5153, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 185, Inner epoch 1, Loss: \tAvg Corr: 0.2309, Corr: 0.7918, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 186, Inner epoch 1, Loss: \tAvg Corr: 0.2249, Corr: 0.8475, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 187, Inner epoch 1, Loss: \tAvg Corr: 0.1597, Corr: 0.4142, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 188, Inner epoch 1, Loss: \tAvg Corr: 0.2466, Corr: 0.8759, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 189, Inner epoch 1, Loss: \tAvg Corr: 0.1665, Corr: 0.4887, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 190, Inner epoch 1, Loss: \tAvg Corr: 0.2192, Corr: 0.7130, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 191, Inner epoch 1, Loss: \tAvg Corr: 0.2269, Corr: 0.7856, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 192, Inner epoch 1, Loss: \tAvg Corr: 0.2364, Corr: 0.8552, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 193, Inner epoch 1, Loss: \tAvg Corr: 0.2017, Corr: 0.6087, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 194, Inner epoch 1, Loss: \tAvg Corr: 0.2113, Corr: 0.7041, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 195, Inner epoch 1, Loss: \tAvg Corr: 0.1566, Corr: 0.4565, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 196, Inner epoch 1, Loss: \tAvg Corr: 0.2457, Corr: 0.9870, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 197, Inner epoch 1, Loss: \tAvg Corr: 0.1947, Corr: 0.5603, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 198, Inner epoch 1, Loss: \tAvg Corr: 0.1828, Corr: 0.5347, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 199, Inner epoch 1, Loss: \tAvg Corr: 0.2374, Corr: 0.9450, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 200, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.7423, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 201, Inner epoch 1, Loss: \tAvg Corr: 0.2152, Corr: 0.7781, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 202, Inner epoch 1, Loss: \tAvg Corr: 0.1713, Corr: 0.4991, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 203, Inner epoch 1, Loss: \tAvg Corr: 0.2316, Corr: 0.7752, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 204, Inner epoch 1, Loss: \tAvg Corr: 0.1901, Corr: 0.6143, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 205, Inner epoch 1, Loss: \tAvg Corr: 0.2113, Corr: 0.6798, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 206, Inner epoch 1, Loss: \tAvg Corr: 0.2162, Corr: 0.9102, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 207, Inner epoch 1, Loss: \tAvg Corr: 0.1969, Corr: 0.7368, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 208, Inner epoch 1, Loss: \tAvg Corr: 0.2366, Corr: 0.7942, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 209, Inner epoch 1, Loss: \tAvg Corr: 0.1879, Corr: 0.6292, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 210, Inner epoch 1, Loss: \tAvg Corr: 0.1650, Corr: 0.4402, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 211, Inner epoch 1, Loss: \tAvg Corr: 0.2871, Corr: 1.3615, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 212, Inner epoch 1, Loss: \tAvg Corr: 0.2128, Corr: 0.6366, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 213, Inner epoch 1, Loss: \tAvg Corr: 0.1994, Corr: 0.8065, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 214, Inner epoch 1, Loss: \tAvg Corr: 0.1788, Corr: 0.5295, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 215, Inner epoch 1, Loss: \tAvg Corr: 0.2182, Corr: 0.6831, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 216, Inner epoch 1, Loss: \tAvg Corr: 0.2120, Corr: 0.9178, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 217, Inner epoch 1, Loss: \tAvg Corr: 0.2182, Corr: 0.7512, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 218, Inner epoch 1, Loss: \tAvg Corr: 0.2881, Corr: 1.1591, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 219, Inner epoch 1, Loss: \tAvg Corr: 0.2774, Corr: 1.2482, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 220, Inner epoch 1, Loss: \tAvg Corr: 0.2545, Corr: 1.0663, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 221, Inner epoch 1, Loss: \tAvg Corr: 0.1837, Corr: 0.5979, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 222, Inner epoch 1, Loss: \tAvg Corr: 0.2849, Corr: 1.2386, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 223, Inner epoch 1, Loss: \tAvg Corr: 0.2031, Corr: 0.7044, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 224, Inner epoch 1, Loss: \tAvg Corr: 0.1753, Corr: 0.5748, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 225, Inner epoch 1, Loss: \tAvg Corr: 0.2431, Corr: 0.9407, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 226, Inner epoch 1, Loss: \tAvg Corr: 0.1313, Corr: 0.2999, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 227, Inner epoch 1, Loss: \tAvg Corr: 0.1836, Corr: 0.5065, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 228, Inner epoch 1, Loss: \tAvg Corr: 0.2168, Corr: 0.7194, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 229, Inner epoch 1, Loss: \tAvg Corr: 0.2420, Corr: 0.9601, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 230, Inner epoch 1, Loss: \tAvg Corr: 0.1723, Corr: 0.4852, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 231, Inner epoch 1, Loss: \tAvg Corr: 0.1779, Corr: 0.5917, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 232, Inner epoch 1, Loss: \tAvg Corr: 0.2036, Corr: 0.6138, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 233, Inner epoch 1, Loss: \tAvg Corr: 0.2178, Corr: 0.7532, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 234, Inner epoch 1, Loss: \tAvg Corr: 0.2450, Corr: 0.9926, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 235, Inner epoch 1, Loss: \tAvg Corr: 0.1785, Corr: 0.4825, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 236, Inner epoch 1, Loss: \tAvg Corr: 0.3090, Corr: 1.3761, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 237, Inner epoch 1, Loss: \tAvg Corr: 0.3071, Corr: 1.4175, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 238, Inner epoch 1, Loss: \tAvg Corr: 0.2361, Corr: 0.9210, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 239, Inner epoch 1, Loss: \tAvg Corr: 0.2308, Corr: 0.7773, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 240, Inner epoch 1, Loss: \tAvg Corr: 0.2509, Corr: 0.9068, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 241, Inner epoch 1, Loss: \tAvg Corr: 0.1815, Corr: 0.6666, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 242, Inner epoch 1, Loss: \tAvg Corr: 0.2580, Corr: 1.1167, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 243, Inner epoch 1, Loss: \tAvg Corr: 0.2755, Corr: 1.0789, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 244, Inner epoch 1, Loss: \tAvg Corr: 0.2229, Corr: 0.7522, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 245, Inner epoch 1, Loss: \tAvg Corr: 0.1628, Corr: 0.5068, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 246, Inner epoch 1, Loss: \tAvg Corr: 0.2625, Corr: 1.1036, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 247, Inner epoch 1, Loss: \tAvg Corr: 0.2431, Corr: 0.9957, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 248, Inner epoch 1, Loss: \tAvg Corr: 0.2357, Corr: 0.9570, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 249, Inner epoch 1, Loss: \tAvg Corr: 0.2146, Corr: 0.6822, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 250, Inner epoch 1, Loss: \tAvg Corr: 0.2269, Corr: 0.7691, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 251, Inner epoch 1, Loss: \tAvg Corr: 0.2478, Corr: 1.0092, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 252, Inner epoch 1, Loss: \tAvg Corr: 0.1585, Corr: 0.4218, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 253, Inner epoch 1, Loss: \tAvg Corr: 0.2411, Corr: 0.8466, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 254, Inner epoch 1, Loss: \tAvg Corr: 0.1867, Corr: 0.6048, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 255, Inner epoch 1, Loss: \tAvg Corr: 0.2112, Corr: 0.7975, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 256, Inner epoch 1, Loss: \tAvg Corr: 0.2338, Corr: 0.8984, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 257, Inner epoch 1, Loss: \tAvg Corr: 0.1999, Corr: 0.9050, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 258, Inner epoch 1, Loss: \tAvg Corr: 0.3158, Corr: 1.5097, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 259, Inner epoch 1, Loss: \tAvg Corr: 0.1508, Corr: 0.4011, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 260, Inner epoch 1, Loss: \tAvg Corr: 0.2606, Corr: 1.0303, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 261, Inner epoch 1, Loss: \tAvg Corr: 0.1975, Corr: 0.6571, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 262, Inner epoch 1, Loss: \tAvg Corr: 0.1653, Corr: 0.4171, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 263, Inner epoch 1, Loss: \tAvg Corr: 0.2213, Corr: 0.7563, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 264, Inner epoch 1, Loss: \tAvg Corr: 0.2341, Corr: 0.7744, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 265, Inner epoch 1, Loss: \tAvg Corr: 0.1968, Corr: 0.7137, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 266, Inner epoch 1, Loss: \tAvg Corr: 0.1987, Corr: 0.6525, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 267, Inner epoch 1, Loss: \tAvg Corr: 0.2630, Corr: 1.0070, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 268, Inner epoch 1, Loss: \tAvg Corr: 0.2421, Corr: 0.9269, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 269, Inner epoch 1, Loss: \tAvg Corr: 0.1967, Corr: 0.7468, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 270, Inner epoch 1, Loss: \tAvg Corr: 0.1683, Corr: 0.5433, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 271, Inner epoch 1, Loss: \tAvg Corr: 0.2415, Corr: 0.8484, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 272, Inner epoch 1, Loss: \tAvg Corr: 0.1985, Corr: 0.5322, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 273, Inner epoch 1, Loss: \tAvg Corr: 0.1743, Corr: 0.5257, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 274, Inner epoch 1, Loss: \tAvg Corr: 0.2304, Corr: 0.9878, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 275, Inner epoch 1, Loss: \tAvg Corr: 0.2550, Corr: 0.9965, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 276, Inner epoch 1, Loss: \tAvg Corr: 0.1990, Corr: 0.7138, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 277, Inner epoch 1, Loss: \tAvg Corr: 0.2161, Corr: 0.8665, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 278, Inner epoch 1, Loss: \tAvg Corr: 0.1927, Corr: 0.5671, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 279, Inner epoch 1, Loss: \tAvg Corr: 0.2009, Corr: 0.6316, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 280, Inner epoch 1, Loss: \tAvg Corr: 0.2195, Corr: 0.7064, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 281, Inner epoch 1, Loss: \tAvg Corr: 0.2992, Corr: 1.3969, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 282, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.6734, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 283, Inner epoch 1, Loss: \tAvg Corr: 0.2196, Corr: 0.7296, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 284, Inner epoch 1, Loss: \tAvg Corr: 0.1872, Corr: 0.5156, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 285, Inner epoch 1, Loss: \tAvg Corr: 0.2065, Corr: 0.7050, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 286, Inner epoch 1, Loss: \tAvg Corr: 0.2083, Corr: 0.7764, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 287, Inner epoch 1, Loss: \tAvg Corr: 0.2740, Corr: 1.0939, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 288, Inner epoch 1, Loss: \tAvg Corr: 0.2282, Corr: 1.0491, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 289, Inner epoch 1, Loss: \tAvg Corr: 0.1805, Corr: 0.5529, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 290, Inner epoch 1, Loss: \tAvg Corr: 0.2306, Corr: 0.8001, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 291, Inner epoch 1, Loss: \tAvg Corr: 0.1976, Corr: 0.6072, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 292, Inner epoch 1, Loss: \tAvg Corr: 0.2258, Corr: 0.8108, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 293, Inner epoch 1, Loss: \tAvg Corr: 0.2203, Corr: 0.8132, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 294, Inner epoch 1, Loss: \tAvg Corr: 0.2147, Corr: 0.8971, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 295, Inner epoch 1, Loss: \tAvg Corr: 0.2432, Corr: 0.9701, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 296, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.7288, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 297, Inner epoch 1, Loss: \tAvg Corr: 0.2039, Corr: 0.6528, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 298, Inner epoch 1, Loss: \tAvg Corr: 0.2306, Corr: 0.8438, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 299, Inner epoch 1, Loss: \tAvg Corr: 0.2352, Corr: 0.9208, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 300, Inner epoch 1, Loss: \tAvg Corr: 0.2292, Corr: 0.8975, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 301, Inner epoch 1, Loss: \tAvg Corr: 0.2250, Corr: 0.7916, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 302, Inner epoch 1, Loss: \tAvg Corr: 0.2676, Corr: 1.0647, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 303, Inner epoch 1, Loss: \tAvg Corr: 0.2078, Corr: 0.7106, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 304, Inner epoch 1, Loss: \tAvg Corr: 0.2348, Corr: 0.9402, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 305, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.8850, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 306, Inner epoch 1, Loss: \tAvg Corr: 0.1906, Corr: 0.5826, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 307, Inner epoch 1, Loss: \tAvg Corr: 0.2187, Corr: 0.8226, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 308, Inner epoch 1, Loss: \tAvg Corr: 0.2288, Corr: 0.8128, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 309, Inner epoch 1, Loss: \tAvg Corr: 0.2929, Corr: 1.2780, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 310, Inner epoch 1, Loss: \tAvg Corr: 0.2106, Corr: 0.8260, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 311, Inner epoch 1, Loss: \tAvg Corr: 0.2280, Corr: 0.7489, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 312, Inner epoch 1, Loss: \tAvg Corr: 0.2766, Corr: 1.3145, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 313, Inner epoch 1, Loss: \tAvg Corr: 0.2341, Corr: 0.8389, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 314, Inner epoch 1, Loss: \tAvg Corr: 0.2258, Corr: 0.8246, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 315, Inner epoch 1, Loss: \tAvg Corr: 0.2148, Corr: 0.7640, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 316, Inner epoch 1, Loss: \tAvg Corr: 0.2469, Corr: 1.0127, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 317, Inner epoch 1, Loss: \tAvg Corr: 0.2562, Corr: 1.0282, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 318, Inner epoch 1, Loss: \tAvg Corr: 0.2753, Corr: 1.1559, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 319, Inner epoch 1, Loss: \tAvg Corr: 0.1694, Corr: 0.4410, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 320, Inner epoch 1, Loss: \tAvg Corr: 0.2156, Corr: 0.6984, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 321, Inner epoch 1, Loss: \tAvg Corr: 0.2416, Corr: 0.8190, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 322, Inner epoch 1, Loss: \tAvg Corr: 0.1767, Corr: 0.5290, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 323, Inner epoch 1, Loss: \tAvg Corr: 0.2329, Corr: 0.7914, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 324, Inner epoch 1, Loss: \tAvg Corr: 0.2329, Corr: 0.9347, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 325, Inner epoch 1, Loss: \tAvg Corr: 0.2007, Corr: 0.5699, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 326, Inner epoch 1, Loss: \tAvg Corr: 0.2149, Corr: 0.8486, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 327, Inner epoch 1, Loss: \tAvg Corr: 0.2279, Corr: 0.8577, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 328, Inner epoch 1, Loss: \tAvg Corr: 0.1955, Corr: 0.6084, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 329, Inner epoch 1, Loss: \tAvg Corr: 0.2506, Corr: 1.1422, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 330, Inner epoch 1, Loss: \tAvg Corr: 0.1438, Corr: 0.3422, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 331, Inner epoch 1, Loss: \tAvg Corr: 0.2908, Corr: 1.3758, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 332, Inner epoch 1, Loss: \tAvg Corr: 0.2821, Corr: 1.3297, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 333, Inner epoch 1, Loss: \tAvg Corr: 0.2305, Corr: 0.9028, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 334, Inner epoch 1, Loss: \tAvg Corr: 0.1911, Corr: 0.5135, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 335, Inner epoch 1, Loss: \tAvg Corr: 0.2351, Corr: 0.8041, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 336, Inner epoch 1, Loss: \tAvg Corr: 0.2271, Corr: 0.8885, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 337, Inner epoch 1, Loss: \tAvg Corr: 0.2373, Corr: 0.8352, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 338, Inner epoch 1, Loss: \tAvg Corr: 0.2442, Corr: 0.9322, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 339, Inner epoch 1, Loss: \tAvg Corr: 0.3162, Corr: 1.3116, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 340, Inner epoch 1, Loss: \tAvg Corr: 0.2338, Corr: 0.8235, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 341, Inner epoch 1, Loss: \tAvg Corr: 0.2311, Corr: 0.7656, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 342, Inner epoch 1, Loss: \tAvg Corr: 0.1961, Corr: 0.7142, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 343, Inner epoch 1, Loss: \tAvg Corr: 0.2356, Corr: 0.9027, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 344, Inner epoch 1, Loss: \tAvg Corr: 0.2092, Corr: 0.6864, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 345, Inner epoch 1, Loss: \tAvg Corr: 0.1511, Corr: 0.4139, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 346, Inner epoch 1, Loss: \tAvg Corr: 0.2206, Corr: 0.7476, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 347, Inner epoch 1, Loss: \tAvg Corr: 0.1906, Corr: 0.6311, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 348, Inner epoch 1, Loss: \tAvg Corr: 0.1714, Corr: 0.5045, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 349, Inner epoch 1, Loss: \tAvg Corr: 0.2146, Corr: 0.7566, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 350, Inner epoch 1, Loss: \tAvg Corr: 0.2258, Corr: 0.8371, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 351, Inner epoch 1, Loss: \tAvg Corr: 0.2271, Corr: 0.8276, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 352, Inner epoch 1, Loss: \tAvg Corr: 0.2457, Corr: 0.8980, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 353, Inner epoch 1, Loss: \tAvg Corr: 0.1796, Corr: 0.4417, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 354, Inner epoch 1, Loss: \tAvg Corr: 0.2908, Corr: 1.3041, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 355, Inner epoch 1, Loss: \tAvg Corr: 0.2133, Corr: 0.7434, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 356, Inner epoch 1, Loss: \tAvg Corr: 0.2282, Corr: 0.7928, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 357, Inner epoch 1, Loss: \tAvg Corr: 0.2453, Corr: 0.9461, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 358, Inner epoch 1, Loss: \tAvg Corr: 0.1927, Corr: 0.5894, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 359, Inner epoch 1, Loss: \tAvg Corr: 0.2100, Corr: 0.8366, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 360, Inner epoch 1, Loss: \tAvg Corr: 0.2068, Corr: 0.6902, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 361, Inner epoch 1, Loss: \tAvg Corr: 0.1569, Corr: 0.4770, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 362, Inner epoch 1, Loss: \tAvg Corr: 0.3267, Corr: 1.5001, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 363, Inner epoch 1, Loss: \tAvg Corr: 0.1896, Corr: 0.6260, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 364, Inner epoch 1, Loss: \tAvg Corr: 0.2467, Corr: 0.8934, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 365, Inner epoch 1, Loss: \tAvg Corr: 0.2402, Corr: 1.0200, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 366, Inner epoch 1, Loss: \tAvg Corr: 0.2246, Corr: 0.8068, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 367, Inner epoch 1, Loss: \tAvg Corr: 0.1727, Corr: 0.4908, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 368, Inner epoch 1, Loss: \tAvg Corr: 0.3554, Corr: 1.6505, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 369, Inner epoch 1, Loss: \tAvg Corr: 0.2605, Corr: 1.0654, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 370, Inner epoch 1, Loss: \tAvg Corr: 0.2227, Corr: 0.7705, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 371, Inner epoch 1, Loss: \tAvg Corr: 0.2047, Corr: 0.6494, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 372, Inner epoch 1, Loss: \tAvg Corr: 0.2740, Corr: 1.1740, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 373, Inner epoch 1, Loss: \tAvg Corr: 0.1895, Corr: 0.6389, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 374, Inner epoch 1, Loss: \tAvg Corr: 0.2394, Corr: 0.9029, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 375, Inner epoch 1, Loss: \tAvg Corr: 0.2295, Corr: 0.9069, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 376, Inner epoch 1, Loss: \tAvg Corr: 0.1830, Corr: 0.5563, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 377, Inner epoch 1, Loss: \tAvg Corr: 0.1925, Corr: 0.6326, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 378, Inner epoch 1, Loss: \tAvg Corr: 0.1989, Corr: 0.7077, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 379, Inner epoch 1, Loss: \tAvg Corr: 0.2176, Corr: 0.7332, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 380, Inner epoch 1, Loss: \tAvg Corr: 0.2261, Corr: 0.7686, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 381, Inner epoch 1, Loss: \tAvg Corr: 0.1764, Corr: 0.4309, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 382, Inner epoch 1, Loss: \tAvg Corr: 0.2145, Corr: 0.8696, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 383, Inner epoch 1, Loss: \tAvg Corr: 0.2589, Corr: 1.0682, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 384, Inner epoch 1, Loss: \tAvg Corr: 0.1910, Corr: 0.7556, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 385, Inner epoch 1, Loss: \tAvg Corr: 0.2283, Corr: 0.7943, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 386, Inner epoch 1, Loss: \tAvg Corr: 0.2471, Corr: 0.9963, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 387, Inner epoch 1, Loss: \tAvg Corr: 0.2235, Corr: 0.8553, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 388, Inner epoch 1, Loss: \tAvg Corr: 0.2906, Corr: 1.3600, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 389, Inner epoch 1, Loss: \tAvg Corr: 0.2020, Corr: 0.6419, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 390, Inner epoch 1, Loss: \tAvg Corr: 0.2432, Corr: 0.8507, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 391, Inner epoch 1, Loss: \tAvg Corr: 0.1611, Corr: 0.3680, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 392, Inner epoch 1, Loss: \tAvg Corr: 0.1953, Corr: 0.7013, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 393, Inner epoch 1, Loss: \tAvg Corr: 0.2800, Corr: 1.1569, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 394, Inner epoch 1, Loss: \tAvg Corr: 0.2114, Corr: 0.6618, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 395, Inner epoch 1, Loss: \tAvg Corr: 0.1943, Corr: 0.6573, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 396, Inner epoch 1, Loss: \tAvg Corr: 0.1906, Corr: 0.6585, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 397, Inner epoch 1, Loss: \tAvg Corr: 0.1884, Corr: 0.5865, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 398, Inner epoch 1, Loss: \tAvg Corr: 0.2243, Corr: 0.8173, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 399, Inner epoch 1, Loss: \tAvg Corr: 0.1824, Corr: 0.5626, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 400, Inner epoch 1, Loss: \tAvg Corr: 0.1827, Corr: 0.6938, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 401, Inner epoch 1, Loss: \tAvg Corr: 0.2101, Corr: 0.8829, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 402, Inner epoch 1, Loss: \tAvg Corr: 0.2159, Corr: 0.7724, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 403, Inner epoch 1, Loss: \tAvg Corr: 0.2172, Corr: 0.8273, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 404, Inner epoch 1, Loss: \tAvg Corr: 0.1847, Corr: 0.5612, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 405, Inner epoch 1, Loss: \tAvg Corr: 0.2353, Corr: 0.9074, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 406, Inner epoch 1, Loss: \tAvg Corr: 0.2364, Corr: 0.9158, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 407, Inner epoch 1, Loss: \tAvg Corr: 0.2528, Corr: 1.0948, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 408, Inner epoch 1, Loss: \tAvg Corr: 0.1904, Corr: 0.5364, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 409, Inner epoch 1, Loss: \tAvg Corr: 0.1797, Corr: 0.5286, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 410, Inner epoch 1, Loss: \tAvg Corr: 0.2081, Corr: 0.6169, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 411, Inner epoch 1, Loss: \tAvg Corr: 0.1966, Corr: 0.5355, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 412, Inner epoch 1, Loss: \tAvg Corr: 0.2164, Corr: 0.7486, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 413, Inner epoch 1, Loss: \tAvg Corr: 0.2649, Corr: 1.1731, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 414, Inner epoch 1, Loss: \tAvg Corr: 0.1851, Corr: 0.6149, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 415, Inner epoch 1, Loss: \tAvg Corr: 0.1589, Corr: 0.4427, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 416, Inner epoch 1, Loss: \tAvg Corr: 0.2305, Corr: 0.7957, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 417, Inner epoch 1, Loss: \tAvg Corr: 0.2834, Corr: 1.2050, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 418, Inner epoch 1, Loss: \tAvg Corr: 0.1819, Corr: 0.5472, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 419, Inner epoch 1, Loss: \tAvg Corr: 0.2441, Corr: 0.9313, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 420, Inner epoch 1, Loss: \tAvg Corr: 0.2574, Corr: 1.1402, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 421, Inner epoch 1, Loss: \tAvg Corr: 0.2070, Corr: 0.7154, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 422, Inner epoch 1, Loss: \tAvg Corr: 0.2491, Corr: 1.0597, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 423, Inner epoch 1, Loss: \tAvg Corr: 0.2455, Corr: 0.8107, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 424, Inner epoch 1, Loss: \tAvg Corr: 0.2532, Corr: 0.9243, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 425, Inner epoch 1, Loss: \tAvg Corr: 0.2119, Corr: 0.7622, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 426, Inner epoch 1, Loss: \tAvg Corr: 0.1873, Corr: 0.5215, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 427, Inner epoch 1, Loss: \tAvg Corr: 0.2315, Corr: 0.9657, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 428, Inner epoch 1, Loss: \tAvg Corr: 0.2143, Corr: 0.7076, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 429, Inner epoch 1, Loss: \tAvg Corr: 0.2333, Corr: 0.7708, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 430, Inner epoch 1, Loss: \tAvg Corr: 0.1907, Corr: 0.5341, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 431, Inner epoch 1, Loss: \tAvg Corr: 0.2081, Corr: 0.6303, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 432, Inner epoch 1, Loss: \tAvg Corr: 0.2088, Corr: 0.6226, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 433, Inner epoch 1, Loss: \tAvg Corr: 0.2450, Corr: 0.9550, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 434, Inner epoch 1, Loss: \tAvg Corr: 0.2038, Corr: 0.7877, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 435, Inner epoch 1, Loss: \tAvg Corr: 0.1698, Corr: 0.5119, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 436, Inner epoch 1, Loss: \tAvg Corr: 0.2210, Corr: 0.7124, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 437, Inner epoch 1, Loss: \tAvg Corr: 0.2455, Corr: 0.9713, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 438, Inner epoch 1, Loss: \tAvg Corr: 0.2624, Corr: 1.0902, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 439, Inner epoch 1, Loss: \tAvg Corr: 0.2103, Corr: 0.7933, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 440, Inner epoch 1, Loss: \tAvg Corr: 0.2098, Corr: 0.7833, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 441, Inner epoch 1, Loss: \tAvg Corr: 0.2350, Corr: 0.8491, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 442, Inner epoch 1, Loss: \tAvg Corr: 0.1883, Corr: 0.5266, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 443, Inner epoch 1, Loss: \tAvg Corr: 0.1781, Corr: 0.4884, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 444, Inner epoch 1, Loss: \tAvg Corr: 0.3118, Corr: 1.4776, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 445, Inner epoch 1, Loss: \tAvg Corr: 0.2540, Corr: 1.0595, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 446, Inner epoch 1, Loss: \tAvg Corr: 0.1945, Corr: 0.6231, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 447, Inner epoch 1, Loss: \tAvg Corr: 0.1761, Corr: 0.6373, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 448, Inner epoch 1, Loss: \tAvg Corr: 0.1800, Corr: 0.5228, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 449, Inner epoch 1, Loss: \tAvg Corr: 0.2500, Corr: 0.9401, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 450, Inner epoch 1, Loss: \tAvg Corr: 0.2797, Corr: 1.2522, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 451, Inner epoch 1, Loss: \tAvg Corr: 0.1578, Corr: 0.4574, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 452, Inner epoch 1, Loss: \tAvg Corr: 0.2146, Corr: 0.6924, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 453, Inner epoch 1, Loss: \tAvg Corr: 0.2336, Corr: 0.9172, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 454, Inner epoch 1, Loss: \tAvg Corr: 0.2092, Corr: 0.7176, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 455, Inner epoch 1, Loss: \tAvg Corr: 0.2126, Corr: 0.7941, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 456, Inner epoch 1, Loss: \tAvg Corr: 0.1869, Corr: 0.6342, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 457, Inner epoch 1, Loss: \tAvg Corr: 0.2404, Corr: 0.9644, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 458, Inner epoch 1, Loss: \tAvg Corr: 0.2060, Corr: 0.6991, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 459, Inner epoch 1, Loss: \tAvg Corr: 0.1607, Corr: 0.4419, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 460, Inner epoch 1, Loss: \tAvg Corr: 0.2515, Corr: 0.9815, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 461, Inner epoch 1, Loss: \tAvg Corr: 0.2365, Corr: 0.9249, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 462, Inner epoch 1, Loss: \tAvg Corr: 0.2286, Corr: 0.8279, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 463, Inner epoch 1, Loss: \tAvg Corr: 0.2103, Corr: 0.7715, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 464, Inner epoch 1, Loss: \tAvg Corr: 0.2054, Corr: 0.6743, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 465, Inner epoch 1, Loss: \tAvg Corr: 0.1907, Corr: 0.5850, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 466, Inner epoch 1, Loss: \tAvg Corr: 0.2295, Corr: 0.8586, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 467, Inner epoch 1, Loss: \tAvg Corr: 0.2319, Corr: 0.8598, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 468, Inner epoch 1, Loss: \tAvg Corr: 0.1951, Corr: 0.5675, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 469, Inner epoch 1, Loss: \tAvg Corr: 0.1793, Corr: 0.6987, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 470, Inner epoch 1, Loss: \tAvg Corr: 0.2955, Corr: 1.2993, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 471, Inner epoch 1, Loss: \tAvg Corr: 0.2532, Corr: 0.9474, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 472, Inner epoch 1, Loss: \tAvg Corr: 0.2280, Corr: 0.8321, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 473, Inner epoch 1, Loss: \tAvg Corr: 0.2854, Corr: 1.1316, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 474, Inner epoch 1, Loss: \tAvg Corr: 0.2187, Corr: 0.8463, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 475, Inner epoch 1, Loss: \tAvg Corr: 0.2936, Corr: 1.3288, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 476, Inner epoch 1, Loss: \tAvg Corr: 0.1705, Corr: 0.5338, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 477, Inner epoch 1, Loss: \tAvg Corr: 0.2043, Corr: 0.6006, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 478, Inner epoch 1, Loss: \tAvg Corr: 0.1785, Corr: 0.5592, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 479, Inner epoch 1, Loss: \tAvg Corr: 0.1998, Corr: 0.6257, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 480, Inner epoch 1, Loss: \tAvg Corr: 0.2453, Corr: 0.8585, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 481, Inner epoch 1, Loss: \tAvg Corr: 0.1798, Corr: 0.4876, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 482, Inner epoch 1, Loss: \tAvg Corr: 0.1769, Corr: 0.4720, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 483, Inner epoch 1, Loss: \tAvg Corr: 0.2453, Corr: 0.9090, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 484, Inner epoch 1, Loss: \tAvg Corr: 0.2118, Corr: 0.8316, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 485, Inner epoch 1, Loss: \tAvg Corr: 0.1884, Corr: 0.5066, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 486, Inner epoch 1, Loss: \tAvg Corr: 0.1730, Corr: 0.5224, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 487, Inner epoch 1, Loss: \tAvg Corr: 0.1777, Corr: 0.5527, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 488, Inner epoch 1, Loss: \tAvg Corr: 0.1853, Corr: 0.6464, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 489, Inner epoch 1, Loss: \tAvg Corr: 0.1503, Corr: 0.4878, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 490, Inner epoch 1, Loss: \tAvg Corr: 0.2725, Corr: 1.1122, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 491, Inner epoch 1, Loss: \tAvg Corr: 0.2103, Corr: 0.6104, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 492, Inner epoch 1, Loss: \tAvg Corr: 0.2361, Corr: 0.8461, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 493, Inner epoch 1, Loss: \tAvg Corr: 0.2746, Corr: 1.0373, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 494, Inner epoch 1, Loss: \tAvg Corr: 0.2481, Corr: 1.0172, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 495, Inner epoch 1, Loss: \tAvg Corr: 0.2129, Corr: 0.8538, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 496, Inner epoch 1, Loss: \tAvg Corr: 0.2065, Corr: 0.7605, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 497, Inner epoch 1, Loss: \tAvg Corr: 0.2512, Corr: 0.9812, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 498, Inner epoch 1, Loss: \tAvg Corr: 0.2359, Corr: 0.9444, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 499, Inner epoch 1, Loss: \tAvg Corr: 0.1608, Corr: 0.4879, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 500, Inner epoch 1, Loss: \tAvg Corr: 0.2583, Corr: 0.9748, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 501, Inner epoch 1, Loss: \tAvg Corr: 0.1981, Corr: 0.6669, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 502, Inner epoch 1, Loss: \tAvg Corr: 0.2130, Corr: 0.8566, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 503, Inner epoch 1, Loss: \tAvg Corr: 0.1951, Corr: 0.7650, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 504, Inner epoch 1, Loss: \tAvg Corr: 0.1735, Corr: 0.5183, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 505, Inner epoch 1, Loss: \tAvg Corr: 0.2395, Corr: 0.9691, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 506, Inner epoch 1, Loss: \tAvg Corr: 0.1796, Corr: 0.5014, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 507, Inner epoch 1, Loss: \tAvg Corr: 0.1887, Corr: 0.6028, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 508, Inner epoch 1, Loss: \tAvg Corr: 0.1667, Corr: 0.4479, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 509, Inner epoch 1, Loss: \tAvg Corr: 0.2801, Corr: 1.1888, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 510, Inner epoch 1, Loss: \tAvg Corr: 0.2289, Corr: 0.8721, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 511, Inner epoch 1, Loss: \tAvg Corr: 0.2147, Corr: 0.7254, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 512, Inner epoch 1, Loss: \tAvg Corr: 0.1809, Corr: 0.5760, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 513, Inner epoch 1, Loss: \tAvg Corr: 0.1848, Corr: 0.5134, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 514, Inner epoch 1, Loss: \tAvg Corr: 0.2354, Corr: 0.9516, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 515, Inner epoch 1, Loss: \tAvg Corr: 0.2399, Corr: 0.9101, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 516, Inner epoch 1, Loss: \tAvg Corr: 0.2337, Corr: 0.9420, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 517, Inner epoch 1, Loss: \tAvg Corr: 0.2211, Corr: 0.7787, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 518, Inner epoch 1, Loss: \tAvg Corr: 0.2719, Corr: 1.0324, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 519, Inner epoch 1, Loss: \tAvg Corr: 0.2251, Corr: 0.8004, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 520, Inner epoch 1, Loss: \tAvg Corr: 0.2316, Corr: 0.9331, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 521, Inner epoch 1, Loss: \tAvg Corr: 0.1692, Corr: 0.4522, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 522, Inner epoch 1, Loss: \tAvg Corr: 0.2235, Corr: 0.8257, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 523, Inner epoch 1, Loss: \tAvg Corr: 0.1861, Corr: 0.7650, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 524, Inner epoch 1, Loss: \tAvg Corr: 0.2060, Corr: 0.6228, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 525, Inner epoch 1, Loss: \tAvg Corr: 0.2347, Corr: 0.8543, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 526, Inner epoch 1, Loss: \tAvg Corr: 0.2392, Corr: 0.9525, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 527, Inner epoch 1, Loss: \tAvg Corr: 0.2042, Corr: 0.5990, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 528, Inner epoch 1, Loss: \tAvg Corr: 0.2075, Corr: 0.7568, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 529, Inner epoch 1, Loss: \tAvg Corr: 0.2526, Corr: 0.9663, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 530, Inner epoch 1, Loss: \tAvg Corr: 0.1980, Corr: 0.6682, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 531, Inner epoch 1, Loss: \tAvg Corr: 0.2481, Corr: 0.9135, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 532, Inner epoch 1, Loss: \tAvg Corr: 0.1764, Corr: 0.5030, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 533, Inner epoch 1, Loss: \tAvg Corr: 0.2645, Corr: 1.0115, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 534, Inner epoch 1, Loss: \tAvg Corr: 0.2109, Corr: 0.8692, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 535, Inner epoch 1, Loss: \tAvg Corr: 0.2342, Corr: 0.8893, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 536, Inner epoch 1, Loss: \tAvg Corr: 0.1552, Corr: 0.3632, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 537, Inner epoch 1, Loss: \tAvg Corr: 0.2326, Corr: 0.8544, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 538, Inner epoch 1, Loss: \tAvg Corr: 0.2632, Corr: 1.1722, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 539, Inner epoch 1, Loss: \tAvg Corr: 0.2293, Corr: 0.8001, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 540, Inner epoch 1, Loss: \tAvg Corr: 0.2955, Corr: 1.3310, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 541, Inner epoch 1, Loss: \tAvg Corr: 0.2385, Corr: 0.9199, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 542, Inner epoch 1, Loss: \tAvg Corr: 0.1877, Corr: 0.6781, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 543, Inner epoch 1, Loss: \tAvg Corr: 0.2527, Corr: 0.8984, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 544, Inner epoch 1, Loss: \tAvg Corr: 0.2399, Corr: 0.9257, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 545, Inner epoch 1, Loss: \tAvg Corr: 0.2202, Corr: 0.8319, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 546, Inner epoch 1, Loss: \tAvg Corr: 0.1891, Corr: 0.6079, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 547, Inner epoch 1, Loss: \tAvg Corr: 0.2322, Corr: 0.9527, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 548, Inner epoch 1, Loss: \tAvg Corr: 0.1597, Corr: 0.4172, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 549, Inner epoch 1, Loss: \tAvg Corr: 0.2433, Corr: 0.7896, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 550, Inner epoch 1, Loss: \tAvg Corr: 0.2208, Corr: 0.7331, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 551, Inner epoch 1, Loss: \tAvg Corr: 0.2523, Corr: 1.0558, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 552, Inner epoch 1, Loss: \tAvg Corr: 0.2248, Corr: 0.7843, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 553, Inner epoch 1, Loss: \tAvg Corr: 0.2734, Corr: 1.2660, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 554, Inner epoch 1, Loss: \tAvg Corr: 0.1944, Corr: 0.5757, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 555, Inner epoch 1, Loss: \tAvg Corr: 0.1746, Corr: 0.4695, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 556, Inner epoch 1, Loss: \tAvg Corr: 0.2670, Corr: 1.2749, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 557, Inner epoch 1, Loss: \tAvg Corr: 0.2382, Corr: 0.9183, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 558, Inner epoch 1, Loss: \tAvg Corr: 0.1835, Corr: 0.5490, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 559, Inner epoch 1, Loss: \tAvg Corr: 0.2406, Corr: 0.9719, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 560, Inner epoch 1, Loss: \tAvg Corr: 0.2199, Corr: 0.7904, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 561, Inner epoch 1, Loss: \tAvg Corr: 0.1778, Corr: 0.4767, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 562, Inner epoch 1, Loss: \tAvg Corr: 0.2474, Corr: 1.0127, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 563, Inner epoch 1, Loss: \tAvg Corr: 0.2205, Corr: 0.7592, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 564, Inner epoch 1, Loss: \tAvg Corr: 0.1707, Corr: 0.6226, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 565, Inner epoch 1, Loss: \tAvg Corr: 0.1742, Corr: 0.5039, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 566, Inner epoch 1, Loss: \tAvg Corr: 0.2399, Corr: 0.9203, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 567, Inner epoch 1, Loss: \tAvg Corr: 0.2096, Corr: 0.6624, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 568, Inner epoch 1, Loss: \tAvg Corr: 0.2258, Corr: 0.8548, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 569, Inner epoch 1, Loss: \tAvg Corr: 0.2398, Corr: 0.9141, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 570, Inner epoch 1, Loss: \tAvg Corr: 0.1895, Corr: 0.8666, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 571, Inner epoch 1, Loss: \tAvg Corr: 0.2427, Corr: 1.0478, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 572, Inner epoch 1, Loss: \tAvg Corr: 0.2624, Corr: 1.0593, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 573, Inner epoch 1, Loss: \tAvg Corr: 0.2699, Corr: 1.1294, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 574, Inner epoch 1, Loss: \tAvg Corr: 0.1908, Corr: 0.6682, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 575, Inner epoch 1, Loss: \tAvg Corr: 0.1947, Corr: 0.6425, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 576, Inner epoch 1, Loss: \tAvg Corr: 0.2389, Corr: 0.9276, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 577, Inner epoch 1, Loss: \tAvg Corr: 0.2450, Corr: 0.9593, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 578, Inner epoch 1, Loss: \tAvg Corr: 0.2558, Corr: 0.9873, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 579, Inner epoch 1, Loss: \tAvg Corr: 0.1926, Corr: 0.6451, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 580, Inner epoch 1, Loss: \tAvg Corr: 0.2443, Corr: 1.0855, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 581, Inner epoch 1, Loss: \tAvg Corr: 0.2865, Corr: 1.2772, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 582, Inner epoch 1, Loss: \tAvg Corr: 0.2661, Corr: 1.2883, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 583, Inner epoch 1, Loss: \tAvg Corr: 0.2502, Corr: 1.0454, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 584, Inner epoch 1, Loss: \tAvg Corr: 0.1948, Corr: 0.5694, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 585, Inner epoch 1, Loss: \tAvg Corr: 0.2757, Corr: 1.1554, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 586, Inner epoch 1, Loss: \tAvg Corr: 0.2038, Corr: 0.7586, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 587, Inner epoch 1, Loss: \tAvg Corr: 0.2239, Corr: 0.9574, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 588, Inner epoch 1, Loss: \tAvg Corr: 0.2170, Corr: 0.8946, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 589, Inner epoch 1, Loss: \tAvg Corr: 0.1865, Corr: 0.5204, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 590, Inner epoch 1, Loss: \tAvg Corr: 0.1939, Corr: 0.7248, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 591, Inner epoch 1, Loss: \tAvg Corr: 0.3165, Corr: 1.3993, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 592, Inner epoch 1, Loss: \tAvg Corr: 0.1893, Corr: 0.5660, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 593, Inner epoch 1, Loss: \tAvg Corr: 0.2455, Corr: 1.0091, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 594, Inner epoch 1, Loss: \tAvg Corr: 0.2154, Corr: 0.7889, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 595, Inner epoch 1, Loss: \tAvg Corr: 0.3245, Corr: 1.5793, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 596, Inner epoch 1, Loss: \tAvg Corr: 0.2977, Corr: 1.3244, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 597, Inner epoch 1, Loss: \tAvg Corr: 0.1868, Corr: 0.6322, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 598, Inner epoch 1, Loss: \tAvg Corr: 0.2371, Corr: 0.8751, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 599, Inner epoch 1, Loss: \tAvg Corr: 0.1947, Corr: 0.6117, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 600, Inner epoch 1, Loss: \tAvg Corr: 0.2220, Corr: 0.7339, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 601, Inner epoch 1, Loss: \tAvg Corr: 0.2021, Corr: 0.6971, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 602, Inner epoch 1, Loss: \tAvg Corr: 0.3211, Corr: 1.4453, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 603, Inner epoch 1, Loss: \tAvg Corr: 0.1676, Corr: 0.4284, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 604, Inner epoch 1, Loss: \tAvg Corr: 0.2511, Corr: 1.0342, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 605, Inner epoch 1, Loss: \tAvg Corr: 0.2493, Corr: 0.8653, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 606, Inner epoch 1, Loss: \tAvg Corr: 0.2202, Corr: 0.7497, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 607, Inner epoch 1, Loss: \tAvg Corr: 0.1786, Corr: 0.5604, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 608, Inner epoch 1, Loss: \tAvg Corr: 0.2507, Corr: 0.9423, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 609, Inner epoch 1, Loss: \tAvg Corr: 0.1686, Corr: 0.4570, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 610, Inner epoch 1, Loss: \tAvg Corr: 0.1964, Corr: 0.6222, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 611, Inner epoch 1, Loss: \tAvg Corr: 0.2327, Corr: 0.9229, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 612, Inner epoch 1, Loss: \tAvg Corr: 0.2397, Corr: 0.8536, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 613, Inner epoch 1, Loss: \tAvg Corr: 0.3238, Corr: 1.5152, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 614, Inner epoch 1, Loss: \tAvg Corr: 0.2256, Corr: 0.9819, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 615, Inner epoch 1, Loss: \tAvg Corr: 0.2171, Corr: 0.8741, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 616, Inner epoch 1, Loss: \tAvg Corr: 0.1888, Corr: 0.4932, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 617, Inner epoch 1, Loss: \tAvg Corr: 0.2542, Corr: 1.0533, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 618, Inner epoch 1, Loss: \tAvg Corr: 0.2104, Corr: 0.6531, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 619, Inner epoch 1, Loss: \tAvg Corr: 0.1913, Corr: 0.6906, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 620, Inner epoch 1, Loss: \tAvg Corr: 0.2368, Corr: 0.9410, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 621, Inner epoch 1, Loss: \tAvg Corr: 0.2253, Corr: 0.8622, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 622, Inner epoch 1, Loss: \tAvg Corr: 0.1980, Corr: 0.5741, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 623, Inner epoch 1, Loss: \tAvg Corr: 0.2501, Corr: 0.8811, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 624, Inner epoch 1, Loss: \tAvg Corr: 0.1858, Corr: 0.6182, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 625, Inner epoch 1, Loss: \tAvg Corr: 0.2135, Corr: 0.7168, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 626, Inner epoch 1, Loss: \tAvg Corr: 0.2082, Corr: 0.7305, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 627, Inner epoch 1, Loss: \tAvg Corr: 0.2308, Corr: 0.9339, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 628, Inner epoch 1, Loss: \tAvg Corr: 0.1703, Corr: 0.4766, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 629, Inner epoch 1, Loss: \tAvg Corr: 0.2343, Corr: 0.8351, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 630, Inner epoch 1, Loss: \tAvg Corr: 0.2120, Corr: 0.7731, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 631, Inner epoch 1, Loss: \tAvg Corr: 0.2053, Corr: 0.7090, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 632, Inner epoch 1, Loss: \tAvg Corr: 0.2483, Corr: 1.0704, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 633, Inner epoch 1, Loss: \tAvg Corr: 0.2019, Corr: 0.6896, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 634, Inner epoch 1, Loss: \tAvg Corr: 0.2329, Corr: 0.8503, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 635, Inner epoch 1, Loss: \tAvg Corr: 0.2351, Corr: 0.8949, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 636, Inner epoch 1, Loss: \tAvg Corr: 0.2307, Corr: 0.9112, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 637, Inner epoch 1, Loss: \tAvg Corr: 0.2113, Corr: 0.7484, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 638, Inner epoch 1, Loss: \tAvg Corr: 0.2079, Corr: 0.6513, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 639, Inner epoch 1, Loss: \tAvg Corr: 0.2440, Corr: 0.9395, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 640, Inner epoch 1, Loss: \tAvg Corr: 0.2036, Corr: 0.7027, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 641, Inner epoch 1, Loss: \tAvg Corr: 0.2767, Corr: 1.2325, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 642, Inner epoch 1, Loss: \tAvg Corr: 0.2921, Corr: 1.3289, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 643, Inner epoch 1, Loss: \tAvg Corr: 0.1895, Corr: 0.5948, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 644, Inner epoch 1, Loss: \tAvg Corr: 0.2159, Corr: 0.7692, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 645, Inner epoch 1, Loss: \tAvg Corr: 0.2633, Corr: 1.0679, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 646, Inner epoch 1, Loss: \tAvg Corr: 0.1895, Corr: 0.5583, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 647, Inner epoch 1, Loss: \tAvg Corr: 0.2456, Corr: 1.0525, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 648, Inner epoch 1, Loss: \tAvg Corr: 0.2494, Corr: 0.9333, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 649, Inner epoch 1, Loss: \tAvg Corr: 0.2288, Corr: 0.9087, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 650, Inner epoch 1, Loss: \tAvg Corr: 0.2414, Corr: 1.1806, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 651, Inner epoch 1, Loss: \tAvg Corr: 0.2336, Corr: 0.8017, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 652, Inner epoch 1, Loss: \tAvg Corr: 0.2306, Corr: 0.8265, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 653, Inner epoch 1, Loss: \tAvg Corr: 0.2260, Corr: 0.7423, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 654, Inner epoch 1, Loss: \tAvg Corr: 0.3159, Corr: 1.4529, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 655, Inner epoch 1, Loss: \tAvg Corr: 0.2020, Corr: 0.6105, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 656, Inner epoch 1, Loss: \tAvg Corr: 0.1365, Corr: 0.3300, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 657, Inner epoch 1, Loss: \tAvg Corr: 0.1952, Corr: 0.6428, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 658, Inner epoch 1, Loss: \tAvg Corr: 0.1986, Corr: 0.7267, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 659, Inner epoch 1, Loss: \tAvg Corr: 0.2370, Corr: 0.8386, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 660, Inner epoch 1, Loss: \tAvg Corr: 0.2154, Corr: 0.9149, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 661, Inner epoch 1, Loss: \tAvg Corr: 0.2246, Corr: 0.9251, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 662, Inner epoch 1, Loss: \tAvg Corr: 0.2097, Corr: 0.6807, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 663, Inner epoch 1, Loss: \tAvg Corr: 0.1977, Corr: 0.6205, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 664, Inner epoch 1, Loss: \tAvg Corr: 0.2273, Corr: 0.8420, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 665, Inner epoch 1, Loss: \tAvg Corr: 0.1939, Corr: 0.6600, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 666, Inner epoch 1, Loss: \tAvg Corr: 0.1722, Corr: 0.5858, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 667, Inner epoch 1, Loss: \tAvg Corr: 0.2135, Corr: 0.6408, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 668, Inner epoch 1, Loss: \tAvg Corr: 0.1569, Corr: 0.3595, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 669, Inner epoch 1, Loss: \tAvg Corr: 0.2546, Corr: 1.0388, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 670, Inner epoch 1, Loss: \tAvg Corr: 0.2892, Corr: 1.3630, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 671, Inner epoch 1, Loss: \tAvg Corr: 0.2130, Corr: 0.7187, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 672, Inner epoch 1, Loss: \tAvg Corr: 0.2593, Corr: 1.0927, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 673, Inner epoch 1, Loss: \tAvg Corr: 0.1814, Corr: 0.5308, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 674, Inner epoch 1, Loss: \tAvg Corr: 0.2667, Corr: 1.0896, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 675, Inner epoch 1, Loss: \tAvg Corr: 0.2398, Corr: 0.9082, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 676, Inner epoch 1, Loss: \tAvg Corr: 0.2497, Corr: 0.9041, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 677, Inner epoch 1, Loss: \tAvg Corr: 0.2673, Corr: 1.0748, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 678, Inner epoch 1, Loss: \tAvg Corr: 0.2077, Corr: 0.7771, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 679, Inner epoch 1, Loss: \tAvg Corr: 0.1653, Corr: 0.4525, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 680, Inner epoch 1, Loss: \tAvg Corr: 0.2392, Corr: 0.9971, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 681, Inner epoch 1, Loss: \tAvg Corr: 0.2106, Corr: 0.6656, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 682, Inner epoch 1, Loss: \tAvg Corr: 0.1736, Corr: 0.4928, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 683, Inner epoch 1, Loss: \tAvg Corr: 0.1963, Corr: 0.5897, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 684, Inner epoch 1, Loss: \tAvg Corr: 0.1731, Corr: 0.5843, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 685, Inner epoch 1, Loss: \tAvg Corr: 0.2345, Corr: 0.8281, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 686, Inner epoch 1, Loss: \tAvg Corr: 0.2395, Corr: 0.7540, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 687, Inner epoch 1, Loss: \tAvg Corr: 0.2014, Corr: 0.5944, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 688, Inner epoch 1, Loss: \tAvg Corr: 0.3373, Corr: 1.7288, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 689, Inner epoch 1, Loss: \tAvg Corr: 0.2378, Corr: 0.9252, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 690, Inner epoch 1, Loss: \tAvg Corr: 0.1907, Corr: 0.6289, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 691, Inner epoch 1, Loss: \tAvg Corr: 0.2339, Corr: 0.9044, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 692, Inner epoch 1, Loss: \tAvg Corr: 0.1633, Corr: 0.3746, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 693, Inner epoch 1, Loss: \tAvg Corr: 0.2176, Corr: 0.7966, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 694, Inner epoch 1, Loss: \tAvg Corr: 0.2106, Corr: 0.7948, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 695, Inner epoch 1, Loss: \tAvg Corr: 0.2408, Corr: 0.9067, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 696, Inner epoch 1, Loss: \tAvg Corr: 0.2068, Corr: 0.5929, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 697, Inner epoch 1, Loss: \tAvg Corr: 0.2246, Corr: 0.7481, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 698, Inner epoch 1, Loss: \tAvg Corr: 0.2653, Corr: 1.0317, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 699, Inner epoch 1, Loss: \tAvg Corr: 0.2160, Corr: 0.7729, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 700, Inner epoch 1, Loss: \tAvg Corr: 0.1948, Corr: 0.5621, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 701, Inner epoch 1, Loss: \tAvg Corr: 0.2559, Corr: 1.0856, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 702, Inner epoch 1, Loss: \tAvg Corr: 0.1915, Corr: 0.7110, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 703, Inner epoch 1, Loss: \tAvg Corr: 0.2276, Corr: 0.8477, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 704, Inner epoch 1, Loss: \tAvg Corr: 0.1889, Corr: 0.5472, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 705, Inner epoch 1, Loss: \tAvg Corr: 0.2349, Corr: 0.8343, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 706, Inner epoch 1, Loss: \tAvg Corr: 0.2732, Corr: 1.2664, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 707, Inner epoch 1, Loss: \tAvg Corr: 0.1802, Corr: 0.6299, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 708, Inner epoch 1, Loss: \tAvg Corr: 0.2194, Corr: 0.7615, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 709, Inner epoch 1, Loss: \tAvg Corr: 0.2542, Corr: 1.0812, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 710, Inner epoch 1, Loss: \tAvg Corr: 0.2395, Corr: 0.9345, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 711, Inner epoch 1, Loss: \tAvg Corr: 0.2235, Corr: 0.8919, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 712, Inner epoch 1, Loss: \tAvg Corr: 0.2021, Corr: 0.5629, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 713, Inner epoch 1, Loss: \tAvg Corr: 0.2274, Corr: 0.8601, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 714, Inner epoch 1, Loss: \tAvg Corr: 0.2534, Corr: 0.9774, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 715, Inner epoch 1, Loss: \tAvg Corr: 0.2233, Corr: 0.7416, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 716, Inner epoch 1, Loss: \tAvg Corr: 0.2672, Corr: 1.0843, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 717, Inner epoch 1, Loss: \tAvg Corr: 0.2871, Corr: 1.2194, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 718, Inner epoch 1, Loss: \tAvg Corr: 0.2478, Corr: 1.0929, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 719, Inner epoch 1, Loss: \tAvg Corr: 0.2635, Corr: 1.1621, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 720, Inner epoch 1, Loss: \tAvg Corr: 0.1904, Corr: 0.6072, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 721, Inner epoch 1, Loss: \tAvg Corr: 0.2216, Corr: 0.7366, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 722, Inner epoch 1, Loss: \tAvg Corr: 0.1984, Corr: 0.6229, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 723, Inner epoch 1, Loss: \tAvg Corr: 0.2046, Corr: 0.6550, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 724, Inner epoch 1, Loss: \tAvg Corr: 0.1979, Corr: 0.5215, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 725, Inner epoch 1, Loss: \tAvg Corr: 0.1989, Corr: 0.6891, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 726, Inner epoch 1, Loss: \tAvg Corr: 0.2004, Corr: 0.7089, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 727, Inner epoch 1, Loss: \tAvg Corr: 0.2340, Corr: 0.8095, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 728, Inner epoch 1, Loss: \tAvg Corr: 0.2304, Corr: 0.9767, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 729, Inner epoch 1, Loss: \tAvg Corr: 0.1857, Corr: 0.5946, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 730, Inner epoch 1, Loss: \tAvg Corr: 0.2449, Corr: 1.0248, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 731, Inner epoch 1, Loss: \tAvg Corr: 0.2422, Corr: 0.8947, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 732, Inner epoch 1, Loss: \tAvg Corr: 0.2303, Corr: 0.7541, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 733, Inner epoch 1, Loss: \tAvg Corr: 0.2457, Corr: 1.1238, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 734, Inner epoch 1, Loss: \tAvg Corr: 0.2548, Corr: 1.1049, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 735, Inner epoch 1, Loss: \tAvg Corr: 0.2837, Corr: 1.2250, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 736, Inner epoch 1, Loss: \tAvg Corr: 0.2788, Corr: 1.4366, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 737, Inner epoch 1, Loss: \tAvg Corr: 0.2369, Corr: 0.8431, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 738, Inner epoch 1, Loss: \tAvg Corr: 0.2199, Corr: 0.8383, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 739, Inner epoch 1, Loss: \tAvg Corr: 0.1704, Corr: 0.4882, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 740, Inner epoch 1, Loss: \tAvg Corr: 0.1850, Corr: 0.5497, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 741, Inner epoch 1, Loss: \tAvg Corr: 0.2147, Corr: 0.7046, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 742, Inner epoch 1, Loss: \tAvg Corr: 0.2512, Corr: 1.0343, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 743, Inner epoch 1, Loss: \tAvg Corr: 0.2239, Corr: 0.8246, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 744, Inner epoch 1, Loss: \tAvg Corr: 0.2381, Corr: 0.8985, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 745, Inner epoch 1, Loss: \tAvg Corr: 0.1968, Corr: 0.7215, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 746, Inner epoch 1, Loss: \tAvg Corr: 0.2568, Corr: 1.0468, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 747, Inner epoch 1, Loss: \tAvg Corr: 0.2252, Corr: 0.7248, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 748, Inner epoch 1, Loss: \tAvg Corr: 0.2848, Corr: 1.1778, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 749, Inner epoch 1, Loss: \tAvg Corr: 0.2074, Corr: 0.5943, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 750, Inner epoch 1, Loss: \tAvg Corr: 0.2137, Corr: 0.7210, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 751, Inner epoch 1, Loss: \tAvg Corr: 0.1959, Corr: 0.6522, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 752, Inner epoch 1, Loss: \tAvg Corr: 0.2032, Corr: 0.6511, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 753, Inner epoch 1, Loss: \tAvg Corr: 0.2152, Corr: 0.6799, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 754, Inner epoch 1, Loss: \tAvg Corr: 0.2321, Corr: 0.8874, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 755, Inner epoch 1, Loss: \tAvg Corr: 0.2309, Corr: 0.8023, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 756, Inner epoch 1, Loss: \tAvg Corr: 0.1857, Corr: 0.6598, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 757, Inner epoch 1, Loss: \tAvg Corr: 0.2499, Corr: 1.1576, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 758, Inner epoch 1, Loss: \tAvg Corr: 0.1799, Corr: 0.5765, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 759, Inner epoch 1, Loss: \tAvg Corr: 0.2182, Corr: 0.7814, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 760, Inner epoch 1, Loss: \tAvg Corr: 0.2065, Corr: 0.7367, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 761, Inner epoch 1, Loss: \tAvg Corr: 0.2805, Corr: 1.2595, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 762, Inner epoch 1, Loss: \tAvg Corr: 0.1866, Corr: 0.5051, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 763, Inner epoch 1, Loss: \tAvg Corr: 0.2140, Corr: 0.6279, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 764, Inner epoch 1, Loss: \tAvg Corr: 0.1964, Corr: 0.5651, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 765, Inner epoch 1, Loss: \tAvg Corr: 0.1991, Corr: 0.7018, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 766, Inner epoch 1, Loss: \tAvg Corr: 0.1696, Corr: 0.4966, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 767, Inner epoch 1, Loss: \tAvg Corr: 0.2575, Corr: 1.0858, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 768, Inner epoch 1, Loss: \tAvg Corr: 0.2359, Corr: 0.8874, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 769, Inner epoch 1, Loss: \tAvg Corr: 0.2085, Corr: 0.6779, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 770, Inner epoch 1, Loss: \tAvg Corr: 0.2093, Corr: 0.6539, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 771, Inner epoch 1, Loss: \tAvg Corr: 0.1906, Corr: 0.5301, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 772, Inner epoch 1, Loss: \tAvg Corr: 0.2158, Corr: 0.6595, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 773, Inner epoch 1, Loss: \tAvg Corr: 0.2325, Corr: 0.8922, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 774, Inner epoch 1, Loss: \tAvg Corr: 0.1835, Corr: 0.5591, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 775, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.8371, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 776, Inner epoch 1, Loss: \tAvg Corr: 0.1702, Corr: 0.4411, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 777, Inner epoch 1, Loss: \tAvg Corr: 0.3010, Corr: 1.4130, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 778, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.7848, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 779, Inner epoch 1, Loss: \tAvg Corr: 0.2612, Corr: 1.0059, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 780, Inner epoch 1, Loss: \tAvg Corr: 0.1977, Corr: 0.6995, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 781, Inner epoch 1, Loss: \tAvg Corr: 0.1923, Corr: 0.6091, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 782, Inner epoch 1, Loss: \tAvg Corr: 0.1824, Corr: 0.4793, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 783, Inner epoch 1, Loss: \tAvg Corr: 0.2595, Corr: 1.0844, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 784, Inner epoch 1, Loss: \tAvg Corr: 0.2430, Corr: 0.8346, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 785, Inner epoch 1, Loss: \tAvg Corr: 0.2306, Corr: 0.9194, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 786, Inner epoch 1, Loss: \tAvg Corr: 0.2106, Corr: 0.8450, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 787, Inner epoch 1, Loss: \tAvg Corr: 0.2112, Corr: 0.6459, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 788, Inner epoch 1, Loss: \tAvg Corr: 0.1893, Corr: 0.5509, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 789, Inner epoch 1, Loss: \tAvg Corr: 0.2268, Corr: 0.7937, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 790, Inner epoch 1, Loss: \tAvg Corr: 0.1699, Corr: 0.4450, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 791, Inner epoch 1, Loss: \tAvg Corr: 0.2184, Corr: 0.7790, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 792, Inner epoch 1, Loss: \tAvg Corr: 0.2113, Corr: 0.8640, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 793, Inner epoch 1, Loss: \tAvg Corr: 0.2473, Corr: 1.0679, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 794, Inner epoch 1, Loss: \tAvg Corr: 0.2190, Corr: 0.8326, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 795, Inner epoch 1, Loss: \tAvg Corr: 0.2242, Corr: 0.7222, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 796, Inner epoch 1, Loss: \tAvg Corr: 0.1820, Corr: 0.7418, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 797, Inner epoch 1, Loss: \tAvg Corr: 0.2496, Corr: 0.9721, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 798, Inner epoch 1, Loss: \tAvg Corr: 0.2140, Corr: 0.6915, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 799, Inner epoch 1, Loss: \tAvg Corr: 0.2493, Corr: 0.9951, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 800, Inner epoch 1, Loss: \tAvg Corr: 0.2249, Corr: 0.7584, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 801, Inner epoch 1, Loss: \tAvg Corr: 0.1616, Corr: 0.4404, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 802, Inner epoch 1, Loss: \tAvg Corr: 0.2300, Corr: 0.8067, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 803, Inner epoch 1, Loss: \tAvg Corr: 0.1798, Corr: 0.5341, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 804, Inner epoch 1, Loss: \tAvg Corr: 0.2947, Corr: 1.5781, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 805, Inner epoch 1, Loss: \tAvg Corr: 0.2099, Corr: 0.6497, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 806, Inner epoch 1, Loss: \tAvg Corr: 0.2714, Corr: 1.3103, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 807, Inner epoch 1, Loss: \tAvg Corr: 0.2183, Corr: 0.7942, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 808, Inner epoch 1, Loss: \tAvg Corr: 0.2196, Corr: 0.8655, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 809, Inner epoch 1, Loss: \tAvg Corr: 0.1919, Corr: 0.5764, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 810, Inner epoch 1, Loss: \tAvg Corr: 0.1872, Corr: 0.6141, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 811, Inner epoch 1, Loss: \tAvg Corr: 0.2094, Corr: 0.7591, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 812, Inner epoch 1, Loss: \tAvg Corr: 0.1689, Corr: 0.4392, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 813, Inner epoch 1, Loss: \tAvg Corr: 0.2820, Corr: 1.1577, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 814, Inner epoch 1, Loss: \tAvg Corr: 0.2214, Corr: 0.8645, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 815, Inner epoch 1, Loss: \tAvg Corr: 0.2601, Corr: 1.0186, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 816, Inner epoch 1, Loss: \tAvg Corr: 0.2667, Corr: 1.2102, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 817, Inner epoch 1, Loss: \tAvg Corr: 0.2635, Corr: 1.2030, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 818, Inner epoch 1, Loss: \tAvg Corr: 0.1902, Corr: 0.6132, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 819, Inner epoch 1, Loss: \tAvg Corr: 0.2179, Corr: 0.7608, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 820, Inner epoch 1, Loss: \tAvg Corr: 0.2299, Corr: 1.0109, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 821, Inner epoch 1, Loss: \tAvg Corr: 0.2118, Corr: 0.7724, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 822, Inner epoch 1, Loss: \tAvg Corr: 0.1991, Corr: 0.6599, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 823, Inner epoch 1, Loss: \tAvg Corr: 0.1975, Corr: 0.6439, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 824, Inner epoch 1, Loss: \tAvg Corr: 0.1944, Corr: 0.6197, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 825, Inner epoch 1, Loss: \tAvg Corr: 0.2608, Corr: 1.1731, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 826, Inner epoch 1, Loss: \tAvg Corr: 0.2290, Corr: 0.7804, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 827, Inner epoch 1, Loss: \tAvg Corr: 0.2560, Corr: 1.0782, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 828, Inner epoch 1, Loss: \tAvg Corr: 0.2334, Corr: 0.8653, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 829, Inner epoch 1, Loss: \tAvg Corr: 0.2000, Corr: 0.7357, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 830, Inner epoch 1, Loss: \tAvg Corr: 0.1778, Corr: 0.4527, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 831, Inner epoch 1, Loss: \tAvg Corr: 0.2081, Corr: 0.6331, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 832, Inner epoch 1, Loss: \tAvg Corr: 0.2078, Corr: 0.8395, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 833, Inner epoch 1, Loss: \tAvg Corr: 0.1722, Corr: 0.4459, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 834, Inner epoch 1, Loss: \tAvg Corr: 0.1934, Corr: 0.6223, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 835, Inner epoch 1, Loss: \tAvg Corr: 0.2349, Corr: 0.9551, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 836, Inner epoch 1, Loss: \tAvg Corr: 0.1993, Corr: 0.6050, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 837, Inner epoch 1, Loss: \tAvg Corr: 0.2386, Corr: 0.9566, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 838, Inner epoch 1, Loss: \tAvg Corr: 0.1859, Corr: 0.5560, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 839, Inner epoch 1, Loss: \tAvg Corr: 0.2733, Corr: 1.2343, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 840, Inner epoch 1, Loss: \tAvg Corr: 0.1902, Corr: 0.6753, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 841, Inner epoch 1, Loss: \tAvg Corr: 0.2269, Corr: 0.7611, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 842, Inner epoch 1, Loss: \tAvg Corr: 0.1833, Corr: 0.5352, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 843, Inner epoch 1, Loss: \tAvg Corr: 0.2051, Corr: 0.6466, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 844, Inner epoch 1, Loss: \tAvg Corr: 0.2556, Corr: 1.0096, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 845, Inner epoch 1, Loss: \tAvg Corr: 0.2217, Corr: 0.8889, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 846, Inner epoch 1, Loss: \tAvg Corr: 0.2339, Corr: 0.8568, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 847, Inner epoch 1, Loss: \tAvg Corr: 0.1962, Corr: 0.7608, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 848, Inner epoch 1, Loss: \tAvg Corr: 0.1956, Corr: 0.6968, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 849, Inner epoch 1, Loss: \tAvg Corr: 0.2191, Corr: 0.7562, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 850, Inner epoch 1, Loss: \tAvg Corr: 0.1798, Corr: 0.5716, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 851, Inner epoch 1, Loss: \tAvg Corr: 0.2520, Corr: 1.1214, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 852, Inner epoch 1, Loss: \tAvg Corr: 0.2157, Corr: 0.6768, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 853, Inner epoch 1, Loss: \tAvg Corr: 0.2319, Corr: 0.9434, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 854, Inner epoch 1, Loss: \tAvg Corr: 0.2161, Corr: 0.7259, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 855, Inner epoch 1, Loss: \tAvg Corr: 0.2598, Corr: 0.9456, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 856, Inner epoch 1, Loss: \tAvg Corr: 0.2158, Corr: 0.7115, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 857, Inner epoch 1, Loss: \tAvg Corr: 0.1881, Corr: 0.6329, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 858, Inner epoch 1, Loss: \tAvg Corr: 0.3359, Corr: 1.8078, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 859, Inner epoch 1, Loss: \tAvg Corr: 0.2404, Corr: 0.9284, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 860, Inner epoch 1, Loss: \tAvg Corr: 0.2663, Corr: 1.1429, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 861, Inner epoch 1, Loss: \tAvg Corr: 0.2457, Corr: 1.0083, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 862, Inner epoch 1, Loss: \tAvg Corr: 0.2336, Corr: 0.7843, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 863, Inner epoch 1, Loss: \tAvg Corr: 0.1614, Corr: 0.3970, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 864, Inner epoch 1, Loss: \tAvg Corr: 0.2485, Corr: 0.8304, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 865, Inner epoch 1, Loss: \tAvg Corr: 0.2028, Corr: 0.7173, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 866, Inner epoch 1, Loss: \tAvg Corr: 0.2492, Corr: 1.0694, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 867, Inner epoch 1, Loss: \tAvg Corr: 0.2244, Corr: 0.7472, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 868, Inner epoch 1, Loss: \tAvg Corr: 0.2107, Corr: 0.7850, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 869, Inner epoch 1, Loss: \tAvg Corr: 0.2402, Corr: 0.8712, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 870, Inner epoch 1, Loss: \tAvg Corr: 0.2773, Corr: 1.2323, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 871, Inner epoch 1, Loss: \tAvg Corr: 0.3068, Corr: 1.4144, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 872, Inner epoch 1, Loss: \tAvg Corr: 0.2704, Corr: 1.2118, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 873, Inner epoch 1, Loss: \tAvg Corr: 0.1889, Corr: 0.6082, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 874, Inner epoch 1, Loss: \tAvg Corr: 0.2313, Corr: 0.8571, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 875, Inner epoch 1, Loss: \tAvg Corr: 0.2500, Corr: 0.9770, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 876, Inner epoch 1, Loss: \tAvg Corr: 0.2140, Corr: 0.8321, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 877, Inner epoch 1, Loss: \tAvg Corr: 0.1895, Corr: 0.5562, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 878, Inner epoch 1, Loss: \tAvg Corr: 0.1915, Corr: 0.6201, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 879, Inner epoch 1, Loss: \tAvg Corr: 0.2340, Corr: 0.8815, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 880, Inner epoch 1, Loss: \tAvg Corr: 0.2254, Corr: 0.7506, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 881, Inner epoch 1, Loss: \tAvg Corr: 0.2931, Corr: 1.3307, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 882, Inner epoch 1, Loss: \tAvg Corr: 0.2815, Corr: 1.2707, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 883, Inner epoch 1, Loss: \tAvg Corr: 0.2007, Corr: 0.6345, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 884, Inner epoch 1, Loss: \tAvg Corr: 0.1710, Corr: 0.4619, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 885, Inner epoch 1, Loss: \tAvg Corr: 0.1743, Corr: 0.4816, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 886, Inner epoch 1, Loss: \tAvg Corr: 0.2104, Corr: 0.8971, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 887, Inner epoch 1, Loss: \tAvg Corr: 0.2191, Corr: 0.8334, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 888, Inner epoch 1, Loss: \tAvg Corr: 0.1846, Corr: 0.5875, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 889, Inner epoch 1, Loss: \tAvg Corr: 0.2067, Corr: 0.6953, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 890, Inner epoch 1, Loss: \tAvg Corr: 0.2931, Corr: 1.3449, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 891, Inner epoch 1, Loss: \tAvg Corr: 0.3643, Corr: 1.7976, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 892, Inner epoch 1, Loss: \tAvg Corr: 0.1995, Corr: 0.5957, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 893, Inner epoch 1, Loss: \tAvg Corr: 0.2065, Corr: 0.6719, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 894, Inner epoch 1, Loss: \tAvg Corr: 0.2941, Corr: 1.2346, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 895, Inner epoch 1, Loss: \tAvg Corr: 0.2050, Corr: 0.6943, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 896, Inner epoch 1, Loss: \tAvg Corr: 0.1976, Corr: 0.6340, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 897, Inner epoch 1, Loss: \tAvg Corr: 0.2325, Corr: 0.8585, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 898, Inner epoch 1, Loss: \tAvg Corr: 0.2402, Corr: 0.9857, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 899, Inner epoch 1, Loss: \tAvg Corr: 0.2356, Corr: 0.8047, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 900, Inner epoch 1, Loss: \tAvg Corr: 0.3381, Corr: 1.7949, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 901, Inner epoch 1, Loss: \tAvg Corr: 0.2033, Corr: 0.6823, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 902, Inner epoch 1, Loss: \tAvg Corr: 0.2118, Corr: 0.6998, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 903, Inner epoch 1, Loss: \tAvg Corr: 0.1749, Corr: 0.4592, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 904, Inner epoch 1, Loss: \tAvg Corr: 0.1787, Corr: 0.4918, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 905, Inner epoch 1, Loss: \tAvg Corr: 0.2451, Corr: 1.0186, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 906, Inner epoch 1, Loss: \tAvg Corr: 0.1806, Corr: 0.4874, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 907, Inner epoch 1, Loss: \tAvg Corr: 0.2395, Corr: 0.9499, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 908, Inner epoch 1, Loss: \tAvg Corr: 0.2367, Corr: 0.9290, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 909, Inner epoch 1, Loss: \tAvg Corr: 0.2805, Corr: 1.2061, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 910, Inner epoch 1, Loss: \tAvg Corr: 0.1675, Corr: 0.4090, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 911, Inner epoch 1, Loss: \tAvg Corr: 0.1466, Corr: 0.3339, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 912, Inner epoch 1, Loss: \tAvg Corr: 0.2377, Corr: 0.8427, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 913, Inner epoch 1, Loss: \tAvg Corr: 0.1671, Corr: 0.4637, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 914, Inner epoch 1, Loss: \tAvg Corr: 0.2121, Corr: 0.6792, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 915, Inner epoch 1, Loss: \tAvg Corr: 0.1958, Corr: 0.7466, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 916, Inner epoch 1, Loss: \tAvg Corr: 0.2566, Corr: 1.0979, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 917, Inner epoch 1, Loss: \tAvg Corr: 0.2171, Corr: 0.8760, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 918, Inner epoch 1, Loss: \tAvg Corr: 0.1875, Corr: 0.6410, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 919, Inner epoch 1, Loss: \tAvg Corr: 0.1854, Corr: 0.6879, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 920, Inner epoch 1, Loss: \tAvg Corr: 0.1755, Corr: 0.5394, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 921, Inner epoch 1, Loss: \tAvg Corr: 0.1743, Corr: 0.4752, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 922, Inner epoch 1, Loss: \tAvg Corr: 0.1784, Corr: 0.6083, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 923, Inner epoch 1, Loss: \tAvg Corr: 0.1900, Corr: 0.5281, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 924, Inner epoch 1, Loss: \tAvg Corr: 0.2241, Corr: 0.6913, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 925, Inner epoch 1, Loss: \tAvg Corr: 0.1579, Corr: 0.3993, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 926, Inner epoch 1, Loss: \tAvg Corr: 0.2221, Corr: 0.7540, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 927, Inner epoch 1, Loss: \tAvg Corr: 0.1229, Corr: 0.2962, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 928, Inner epoch 1, Loss: \tAvg Corr: 0.1930, Corr: 0.6315, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 929, Inner epoch 1, Loss: \tAvg Corr: 0.2222, Corr: 0.8780, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 930, Inner epoch 1, Loss: \tAvg Corr: 0.1738, Corr: 0.4848, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 931, Inner epoch 1, Loss: \tAvg Corr: 0.2832, Corr: 1.3000, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 932, Inner epoch 1, Loss: \tAvg Corr: 0.2232, Corr: 0.8484, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 933, Inner epoch 1, Loss: \tAvg Corr: 0.2122, Corr: 0.6664, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 934, Inner epoch 1, Loss: \tAvg Corr: 0.1724, Corr: 0.4632, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 935, Inner epoch 1, Loss: \tAvg Corr: 0.1941, Corr: 0.6028, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 936, Inner epoch 1, Loss: \tAvg Corr: 0.2589, Corr: 1.1204, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 937, Inner epoch 1, Loss: \tAvg Corr: 0.2098, Corr: 0.7732, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 938, Inner epoch 1, Loss: \tAvg Corr: 0.2561, Corr: 1.1703, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 939, Inner epoch 1, Loss: \tAvg Corr: 0.2157, Corr: 0.8612, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 940, Inner epoch 1, Loss: \tAvg Corr: 0.2655, Corr: 1.0470, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 941, Inner epoch 1, Loss: \tAvg Corr: 0.2273, Corr: 0.8411, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 942, Inner epoch 1, Loss: \tAvg Corr: 0.1980, Corr: 0.7071, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 943, Inner epoch 1, Loss: \tAvg Corr: 0.1904, Corr: 0.5663, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 944, Inner epoch 1, Loss: \tAvg Corr: 0.2405, Corr: 0.9174, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 945, Inner epoch 1, Loss: \tAvg Corr: 0.2441, Corr: 0.9953, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 946, Inner epoch 1, Loss: \tAvg Corr: 0.1900, Corr: 0.5057, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 947, Inner epoch 1, Loss: \tAvg Corr: 0.2729, Corr: 1.0389, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 948, Inner epoch 1, Loss: \tAvg Corr: 0.2743, Corr: 1.1548, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 949, Inner epoch 1, Loss: \tAvg Corr: 0.1689, Corr: 0.5033, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 950, Inner epoch 1, Loss: \tAvg Corr: 0.1722, Corr: 0.5159, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 951, Inner epoch 1, Loss: \tAvg Corr: 0.2299, Corr: 0.8727, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 952, Inner epoch 1, Loss: \tAvg Corr: 0.1747, Corr: 0.5391, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 953, Inner epoch 1, Loss: \tAvg Corr: 0.2048, Corr: 0.5910, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 954, Inner epoch 1, Loss: \tAvg Corr: 0.2433, Corr: 0.9974, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 955, Inner epoch 1, Loss: \tAvg Corr: 0.2200, Corr: 0.7457, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 956, Inner epoch 1, Loss: \tAvg Corr: 0.2050, Corr: 0.7010, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 957, Inner epoch 1, Loss: \tAvg Corr: 0.1707, Corr: 0.6408, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 958, Inner epoch 1, Loss: \tAvg Corr: 0.2033, Corr: 0.7395, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 959, Inner epoch 1, Loss: \tAvg Corr: 0.2374, Corr: 0.8937, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 960, Inner epoch 1, Loss: \tAvg Corr: 0.2251, Corr: 0.8248, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 961, Inner epoch 1, Loss: \tAvg Corr: 0.2252, Corr: 0.7812, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 962, Inner epoch 1, Loss: \tAvg Corr: 0.2022, Corr: 0.6643, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 963, Inner epoch 1, Loss: \tAvg Corr: 0.1996, Corr: 0.6795, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 964, Inner epoch 1, Loss: \tAvg Corr: 0.2814, Corr: 1.2057, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 965, Inner epoch 1, Loss: \tAvg Corr: 0.1775, Corr: 0.5691, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 966, Inner epoch 1, Loss: \tAvg Corr: 0.2523, Corr: 0.8830, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 967, Inner epoch 1, Loss: \tAvg Corr: 0.2304, Corr: 0.7837, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 968, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.7500, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 969, Inner epoch 1, Loss: \tAvg Corr: 0.2149, Corr: 0.7784, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 970, Inner epoch 1, Loss: \tAvg Corr: 0.2714, Corr: 1.1434, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 971, Inner epoch 1, Loss: \tAvg Corr: 0.2476, Corr: 1.0176, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 972, Inner epoch 1, Loss: \tAvg Corr: 0.2106, Corr: 0.7201, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 973, Inner epoch 1, Loss: \tAvg Corr: 0.2692, Corr: 1.1085, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 974, Inner epoch 1, Loss: \tAvg Corr: 0.2020, Corr: 0.7337, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 975, Inner epoch 1, Loss: \tAvg Corr: 0.2062, Corr: 0.7021, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 976, Inner epoch 1, Loss: \tAvg Corr: 0.1935, Corr: 0.7405, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 977, Inner epoch 1, Loss: \tAvg Corr: 0.1948, Corr: 0.5725, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 978, Inner epoch 1, Loss: \tAvg Corr: 0.1718, Corr: 0.4541, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 979, Inner epoch 1, Loss: \tAvg Corr: 0.2421, Corr: 0.8695, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 980, Inner epoch 1, Loss: \tAvg Corr: 0.1742, Corr: 0.6092, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 981, Inner epoch 1, Loss: \tAvg Corr: 0.2764, Corr: 1.1398, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 982, Inner epoch 1, Loss: \tAvg Corr: 0.1836, Corr: 0.6044, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 983, Inner epoch 1, Loss: \tAvg Corr: 0.1829, Corr: 0.5215, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 984, Inner epoch 1, Loss: \tAvg Corr: 0.2307, Corr: 0.8008, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 985, Inner epoch 1, Loss: \tAvg Corr: 0.1808, Corr: 0.4970, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 986, Inner epoch 1, Loss: \tAvg Corr: 0.2253, Corr: 0.8096, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 987, Inner epoch 1, Loss: \tAvg Corr: 0.2460, Corr: 0.9692, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 988, Inner epoch 1, Loss: \tAvg Corr: 0.2353, Corr: 0.8685, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 989, Inner epoch 1, Loss: \tAvg Corr: 0.2079, Corr: 0.6578, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 990, Inner epoch 1, Loss: \tAvg Corr: 0.1656, Corr: 0.4501, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 991, Inner epoch 1, Loss: \tAvg Corr: 0.2453, Corr: 1.0200, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 992, Inner epoch 1, Loss: \tAvg Corr: 0.2546, Corr: 0.9798, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 993, Inner epoch 1, Loss: \tAvg Corr: 0.2450, Corr: 0.9382, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 994, Inner epoch 1, Loss: \tAvg Corr: 0.2007, Corr: 0.5550, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 995, Inner epoch 1, Loss: \tAvg Corr: 0.2156, Corr: 0.6797, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 996, Inner epoch 1, Loss: \tAvg Corr: 0.2334, Corr: 0.9442, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 997, Inner epoch 1, Loss: \tAvg Corr: 0.2155, Corr: 0.7188, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 998, Inner epoch 1, Loss: \tAvg Corr: 0.2077, Corr: 0.6392, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n",
      "Outer epoch 999, Inner epoch 1, Loss: \tAvg Corr: 0.2715, Corr: 1.2467, Sparsity: 0.0000, IE: 0.0000, EI: 0.0000\n"
     ]
    }
   ],
   "source": [
    "losses = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b8aabaa70054798",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.960642Z",
     "start_time": "2024-08-01T17:46:56.958505Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(torch.arange(losses.shape[0]), losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss ratio\")\n",
    "    plt.title(\"Loss ratio per realization over time\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3843b2195b086d5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.959377Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = correlated_mitral_activity()\n",
    "hbar_ff = compute_feedforward_activity(i)\n",
    "W_random = compute_initial_recurrent_weights()\n",
    "R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4da719c5a4a5ebbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.976483Z",
     "start_time": "2024-08-01T17:46:56.961173Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stats at initialization\n",
    "mu_familiar_0 = torch.mean(R_random[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_0 = torch.var(R_random[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_0 = torch.mean(R_random[:num_e, novel_inds], dim=1)\n",
    "sig_novel_0 = torch.var(R_random[:num_e, novel_inds], dim=1)\n",
    "#print(f\"Initialization: \\nFamiliar -  mean: {mu_familiar_0}, var: {sig_familiar_0}\\nNovel -  mean: {mu_novel_0}, var: {sig_novel_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67971e29f9bb692",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.962883Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random = R_random.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8aea9b6d1ff27717",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.964663Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loss_after_odors() missing 1 required positional argument: 'lambda_sp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     22\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_inner):\n\u001b[0;32m---> 23\u001b[0m         _, W_plastic, R_plastic \u001b[39m=\u001b[39m loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, with_loss\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     24\u001b[0m         W_tracked[i, :, \u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m W_plastic[ie_update_inds][ie_track_inds]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     25\u001b[0m         W_tracked[i, :, \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m W_plastic[ei_update_inds][ei_track_inds]\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[0;31mTypeError\u001b[0m: loss_after_odors() missing 1 required positional argument: 'lambda_sp'"
     ]
    }
   ],
   "source": [
    "W_plastic = W_random.detach().clone()\n",
    "R_plastic = R_random.detach().clone()\n",
    "with torch.no_grad():\n",
    "    ie_update_inds = get_update_inds(ie_post, ie_pre, W_plastic)\n",
    "    ei_update_inds = get_update_inds(ei_post, ei_pre, W_plastic)\n",
    "    clamp_min = torch.zeros_like(W_plastic)\n",
    "    clamp_min[ei_update_inds] = ei_min_weight\n",
    "    clamp_min[ie_update_inds] = ie_min_weight\n",
    "    clamp_max = torch.zeros_like(W_plastic)\n",
    "    clamp_max[ie_update_inds] = ie_max_weight\n",
    "    clamp_max[ei_update_inds] = ei_max_weight\n",
    "    weight_range = (clamp_min, clamp_max)\n",
    "\n",
    "\n",
    "# Number of weights from each group to track\n",
    "num_samples = 100\n",
    "W_tracked = torch.empty((n_inner, num_samples, 2))\n",
    "R_tracked = torch.empty((n_inner, num_i))\n",
    "ie_track_inds = torch.randint(0, len(ie_update_inds), size=(num_samples,))\n",
    "ei_track_inds = torch.randint(0, len(ei_update_inds), size=(num_samples,))\n",
    "with torch.no_grad():\n",
    "    for i in range(n_inner):\n",
    "        _, W_plastic, R_plastic = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_plastic, R_plastic, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        W_tracked[i, :, 0] = W_plastic[ie_update_inds][ie_track_inds].detach()\n",
    "        W_tracked[i, :, 1] = W_plastic[ei_update_inds][ei_track_inds].detach()\n",
    "        R_tracked[i, :] = torch.mean(R_plastic[num_e:, :], dim=1).detach()\n",
    "\n",
    "mu_familiar_f = torch.mean(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "sig_familiar_f = torch.var(R_plastic[:num_e, familiar_inds], dim=1)\n",
    "mu_novel_f = torch.mean(R_plastic[:num_e, novel_inds], dim=1)\n",
    "sig_novel_f = torch.var(R_plastic[:num_e, novel_inds], dim=1)\n",
    "mu_novel_diff = mu_novel_f - mu_novel_0\n",
    "mu_familiar_diff = mu_familiar_f - mu_familiar_0\n",
    "sig_novel_diff = sig_novel_f - sig_novel_0\n",
    "sig_familiar_diff = sig_familiar_f - sig_familiar_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6bbd72d52ab77",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.966417Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random = R_random.cpu()\n",
    "R_plastic = R_plastic.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630eb42d255674cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.968115Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(R_tracked)\n",
    "plt.title(\"I responses across inner epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddea879c3b9927e2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.970104Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at how weights evolve over the course of inner epoch, for a random realization\n",
    "# Store a random set of 10 weights and see how the 10 weights change over the course of the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd48d9b5cdbe58",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.971685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    ax1.hist(torch.flatten(R_random[:num_e]), density=True, label=\"E responses\")\n",
    "    ax1.hist(torch.flatten(R_random[num_e:]), density=True, label=\"I responses\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    ax2.hist(torch.flatten(R_plastic[:num_e]), density=True, label=\"E responses\")\n",
    "    ax2.hist(torch.flatten(R_plastic[num_e:]), density=True, label=\"I responses\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Responses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7a48ffa1db260",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.973328Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R_random[num_e:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a783a635f96f3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.975049Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO figure out why I responses keep getting blown up to 0\n",
    "R_plastic[num_e:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2901f2e35c4e550",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:56.986239Z",
     "start_time": "2024-08-01T17:46:56.976720Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ei_update_inds].cpu().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71d4a512bc4881",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.978392Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(W_plastic[ie_update_inds].cpu().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547affea0c66ded8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.979980Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO model is learning to set all E->I weights to 0, and that's causing I responses to go to 0, then I->E weights can't do anything to fix that\n",
    "torch.unique((W_plastic[ie_update_inds].cpu().flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e970014f84261189",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.981629Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2)\n",
    "    sparsities_0 = sparsity_per_odor(R_random)\n",
    "    spars_novel = sparsities_0[novel_inds]\n",
    "    spars_familiar = sparsities_0[familiar_inds]\n",
    "    ax1.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax1.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax1.set_title(\"Before plasticity\")\n",
    "    ax1.legend()\n",
    "    \n",
    "    sparsities_f = sparsity_per_odor(R_plastic)\n",
    "    spars_novel = sparsities_f[novel_inds]\n",
    "    spars_familiar = sparsities_f[familiar_inds]\n",
    "    ax2.hist(spars_novel, density=True, label=\"Novel\")\n",
    "    ax2.hist(spars_familiar, density=True, label=\"Familiar\")\n",
    "    ax2.set_title(\"After plasticity\")\n",
    "    ax2.legend()\n",
    "    fig.suptitle(\"Sparsity per odor\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68603cd329249b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.983228Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def verification_set(ie_model, ei_model, runs=10):\n",
    "    corrs = torch.empty((runs,))\n",
    "    ratios = torch.empty((runs,))\n",
    "    spars_change = torch.empty((runs,))\n",
    "    for i in range(runs):\n",
    "        I_ff = correlated_mitral_activity()\n",
    "        hbar_ff = compute_feedforward_activity(I_ff)\n",
    "        W_random = compute_initial_recurrent_weights()\n",
    "        R_random = compute_piriform_response(hbar_ff, W_random, threshold_multiplier)\n",
    "        _, initial_corr = odor_corrs(R_random)\n",
    "        spars_initial = sparsity_per_odor(R_random)\n",
    "        initial_spars_diff = torch.abs(torch.mean(spars_initial[novel_inds]) - torch.mean(spars_initial[familiar_inds]))\n",
    "        \n",
    "        ie_update_inds = get_update_inds(ie_post, ie_pre, W_random)\n",
    "        ei_update_inds = get_update_inds(ei_post, ei_pre, W_random)\n",
    "        \n",
    "        for _ in range(n_inner):\n",
    "            _, W_random, R_random = loss_after_odors(ie_model, ei_model, ie_update_inds, ei_update_inds, W_random, R_random, hbar_ff, threshold_multiplier, plasticity_ie, plasticity_ei, weight_decay, weight_range, lambda_corr, lambda_mu, lambda_var, lambda_sp, detach_grad=True, with_loss=False)\n",
    "        \n",
    "        _, final_corr = odor_corrs(R_random)\n",
    "        ratio = final_corr / initial_corr\n",
    "        spars_final = sparsity_per_odor(R_random)\n",
    "        # Plot actual difference in the sparsity number, not the percent change ratio\n",
    "        # - Also see whether \n",
    "        final_spars_diff = torch.abs(torch.mean(spars_final[novel_inds]) - torch.mean(spars_final[familiar_inds]))\n",
    "        \n",
    "        print(f\"Corr: {initial_corr} -> {final_corr}, Sparsity: {initial_spars_diff} -> {final_spars_diff}, Loss Ratio: {ratio}\")\n",
    "        corrs[i] = final_corr.item()\n",
    "        ratios[i] = ratio.item()\n",
    "        spars_change[i] = (((final_spars_diff - initial_spars_diff) / initial_spars_diff) * 100).item()\n",
    "        \n",
    "    return corrs, ratios, spars_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bafcc0031d6f22",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.984704Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    corrs, ratios, spars_change = verification_set(ie_model, ei_model, runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae243066bddc4c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.986194Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(ratios, bins=20)\n",
    "plt.title(\"Loss ratios\")\n",
    "plt.show()\n",
    "plt.hist(spars_change, bins=20)\n",
    "plt.title(\"Percent change in sparsity per odor for novel vs. familiar families\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1cf70906cd0175",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.987518Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spars_change[spars_change < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279dce2095c309d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.988670Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# version = \"6\"\n",
    "# ie_path = f\"./joint_models/ie_models/ie_model_{version}\"\n",
    "# torch.save(ie_model.state_dict(), ie_path)\n",
    "# ei_path = f\"./joint_models/ei_models/ei_model_{version}\"\n",
    "# torch.save(ei_model.state_dict(), ei_path)\n",
    "\n",
    "# Model 2 - loss ratios between 0.75 and 1, most sparsity changes within 100%\n",
    "# Model 3 - loss ratios between 0.94 and 1.04, reduces sparsity close to 100% (but has NaN values) \n",
    "# Model 4 - reduces sparsity close to 100%, loss ratios between 0.85 and 1 (basically tries to set weights to 0)\n",
    "# Model 5 - loss ratios between 0.9 and 1.1, most sparsity changes go to 0 (negative 100% sparsity change)\n",
    "\n",
    "# Interesting result from model 1 - tries to set all weights to 0\n",
    "#ie_model = create_model()\n",
    "#ie_model.load_state_dict(torch.load(ie_path))\n",
    "#ei_model = create_model()\n",
    "#ei_model.load_state_dict(torch.load(ei_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8ceeefdbfa5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.990202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_colormap(model, type=\"ie\"):\n",
    "    num_ticks = 100\n",
    "    E_min, E_max = torch.min(R_plastic[:num_e, :]), torch.max(R_plastic[:num_e, :])\n",
    "    I_min, I_max = torch.min(R_plastic[num_e:, :]), torch.max(R_plastic[num_e:, :])\n",
    "    E_vals = torch.linspace(E_min, E_max, num_ticks)\n",
    "    I_vals = torch.linspace(I_min, I_max, num_ticks)\n",
    "    E_coords, I_coords = torch.meshgrid(E_vals, I_vals, indexing=\"ij\")\n",
    "    R_plot = 0\n",
    "    if type == \"ie\":\n",
    "        R_plot = torch.stack((E_coords.flatten(), I_coords.flatten()), dim=1)\n",
    "    elif type == \"ei\":\n",
    "        R_plot = torch.stack((I_coords.flatten(), E_coords.flatten()), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        plasticity_vals = model(R_plot.to(gpu)).squeeze(0).cpu()\n",
    "        \n",
    "    plot = plt.scatter(R_plot[:, 0], R_plot[:, 1], c=plasticity_vals, cmap='rainbow')\n",
    "    clrbar = plt.colorbar(plot)\n",
    "    clrbar.set_label('Model plasticity')\n",
    "    plt.xlabel(\"E responses\")\n",
    "    plt.ylabel(\"I responses\")\n",
    "        \n",
    "    return E_min, E_max, I_min, I_max, R_plot, plasticity_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28c037048e3b5a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.991543Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def fit_model(model, type=\"ie\", degree=3):\n",
    "    E_min, E_max, I_min, I_max, R_plot, plasticity_vals = make_colormap(model, type)\n",
    "    \n",
    "\n",
    "    std = 2 # Take data within 2 std of responses mean\n",
    "    mu_e = torch.mean(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    sig_e = torch.std(torch.flatten(R_plastic[:num_e].cpu()))\n",
    "    mu_i = torch.mean(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    sig_i = torch.std(torch.flatten(R_plastic[num_e:].cpu()))\n",
    "    e_train_bounds = (torch.max(E_min, mu_e - std*sig_e), torch.min(E_max, mu_e + std*sig_e))\n",
    "    i_train_bounds = (torch.max(I_min, mu_i - std*sig_i), torch.min(I_max, mu_i + std*sig_i))\n",
    "    \n",
    "    reg = Pipeline([('poly', PolynomialFeatures(degree=degree)),\n",
    "                  ('linear', LinearRegression(fit_intercept=False))])\n",
    "    \n",
    "    e_inds = torch.logical_and(torch.ge(R_plot[:, 0], e_train_bounds[0]), torch.le(R_plot[:, 0], e_train_bounds[1]))\n",
    "    i_inds = torch.logical_and(torch.ge(R_plot[:, 1], i_train_bounds[0]), torch.le(R_plot[:, 1], i_train_bounds[1]))\n",
    "    b = torch.logical_and(e_inds, i_inds)\n",
    "    X = R_plot[b]\n",
    "    y = plasticity_vals[b].squeeze(0)\n",
    "    \n",
    "    reg = reg.fit(X, y)\n",
    "    # Exclude bias term\n",
    "    coefs = reg.named_steps['linear'].coef_[0][1:]\n",
    "    # How many terms to consider\n",
    "    take = 3\n",
    "    # Take largest magnitude terms\n",
    "    inds = np.argsort(np.abs(coefs))[-take:]\n",
    "\n",
    "    # Exclude bias term\n",
    "    term_list = reg.named_steps['poly'].powers_[1:, :]\n",
    "    terms = f\"[Pre, Post] Powers:\\n\"\n",
    "    for i in range(take - 1, -1, -1):\n",
    "        term = f\"{term_list[inds][i]}: {coefs[inds][i]}\\n\"\n",
    "        terms += term\n",
    "    \n",
    "    print(terms)\n",
    "    \n",
    "    preds = reg.predict(X).ravel()\n",
    "    return X, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac6392aab0837e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.992481Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ie_X, ie_preds = fit_model(ie_model, type=\"ie\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ie_X[:, 0], ie_X[:, 1], c=ie_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"E-I plasticity based on neural responses\")\n",
    "plt.xlabel(\"E responses\")\n",
    "plt.ylabel(\"I responses\")\n",
    "plt.show()\n",
    "\n",
    "ei_X, ei_preds = fit_model(ei_model, type=\"ei\", degree=3)\n",
    "\n",
    "pred_plot = plt.scatter(ei_X[:, 0], ei_X[:, 1], c=ei_preds, cmap='rainbow')\n",
    "clrbar2 = plt.colorbar(pred_plot)\n",
    "clrbar2.set_label('Predicted')\n",
    "plt.title(\"I-E plasticity based on neural responses\")\n",
    "plt.xlabel(\"I responses\")\n",
    "plt.ylabel(\"E responses\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa938bb7961846a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T17:46:57.060814Z",
     "start_time": "2024-08-01T17:46:56.993604Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do a plot of rho_0 vs rho_final (for ex. 100 random initializations) to see whether the model can decorrelate odors\n",
    "# Even though potentiation is always positive for the set of responses, since we are doing E to I, increasing the connection strength effectively increases inhibition too, so it balances out\n",
    "# Not directly hebbian because that would mean there is some constant c*r_i*r_j, but when one of the responses is decreasing, the resulting potentiation doesn't decrease\n",
    "# Instead, it will be an a * r_i + b * r_j term\n",
    "# These terms come from the polynomial expansion of the function defined by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1fe6484d9fbc92",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.994660Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at statistics of E neurons (mean firing rate across odors, variance, etc) at initialization compared to after plasticity\n",
    "# Also look at sparsity - one across odors and another across neurons (sparsity is essentially 1 minus the square of the coefficient of variation)\n",
    "# We care mostly about the sparsity across neurons (ex between neurons) and what it would be between odors (should be similar between the familiar odors b/c that's where we applied the plasticity) and we also know what it's like between novel odors\n",
    "# We don't want a change in firing rate, because it should be same for novel and familiar odors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed3911273faa8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.995682Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pct_change = ((W_tracked[:, :, 0] - w_ie) / w_ie) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"E-I weight change\")\n",
    "    plt.show()\n",
    "    \n",
    "    pct_change = ((W_tracked[:, :, 1] - w_ei) / w_ei) * 100\n",
    "    plt.plot(pct_change)\n",
    "    plt.xlabel(\"Inner epochs\")\n",
    "    plt.ylabel(\"Weight percent change\")\n",
    "    plt.title(\"I-E weight change\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d8df91ba43350",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.996672Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_change = ((W_plastic[ie_update_inds].cpu() - w_ie) / w_ie) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"E-I Percent weight change from initialization\")\n",
    "plt.show()\n",
    "\n",
    "W_change = ((W_plastic[ei_update_inds].cpu() - w_ei) / w_ei) * 100\n",
    "plt.hist(W_change, density=True, bins=50)\n",
    "plt.title(\"I-E Percent weight change from initialization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d07fb09871512a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.997803Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_familiar_0.detach().cpu() - mu_novel_0.detach().cpu(), bins=50, label=\"Before\")\n",
    "plt.hist(mu_familiar_f.detach().cpu() - mu_novel_f.detach().cpu(), bins=50, label=\"After\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90208f664a85ae",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.998838Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(mu_novel_diff, bins=50, label=\"Novel\")\n",
    "plt.hist(mu_familiar_diff, bins=50, label=\"Familiar\")\n",
    "plt.title(\"Difference in mean responses before and after plasticity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc851bc8f43f35",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:56.999877Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sp_novel_0 = sparsity_per_neuron(R_random, novel_inds)\n",
    "sp_novel_f = sparsity_per_neuron(R_plastic, novel_inds)\n",
    "sp_familiar_0 = sparsity_per_neuron(R_random, familiar_inds)\n",
    "sp_familiar_f = sparsity_per_neuron(R_plastic, familiar_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee33d1ae62b6cd3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.000850Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.hist(sp_novel_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_0\")\n",
    "    plt.hist(sp_familiar_0, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_0\")\n",
    "    plt.hist(sp_novel_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Novel_f\")\n",
    "    plt.hist(sp_familiar_f, cumulative=True, bins=num_e, histtype=\"step\", label=\"Familiar_f\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce75bd2a06f39a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.001884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create scatter plot of odor correlation vs sparsity difference - odors that are highly positively correlated should have similar sparsities across neurons, and those that are highly negatively correlated should have different sparsities across neurons\n",
    "# Check - increase P' all the way to 16, and with lower correlations, there should be less variability between the sparsity for each odor\n",
    "# - would tell us how much the natural spread in sparsity is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcd6174a08701aa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.003211Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Re-run model w/ lower threshold (0 stdev above mean)\n",
    "# 2. Figure out percentage-wise how much the weights actually change (do a hist)\n",
    "# - if it's 1-2% it's too small, we want the weights to change 10-100%, the weights could even change 10-fold\n",
    "# 3. Add metrics to training function - automatically compute sparsity etc (also look at the sparsity between odors, not just the sparsity between neurons, see if it changes for novel vs familiar)\n",
    "# Create bar plot for each number of odors, how many neurons respond to that number of odors (ideally, if we have a threshold of 0 stdev, most neurons should respond to ~4 odors, since on average a neuron will respond to half of the total odors, so 8 odors, and out of those, it should be equal between 4 novel and 4 familiar)\n",
    "# Hypothesis right now - weights aren't changing that much, so we can add more epoch_inner steps (since the gradient isn't blowing up)\n",
    "# Then, try removing plasticity ramp - keep 1e-3 (don't do epoch_inner increase and remove plasticity ramp at same time)\n",
    "# Goal: understand what mechanism the meta-learning discovered that makes correlations smaller (see whether it acts on sparsity etc)\n",
    "# Think of other metrics to quantify network behavior to understand change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57e8808ab6eb092",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-01T17:46:57.004349Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f64f3bee69275d7dadabcd164c00bee7a237ebc40dc30e8b43706029d0d9fbd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
